{
    "docs": [
        {
            "location": "/", 
            "text": "HADeploy\n\n\nOverview\n\n\nAn Hadoop application is composed of many independent components and resources:\n\n\n\n\nScripts or programs (Hive, MapReduce, Spark, Pig,\u2026)\n\n\nHDFS Layout\n\n\nHive table (Hive Metastore definition).\n\n\nHBase table.\n\n\nKafka topics,\n\n\nInitial data sets.\n\n\n\u2026\n\n\n\n\nCurrent deployment tools do not provide plugins for Hadoop components. And each component has its own configuration and authorization  system, designed and implemented independently.\n\n\nHADeploy is designed to solve these issues. Thus facilitating the adoption of Continuous Integration and Continuous Deployment practices in the Hadoop world.\n\n\nHow it works\n\n\n\n\nHADeploy can be installed on an dedicated node, or on a user\u2019s workstation.\nFrom this node, it will issue appropriate commands to be executed on the different nodes of the target cluster.\n\n\nIt will also push application artifact, or trigger artifact fetching from any repository server (Nexus, Artifactory, Jenkins, or a simple Http server).\n\n\nA single HADeploy installation can deal with multiple clusters.\n\n\nBase principles\n\n\nApplication manifest\n\n\nAn application can be fully described in a single file, along with all their required components and resources.\n\n\nInfrastructure agnostic\n\n\nThe Application Manifest model is agnostic to the target physical infrastructure. \nThe target cluster is defined in a separate file that will be accessed at deployment time, and that can be used for all application deployments on this cluster.\n\n\nEnvironment agnostic.\n\n\nHADeploy can also take as input an environment description file that encourages re-use, consistency and repeatable deployments in different execution contexts \n(e.g. Development vs Production environments\u2026)\n\n\nIdempotence\n\n\nHADeploy embeds the lightweight Ansible tool as its main execution engine. By doing so, it benefits from the idempotence principles at the core of Ansible design.\nAs with Ansible, you will be able to run the same application deployment multiple times and expect the same result. If the application is in the expected state on the target cluster, \nthen HADeploy will not perform any actions, but simply display that it is satisfied with the current state.\n\n\nTarget State vs Programming\n\n\nHADeploy is a purely descriptive tool. No programming is required. Admins define the expected state of the application and let the tool perform \nthe reconciliation between expected and actual state.\n\n\nApplication instance isolation.\n\n\nA typical deployment pattern allowed by HADeploy is to define \u2018Application Container\u2019, or \u2018Application Corridor\u2019. Then several instances (or versions) of an application can be installed and run in parallel.\n\n\nKerberos support\n\n\nHADeploy is Kerberos friendly and will happily deploy applications on your secured cluster. \n\n\nPrivilege management\n\n\nHADeploy will manage all permissions associated to the deployed components and resources\n\n\nPlugins architecture\n\n\nHADeploy is designed with a modular plugin architecture, thus allowing easy third party extension.\n\n\nApplication Removal\n\n\nSince HADeploy knows about all the components of your application, it provides a REMOVAL mode that will restore the target cluster in its initial state. \n\n\nOpen Source\n\n\nHADeploy is a fully open source project, under GNU General Public License. \nHosted on Github\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#hadeploy", 
            "text": "", 
            "title": "HADeploy"
        }, 
        {
            "location": "/#overview", 
            "text": "An Hadoop application is composed of many independent components and resources:   Scripts or programs (Hive, MapReduce, Spark, Pig,\u2026)  HDFS Layout  Hive table (Hive Metastore definition).  HBase table.  Kafka topics,  Initial data sets.  \u2026   Current deployment tools do not provide plugins for Hadoop components. And each component has its own configuration and authorization  system, designed and implemented independently.  HADeploy is designed to solve these issues. Thus facilitating the adoption of Continuous Integration and Continuous Deployment practices in the Hadoop world.", 
            "title": "Overview"
        }, 
        {
            "location": "/#how-it-works", 
            "text": "HADeploy can be installed on an dedicated node, or on a user\u2019s workstation.\nFrom this node, it will issue appropriate commands to be executed on the different nodes of the target cluster.  It will also push application artifact, or trigger artifact fetching from any repository server (Nexus, Artifactory, Jenkins, or a simple Http server).  A single HADeploy installation can deal with multiple clusters.", 
            "title": "How it works"
        }, 
        {
            "location": "/#base-principles", 
            "text": "", 
            "title": "Base principles"
        }, 
        {
            "location": "/#application-manifest", 
            "text": "An application can be fully described in a single file, along with all their required components and resources.", 
            "title": "Application manifest"
        }, 
        {
            "location": "/#infrastructure-agnostic", 
            "text": "The Application Manifest model is agnostic to the target physical infrastructure. \nThe target cluster is defined in a separate file that will be accessed at deployment time, and that can be used for all application deployments on this cluster.", 
            "title": "Infrastructure agnostic"
        }, 
        {
            "location": "/#environment-agnostic", 
            "text": "HADeploy can also take as input an environment description file that encourages re-use, consistency and repeatable deployments in different execution contexts \n(e.g. Development vs Production environments\u2026)", 
            "title": "Environment agnostic."
        }, 
        {
            "location": "/#idempotence", 
            "text": "HADeploy embeds the lightweight Ansible tool as its main execution engine. By doing so, it benefits from the idempotence principles at the core of Ansible design.\nAs with Ansible, you will be able to run the same application deployment multiple times and expect the same result. If the application is in the expected state on the target cluster, \nthen HADeploy will not perform any actions, but simply display that it is satisfied with the current state.", 
            "title": "Idempotence"
        }, 
        {
            "location": "/#target-state-vs-programming", 
            "text": "HADeploy is a purely descriptive tool. No programming is required. Admins define the expected state of the application and let the tool perform \nthe reconciliation between expected and actual state.", 
            "title": "Target State vs Programming"
        }, 
        {
            "location": "/#application-instance-isolation", 
            "text": "A typical deployment pattern allowed by HADeploy is to define \u2018Application Container\u2019, or \u2018Application Corridor\u2019. Then several instances (or versions) of an application can be installed and run in parallel.", 
            "title": "Application instance isolation."
        }, 
        {
            "location": "/#kerberos-support", 
            "text": "HADeploy is Kerberos friendly and will happily deploy applications on your secured cluster.", 
            "title": "Kerberos support"
        }, 
        {
            "location": "/#privilege-management", 
            "text": "HADeploy will manage all permissions associated to the deployed components and resources", 
            "title": "Privilege management"
        }, 
        {
            "location": "/#plugins-architecture", 
            "text": "HADeploy is designed with a modular plugin architecture, thus allowing easy third party extension.", 
            "title": "Plugins architecture"
        }, 
        {
            "location": "/#application-removal", 
            "text": "Since HADeploy knows about all the components of your application, it provides a REMOVAL mode that will restore the target cluster in its initial state.", 
            "title": "Application Removal"
        }, 
        {
            "location": "/#open-source", 
            "text": "HADeploy is a fully open source project, under GNU General Public License.  Hosted on Github .", 
            "title": "Open Source"
        }, 
        {
            "location": "/installation/", 
            "text": "Installation\n\n\nVersion\n\n\nThe latest stable version of HADeploy is \n0.6.0\n.\n\n\nPrerequisite\n\n\nHADeploy can be installed on a Linux system, or on a MAC OS X workstation\n\n\nInstallation on Windows is not supported, but VirtualBox and Vagrant would be your best friends in this case.\n\n\nAnsible\n\n\nHADeploy internally use \nAnsible\n. If it is not already installed, you can install it independently, or let HADeploy install it automatically.\n\n\nNB: HADeploy require at least \nAnsible version 2.3.0\n \n\n\nVirtual environnement\n\n\nHADeploy (And Ansible) can be installed inside Python \nvirtual Environnemnt\n.\n\n\nInstalling inside virtual environment has several advantages:\n\n\n\n\n\n\nInstallation does not require root access.\n\n\n\n\n\n\nYou can have several isolated version installed on the same system.\n\n\n\n\n\n\nYour installation will not interfere with others users. \n\n\n\n\n\n\nOne drawback is you will need to activate your virtual environment on each login. But you can automate this in your \n~/.profile\n.\n\n\nOf course, if you want to share a single installation of HADeploy amongst all users of your system, you must install it globally.\n\n\nLatest release via \npip\n (Linux)\n\n\nCurrent installation process has been tested on RHEL/CentOS7 (Python 2.7). Installation on other Linux Variant may works. All feedbacks are welcome.\n\n\npip\n is the Python package manager. If not already installed, you can easely install it. For example, on RHEL/CentOS7:\n\n\nsudo yum install -y python-pip\n\n\n\n\nIf Ansible is not already installed, you must ensure several required package are present. For example, on RHEL/CentOS7:\n\n\nsudo yum install -y python-devel openssl-devel gcc\n\n\n\n\nWe strongly adivise you to ensure you have the latest \npip\n version:\n\n\nsudo pip install --upgrade pip\n\n\n\n\nThen, you can install HADeploy:\n\n\nsudo pip install HADeploy\n\n\n\n\nOr simply \n\n\npip install HADeploy\n\n\n\n\nIf you are in a virtual environment\n\n\n\n\nIf you encounter any trouble during these last steps, again, be sure you have the latest pip version, as stated above. \n\n\n\n\nYou are now ready to used HADeploy from this workstation.\n\n\nLatest release via \npip\n (Mac OS X)\n\n\npip\n is the Python package manager. If not already installed, you can easily install it. \n\n\nsudo easy_install pip\n\n\n\n\nThen, you can install HADeploy:\n\n\nsudo pip install HADeploy\n\n\n\n\nOr simply \n\n\npip install HADeploy\n\n\n\n\nIf you are in a virtual environment\n\n\nYou are now ready to used HADeploy from this workstation.\n\n\n\n\nDo NOT use \neasy_install\n to install HADeploy directly.\n\n\n\n\nInstall from source GIT repository.\n\n\nHADeploy is easy to install directly from source. If you want to have the very latest versions, or a specific one, or intend to contribute, this is the method of choice.\n\n\nFor Linux only, install several required package:\n\n\nsudo yum install -y git python-pip python-devel openssl-devel\n\n\n\n\nThen clone the HADeploy git repository:\n\n\ngit clone https://github.com/BROADSoftware/hadeploy.git\n\n\n\n\nMove to the newly create directory: \n\n\ncd hadeploy\n\n\n\n\nIf you need, switch on a specific version.\n\n\ngit checkout v0.X.X\n\n\n\n\n(Use \ngit tag\n to have a list of all versions)\n\n\nAnd perform required python module installation using pip:\n\n\nsudo pip install -r requirements.txt\n\n\n\n\nor simply\n\n\npip install -r requirements.txt\n\n\n\n\nIf you are in a virtual environment\n\n\nTo perform some Hive, Kafka and HBase configuration, HADeploy will use some specific modules. These need to be downloaded to complete the installation. For this:\n\n\nchmod +x lib/hadeploy/plugins/setup.sh\n./lib/hadeploy/plugins/setup.sh\n\n\n\n\nAnd, last but not least, add HADeploy to your path:\n\n\ncd ../bin\nexport PATH=$PATH:$(pwd)\n# or\nexport PATH=$PATH:\nwhereHADeployWasCloned\n/hadeploy/bin\n\n\n\n\nYou are now ready to used HADeploy from this workstation.\n\n\nUsing ZIP archive\n\n\nIf, for any reason, you are unable to clone from Github, you can download the source code:\n\n\n\n\nBefore downloading, you may take care of selecting the appropriate version (\nTag:\n, on the left).\n\n\nAll other installation steps described above remains unchanged.\n\n\nNo direct Internet access ?\n\n\nIf the computer on which you want to install HADeploy can access Internet using a proxy, you can configure \npip\n to use it. If this is not possible, here is an alternate method:\n\n\nThese instructions are for RHEL/CentOS7. They could be adapted for other environment.\n\n\nIt is assumed the computer has access to a local yum package repository, including EPEL. \n\n\nIt is also assumed you have an Internet access from another system, and be able to copy files from this system to the isolated target.\n\n\nFirst, install \nAnsible\n and \npip\n using yum:\n\n\nsudo yum install -y ansible python-pip\n\n\n\n\nThen, you will need to download a set of python packages from another (connected) computer: \n\n\n\n\nDownload the latest \nHADeploy\n 'wheel' package (.whl file) from the \npypi site\n \n\n\nDownload the latest \npykwalify\n 'wheel' package (.whl file) from the \npypi site\n\n\nDownload the latest \ndocopt package\n (.tar.gz file) from the \npypi site\n\n\nDownload the latest \nPyYAML package\n (.tar.gz file) from the \npypi site\n\n\n\n\nDownload the latest \npython-dateutil\n 'wheel' package (.whl file) from the \npypi site\n\n\n\n\n\n\nCopy all these this package files to the target system, for example in \n/tmp\n.\n\n\n\n\n\n\nInstall the packages in this order:\n\n\n\n\n\n\nsudo pip install /tmp/python_dateutil-X.Y.Z-py2.py3-none-any.whl\nsudo pip install /tmp/PyYAML-X.YZ.tar.gz\nsudo pip install /tmp/docopt-X.Y.Z.tar.gz\nsudo pip install /tmp/pykwalify-X.Y.Z-py2.py3-none-any.whl\nsudo pip install /tmp/HADeploy-X.Y.Z-py2-none-any.whl\n\n\n\n\nAdjust the version number for the downloaded one. And remove the sudo if you install in a virtual environment.\n\n\nYou should be now ready to used HADeploy from this workstation.\n\n\nNote this procedure reflect the package dependencies at the time of this writing. This can evolve, but, applying the same principle, you should be able to adjust.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#version", 
            "text": "The latest stable version of HADeploy is  0.6.0 .", 
            "title": "Version"
        }, 
        {
            "location": "/installation/#prerequisite", 
            "text": "HADeploy can be installed on a Linux system, or on a MAC OS X workstation  Installation on Windows is not supported, but VirtualBox and Vagrant would be your best friends in this case.", 
            "title": "Prerequisite"
        }, 
        {
            "location": "/installation/#ansible", 
            "text": "HADeploy internally use  Ansible . If it is not already installed, you can install it independently, or let HADeploy install it automatically.  NB: HADeploy require at least  Ansible version 2.3.0", 
            "title": "Ansible"
        }, 
        {
            "location": "/installation/#virtual-environnement", 
            "text": "HADeploy (And Ansible) can be installed inside Python  virtual Environnemnt .  Installing inside virtual environment has several advantages:    Installation does not require root access.    You can have several isolated version installed on the same system.    Your installation will not interfere with others users.     One drawback is you will need to activate your virtual environment on each login. But you can automate this in your  ~/.profile .  Of course, if you want to share a single installation of HADeploy amongst all users of your system, you must install it globally.", 
            "title": "Virtual environnement"
        }, 
        {
            "location": "/installation/#latest-release-via-pip-linux", 
            "text": "Current installation process has been tested on RHEL/CentOS7 (Python 2.7). Installation on other Linux Variant may works. All feedbacks are welcome.  pip  is the Python package manager. If not already installed, you can easely install it. For example, on RHEL/CentOS7:  sudo yum install -y python-pip  If Ansible is not already installed, you must ensure several required package are present. For example, on RHEL/CentOS7:  sudo yum install -y python-devel openssl-devel gcc  We strongly adivise you to ensure you have the latest  pip  version:  sudo pip install --upgrade pip  Then, you can install HADeploy:  sudo pip install HADeploy  Or simply   pip install HADeploy  If you are in a virtual environment   If you encounter any trouble during these last steps, again, be sure you have the latest pip version, as stated above.    You are now ready to used HADeploy from this workstation.", 
            "title": "Latest release via pip (Linux)"
        }, 
        {
            "location": "/installation/#latest-release-via-pip-mac-os-x", 
            "text": "pip  is the Python package manager. If not already installed, you can easily install it.   sudo easy_install pip  Then, you can install HADeploy:  sudo pip install HADeploy  Or simply   pip install HADeploy  If you are in a virtual environment  You are now ready to used HADeploy from this workstation.   Do NOT use  easy_install  to install HADeploy directly.", 
            "title": "Latest release via pip (Mac OS X)"
        }, 
        {
            "location": "/installation/#install-from-source-git-repository", 
            "text": "HADeploy is easy to install directly from source. If you want to have the very latest versions, or a specific one, or intend to contribute, this is the method of choice.  For Linux only, install several required package:  sudo yum install -y git python-pip python-devel openssl-devel  Then clone the HADeploy git repository:  git clone https://github.com/BROADSoftware/hadeploy.git  Move to the newly create directory:   cd hadeploy  If you need, switch on a specific version.  git checkout v0.X.X  (Use  git tag  to have a list of all versions)  And perform required python module installation using pip:  sudo pip install -r requirements.txt  or simply  pip install -r requirements.txt  If you are in a virtual environment  To perform some Hive, Kafka and HBase configuration, HADeploy will use some specific modules. These need to be downloaded to complete the installation. For this:  chmod +x lib/hadeploy/plugins/setup.sh\n./lib/hadeploy/plugins/setup.sh  And, last but not least, add HADeploy to your path:  cd ../bin\nexport PATH=$PATH:$(pwd)\n# or\nexport PATH=$PATH: whereHADeployWasCloned /hadeploy/bin  You are now ready to used HADeploy from this workstation.", 
            "title": "Install from source GIT repository."
        }, 
        {
            "location": "/installation/#using-zip-archive", 
            "text": "If, for any reason, you are unable to clone from Github, you can download the source code:   Before downloading, you may take care of selecting the appropriate version ( Tag: , on the left).  All other installation steps described above remains unchanged.", 
            "title": "Using ZIP archive"
        }, 
        {
            "location": "/installation/#no-direct-internet-access", 
            "text": "If the computer on which you want to install HADeploy can access Internet using a proxy, you can configure  pip  to use it. If this is not possible, here is an alternate method:  These instructions are for RHEL/CentOS7. They could be adapted for other environment.  It is assumed the computer has access to a local yum package repository, including EPEL.   It is also assumed you have an Internet access from another system, and be able to copy files from this system to the isolated target.  First, install  Ansible  and  pip  using yum:  sudo yum install -y ansible python-pip  Then, you will need to download a set of python packages from another (connected) computer:    Download the latest  HADeploy  'wheel' package (.whl file) from the  pypi site    Download the latest  pykwalify  'wheel' package (.whl file) from the  pypi site  Download the latest  docopt package  (.tar.gz file) from the  pypi site  Download the latest  PyYAML package  (.tar.gz file) from the  pypi site   Download the latest  python-dateutil  'wheel' package (.whl file) from the  pypi site    Copy all these this package files to the target system, for example in  /tmp .    Install the packages in this order:    sudo pip install /tmp/python_dateutil-X.Y.Z-py2.py3-none-any.whl\nsudo pip install /tmp/PyYAML-X.YZ.tar.gz\nsudo pip install /tmp/docopt-X.Y.Z.tar.gz\nsudo pip install /tmp/pykwalify-X.Y.Z-py2.py3-none-any.whl\nsudo pip install /tmp/HADeploy-X.Y.Z-py2-none-any.whl  Adjust the version number for the downloaded one. And remove the sudo if you install in a virtual environment.  You should be now ready to used HADeploy from this workstation.  Note this procedure reflect the package dependencies at the time of this writing. This can evolve, but, applying the same principle, you should be able to adjust.", 
            "title": "No direct Internet access ?"
        }, 
        {
            "location": "/getting_started/first_step/", 
            "text": "First step\n\n\nAn application deployment is performed based on a file (or a set of files) describing all its components. This description represents a target state HADeploy will have to reach by performing all transformation operation.\n\n\nThis description can be defined in a single file, or can be spread over several files for ease of management and separation of concerns.\n\n\nAll these files are defined using YAML syntax, as this syntax will allow both readability and concision. (If necessary, you will find a bunch of YAML tutorial on the net).\n\n\nFollowing is a simple deployment: \n\n\n# We only need to access the cluster's edge node. (1)\nhosts:\n- name: en\n  ssh_host: en.cluster1.mydomain.com\n  ssh_user: root\n  ssh_private_key_file: \nkeys/build_key\n \n\n# HDFS operations will be issued from this host: (2)\nhdfs_relay:\n  host: en\n\n# Folders creation (3)\nfolders:\n- { scope: en, path: /opt/broadapp, owner: broaduser, group: broadgroup, mode: \n0755\n }\n- { scope: hdfs, path: /apps/broadapp, owner: broaduser, group: broadgroup, mode: \n0755\n }\n\n# Files copy (4)\nfiles:\n- { scope: en, src: \nhttp://my.server.com/repository/broadapp-0.3.2.jar\n, dest_folder: \n/opt/broadapp\n,  owner: broaduser, group: broadgroup, mode: \n0644\n }\n- { scope: en, src: \ntmpl://launcher.sh\n, dest_folder: /opt/broadapp, owner: broaduser, group: broadgroup, mode: \n0644\n }\n\n- { scope: hdfs, src: \ntmpl://broadapp.cfg\n, dest_folder: \n/apps/broadapp\n, owner: broaduser, group: broadgroup, mode: \n0644\n }\n\n\n\n\n\n(1) First, we begin by the inventory part, describing the target cluster. Here, we will just need access to the edge node. \n\n\n(2) Then we define this last as the relay we will use to access HDFS.\n\n\n\n\nIssuing some commands to specifics subsystem, such as HDFS require a quite complex client configuration. \nTo avoid this, HADeploy will not issue such command directly, but push the command on one of the cluster node (Typically an Edge node)\n\n\n\n\n(3) Then, we create folders to store application items, one on the edge node, and one on HDFS. Note we also adjust related permissions and ownership.\n\n\n(4) And last, we deploy the application's files. A jar and the launching script on the edge node, and a config file on HDFS.\n\n\n\n\nThe launcher and config file are prefixed by \ntmpl\n. This means they can include parameters set on deployment to configure them accordingly to the target infrastructure. Such file will be called 'tempalates'", 
            "title": "First step"
        }, 
        {
            "location": "/getting_started/first_step/#first-step", 
            "text": "An application deployment is performed based on a file (or a set of files) describing all its components. This description represents a target state HADeploy will have to reach by performing all transformation operation.  This description can be defined in a single file, or can be spread over several files for ease of management and separation of concerns.  All these files are defined using YAML syntax, as this syntax will allow both readability and concision. (If necessary, you will find a bunch of YAML tutorial on the net).  Following is a simple deployment:   # We only need to access the cluster's edge node. (1)\nhosts:\n- name: en\n  ssh_host: en.cluster1.mydomain.com\n  ssh_user: root\n  ssh_private_key_file:  keys/build_key  \n\n# HDFS operations will be issued from this host: (2)\nhdfs_relay:\n  host: en\n\n# Folders creation (3)\nfolders:\n- { scope: en, path: /opt/broadapp, owner: broaduser, group: broadgroup, mode:  0755  }\n- { scope: hdfs, path: /apps/broadapp, owner: broaduser, group: broadgroup, mode:  0755  }\n\n# Files copy (4)\nfiles:\n- { scope: en, src:  http://my.server.com/repository/broadapp-0.3.2.jar , dest_folder:  /opt/broadapp ,  owner: broaduser, group: broadgroup, mode:  0644  }\n- { scope: en, src:  tmpl://launcher.sh , dest_folder: /opt/broadapp, owner: broaduser, group: broadgroup, mode:  0644  }\n\n- { scope: hdfs, src:  tmpl://broadapp.cfg , dest_folder:  /apps/broadapp , owner: broaduser, group: broadgroup, mode:  0644  }  (1) First, we begin by the inventory part, describing the target cluster. Here, we will just need access to the edge node.   (2) Then we define this last as the relay we will use to access HDFS.   Issuing some commands to specifics subsystem, such as HDFS require a quite complex client configuration. \nTo avoid this, HADeploy will not issue such command directly, but push the command on one of the cluster node (Typically an Edge node)   (3) Then, we create folders to store application items, one on the edge node, and one on HDFS. Note we also adjust related permissions and ownership.  (4) And last, we deploy the application's files. A jar and the launching script on the edge node, and a config file on HDFS.   The launcher and config file are prefixed by  tmpl . This means they can include parameters set on deployment to configure them accordingly to the target infrastructure. Such file will be called 'tempalates'", 
            "title": "First step"
        }, 
        {
            "location": "/getting_started/next_step/", 
            "text": "Next step\n\n\nWe will describe here the deployment of a simple application, 'broadapp', which will need to deploy some artifacts and resources on a Hadoop cluster, both on nodes and on HDFS.\n\n\nWe will not deeply enter in the detail of all items. You will find more exhaustive description in the Reference part of this document.\n\n\n\n\nNOTE: This sample is intended to demonstrate the basic features of HADeploy. It is not intended to define 'Best Practices' about Hadoop Application deployment.\n\n\n\n\nThis example will be divided in two parts: One describing the target Hadoop cluster (Infrastructure description) and a second one the application deployment itself (Application description) \n\n\nInfrastructure description\n\n\nThere is two methods to provide HADeploy with the target infrastructure description. One is to explicitly list all hosts. Another one is to refer to an already existing Ansible Inventory.\n\n\nLet's start with the first one: \n\n\nExplicit description\n\n\nThe infrastructure is described in a file with the following content:\n\n\n# ------------------------------------- Infrastructure part\n\nhosts:\n# A first host expressed the standard yaml style\n- name: sr\n  ssh_host: sr.cluster1.mydomain.com\n  ssh_user: root\n  ssh_private_key_file: \nkeys/build_key\n \n# And these others using more consise yaml 'flow style'\n- { name: en1, ssh_host: en.cluster1.mydomain.com, ssh_user: root, ssh_private_key_file: \nkeys/build_key\n }\n- { name: nn1, ssh_host: nn1.cluster1.mydomain.com, ssh_user: root, ssh_private_key_file: \nkeys/build_key\n }\n- { name: nn2, ssh_host: nn2.cluster1.mydomain.com, ssh_user: root, ssh_private_key_file: \nkeys/build_key\n }\n- { name: dn1, ssh_host: dn1.cluster1.mydomain.com, ssh_user: root, ssh_private_key_file: \nkeys/build_key\n }\n- { name: dn2, ssh_host: dn2.cluster1.mydomain.com, ssh_user: root, ssh_private_key_file: \nkeys/build_key\n }\n- { name: dn3, ssh_host: dn3.cluster1.mydomain.com, ssh_user: root, ssh_private_key_file: \nkeys/build_key\n }\n\nhost_groups:\n- name: data_nodes\n  hosts:    # This host list in standart YAML style\n  - dn1\n  - dn2\n  - dn3\n- name: control_nodes\n  hosts: [ \nsr\n, \nen\n, \nnn1\n, \nnn2\n ]   # And these in YAML 'flow style'\n- name: zookeepers\n  hosts: [ \nsr\n, \nnn1\n, \nnn2\n ]\n\nhdfs_relay:\n  host: en1\n\nkafka_relay:\n  host: br1\n  zookeeper:\n    host_group: zookeepers\n\nhbase_relay:\n  host: en1\n\n\n\n\nFirst, we will describe the several hosts our cluster is made of. Each host as a name: entry, by which we will reference it, and others entries describing how to access it through ssh.\n\n\nThe specified \nssh_user:\n is typically \nroot\n, as it must be a user with enough rights to perform all the required actions.\n\n\nThen we will find a block of \nhost_groups\n: description. This will simply allow grouping hosts by function, and easing hosts's reference.\n\n\nThen is the definition of the \nhdfs_relay\n: specifying which host will support all HDFS related operations.\n\n\nThen is the definition of the \nkafka_relay\n: which will be relaying all topics creation, modification and removal operations. As such it need to know how to access zookeeper and to be able to access Kafka library folder as it use corresponding jar files. The simplest solution is to use a broker node for this task.\n\n\nAnd last is the definition of \nhbase_relay\n: which will be relaying all HBase table creation, modification and removal operation. \n\n\nFrom an existing Ansible inventory.\n\n\nThose familiar to Ansible should have noted such description is very similar to the way Ansible describe its inventory.\n\n\nIf the target cluster is already described in an Ansible inventory file, then one may simple reference it instead of duplicate information.\n\n\nIn such case, the infrastructure part of the file will look like:\n\n\n# ------------------------------------- Infrastructure part\n\nansible_inventories:\n- file: \n.../some-ansible-build/inventory\n\n\nhost_groups:\n- name: control_nodes\n  hosts: [ \nsr1\n, \nen1\n, \nen2\n, \nnn1\n, \nnn2\n ]\n- name: zookeepers\n  hosts: [ \nsr1\n, \nnn1\n, \nnn2\n ]\n\nhdfs_relay:\n  host: en1\n\nkafka_relay:\n  host: br1\n  zookeeper:\n    host_group: zookeepers\n\nhbase_relay:\n  host: en1\n\n\n\n\nIn our case, we add some \nhost_groups\n as we need them for the following, and there were not defined in the source Ansible inventory\n\n\nNote we also have the ability to modify only some attributes of one, several or all hosts of this Ansible inventory. See \nhost_overrides\n in the reference part.\n\n\nApplication description\n\n\nHere is the file describing the application deployment:\n\n\n# ------------------------------------ Environment part\n# Except if begining with /, All path are relative to this file\nlocal_files_folders:\n- \n./files\n\n\nlocal_templates_folders:\n- \n./templates\n\n\n# ------------------------------------- Application part\n\nvars:\n  app_version: \n0.1.1\n\n  repository_server: \nmyserver.com\n\n  app_jar_url: \nhttp://${repository_server}/repo/broadapp-${app_version}.jar\n\n  zookeeper:\n    connection_timeout: 60000\n    zkpath: /broadapp\n\n\ngroups:\n  - scope: all\n    name: broadgroup\n\nusers:\n- login: broadapp\n  comment: \nBroadapp technical account\n\n  groups: \nbroadgroup\n\n\n- { login: broaduser, scope: control_nodes, comment: \nBroadapp user account\n, groups: \nbroadgroup\n, can_sudo_to: \nhdfs, yarn\n }\n\nfolders:\n- { scope: \nen1:data_nodes\n, path: /var/log/broadapp, owner: broadapp, group: broadgroup, mode: \n0755\n }\n- { scope: en1, path: /etc/broadapp, owner: root, group: root, mode: \n0755\n }\n- { scope: en1, path: /opt/broadapp, owner: root, group: root, mode: \n0755\n }\n\n- { scope: hdfs, path: /apps/broadapp, owner: broadapp, group: broadgroup, mode: \n0755\n }\n\nfiles:\n- { scope: en1, src: \n${app_jar_url}\n, dest_folder: \n/opt/broadapp\n,  owner: root, group: root, mode: \n0644\n, validate_certs: no }\n- { scope: en1, src: \ntmpl://broadapp.cfg.j2\n, dest_folder: /etc/broadapp, dest_name: broadapp.cfg, owner: root, group: root, mode: \n0644\n }\n\n- { scope: hdfs, src: \n${app_jar_url}\n, dest_folder: \n/apps/broadapp\n, dest_name: broadapp.jar, owner: broadapp, group: broadgroup, mode: \n0644\n, validate_certs: no }\n\ntrees:\n- { scope: hdfs, src: \nfile://data\n, dest_folder: \n/apps/broadapp/init_data\n, owner: broadapp, group: broadgroup, file_mode: \n0644\n, folder_mode: \n0755\n }\n\n\nkafka_topics:\n- name: broadapp_t1\n  partition_factor: 3\n  replication_factor: 2\n  properties:\n    retention.ms: 630720000000\n    retention.bytes: 858993459200\n\nhbase_namespaces:\n- name: broadgroup\n\nhbase_tables: \n- name: broadapp_t1\n  namespace: broadgroup\n  properties: \n    regionReplication: 1\n    durability: ASYNC_WAL\n  column_families:\n  - name: cf1\n    properties: \n      maxVersions: 12\n      minVersions: 2\n      timeToLive: 200000\n\n\n\n\nLet's describe it part by part:\n\n\nEnvironment part\n\n\n# ------------------------------------ Environment part\n# Except if begining with /, All path are relative to this file\nlocal_files_folders:\n- \n./files\n\n\nlocal_templates_folders:\n- \n./templates\n\n\n\n\n\nThis part describes the local environment, from where to find the files, which will be pushed on the target clusters.\n\n\nAlso allow defining a template folder, where all the template source files will be stored (Templating is a powerful mechanism for defining configuration files. HADeploy use Ansible template subsystem, based on Jinja2).\n\n\nVars\n\n\nvars:\n  app_version: \n0.1.1\n\n  repository_server: \nmyserver.com\n\n  app_jar_url: \nhttp://${repository_server}/repo/broadapp-${app_version}.jar\n\n  zookeeper:\n    connection_timeout: 60000\n    zkpath: /broadapp\n\n\n\n\nHADeploy allow you to defined variable which then will be substituted by surrounding the variable name with  \"${ \"and \"}\"\n\n\nAnd, as demonstrated by the \napp_jar_url:\n entry, variable value can themself include other variable.\n\n\nNote also than variable can be a scalar (String, int, etc...) but also a map.\n\n\nThere is also a method to provide variable definition on the command line when launching HADeploy. See \nLaunching HADeploy\n below.\n\n\nGroups\n\n\nThis block allows definition of Linux groups.\n\n\ngroups:\n  - scope: all\n    name: broadgroup\n\n\n\n\nHere we create a Linux local group named broadgroup on all hosts of the clusters.\n\n\nSome of the items of HADeploy definition file have a \nscope:\n attribute, which defines the target on which action will be performed. Such attribute may be optional and may have \nall\n as default value. So, the following is equivalent:\n\n\ngroups:\n  - name: broadgroup\n\n\n\n\nUsers\n\n\nThis block will allow us to define Linux account required by the application.\n\n\nusers:\n- login: broadapp\n  comment: \nBroadapp technical account\n\n  groups: \nbroadgroup\n\n\n- { login: broaduser, scope: control_nodes, comment: \nBroadapp user account\n, groups: \nbroadgroup\n,\n      can_sudo_to: \nhdfs, yarn\n }\n\n\n\n\nEach entry will support most of the usual user's configuration parameters. See \nusers\n in the Reference part.\n\n\nAs for groups, we can define on which hosts users will be created using the \nscope:\n attribute.\n\n\nFolders\n\n\nThis block allow us to define the directory used by the application, with all associated permissions\n\n\nfolders:\n- { scope: en1, path: /etc/broadapp, owner: root, group: root, mode: \n0755\n }\n- { scope: en1, path: /opt/broadapp, owner: root, group: root, mode: \n0755\n }\n- { scope: \nen1:data_nodes\n, path: /var/log/broadapp, owner: broadapp, group: broadgroup, mode: \n0755\n }\n\n- { scope: hdfs, path: /apps/broadapp, owner: broadapp, group: broadgroup, mode: \n0755\n }\n\n\n\n\nNote again the \nscope:\n attribute; with allow specifying where the directory will be created:\n\n\nFor a single node of name en1:\n\n\nscope: en1\n\n\n\n\nFor the same en node, plus all hosts belonging to the \ndata_nodes\n group:\n\n\nscope: en1:data_nodes\n\n\n\n\nHere the folder will not be create on one or several hosts, but on the HDFS file system:\n\n\nscope: hdfs\n\n\n\n\nFiles\n\n\nThen the following block will set the application file in place, also with permission.\n\n\nfiles:\n- { scope: en1, src: \n${app_jar_url}\n, dest_folder: \n/opt/broadapp\n,  owner: root, group: root, mode: \n0644\n, validate_certs: no }\n- { scope: en1, src: \ntmpl://broadapp.cfg.j2\n, dest_folder: /etc/broadapp, dest_name: broadapp.cfg, owner: root, group: root, mode: \n0644\n }\n\n- { scope: hdfs, src: \n${app_jar_url}\n, dest_folder: \n/apps/broadapp\n, dest_name: broadapp.jar, owner: broadapp, group: broadgroup, mode: \n0644\n, validate_certs: no }\n\n\n\n\nHere again, the \nscope:\n attribute indicates where to store the file. And the \nsrc:\n attribute is an URI, providing a choice of sources. More on that in the reference part.\n\n\nNote also variable are surrounded by quotes. This is required when using 'flow style', otherwise the opening '{' will be confused when the start of map delimiter. \nSee also \nalternate variable notation\n for non-String variables.\n\n\nTrees\n\n\nHADeploy also provide ability to copy recursively a folder with all its content.\n\n\nThe following definition will copy all the content of the data folder on HDFS, under the path \n/apps/broadapp/init_data\n.\n\n\nAll copied files ownership and permission will be adjusted with the provided values. Also, inner subfolder will be created if any, and ownership and permission also adjusted.\n\n\ntrees:\n- { scope: hdfs, src: \nfile://data\n, dest_folder: \n/apps/broadapp/init_data\n, owner: broadapp, group: broadgroup, file_mode: \n0644\n, folder_mode: \n0755\n }\n\n\n\n\nkafka_topics\n\n\nThis block define a Kafka topic which would be required by the application:\n\n\nkafka_topics:\n- name: broadapp_t1\n  partition_factor: 3\n  replication_factor: 2\n  properties:\n    retention.ms: 630720000000\n    retention.bytes: 858993459200\n\n\n\n\nYou will find here typical parameters of a Kafka topic: name, partition and replication factor, and a set of properties.\n\n\nHbase namespaces and tables\n\n\nhbase_namespaces:\n- name: broadgroup\n\nhbase_tables: \n- name: broadapp_t1\n  namespace: broadgroup\n  properties: \n    regionReplication: 1\n    durability: ASYNC_WAL\n  column_families:\n  - name: cf1\n    properties: \n      maxVersions: 12\n      minVersions: 2\n      timeToLive: 200000\n\n\n\n\nThis block defines one namespace broadgroup and a HBase table named broadapp_t1 in it, with specifics properties and one column family.\n\n\nTemplates\n\n\nNote in the \nfiles:\n part of this sample the second file with \nsrc:\n attribute: \ntmpl://broadapp.cfg.j2\n.\n\n\nfiles:\n...\n- { scope: en1, src: \ntmpl://broadapp.cfg.j2\n, dest_folder: /etc/broadapp, dest_name: broadapp.cfg, owner: root, group: root, mode: \n0644\n }\n\n\n\n\nThe tmpl scheme stands for 'template'. And the corresponding source template will be fetched from one of the folders defined by the \nlocal_templates_folders\n list described previously.\n\n\nNow, let's dig inside this file. Here is what it could look like:\n\n\nBroadapp Config file:\n\n\nzookeeper.connect={% for host in groups['zookeepers'] %}{% if not loop.first %},{% endif %}{{  hostvars[host]['ansible_fqdn'] }}:2181{% endfor %}${zookeeper.zkpath}\n\nzookeeper.connection.timeout.ms={{ zookeeper.connection_timeout }}\n\n\n\n\nHere we see how we can refer to the \nzookeeper.connection_timeout\n variable we defined in a previous \nvars:\n block.\n\n\nThe \nzookeeper.connect\n value is a bit more complex. It is a typical Ansible pattern used to build a zookeeper quorum value. \n\n\nImportant point to note is, in a template, we have access to both variable defined in the HADeploy definition file and to all the variables grabbed by Ansible from remote hosts (Called Facts in Ansible terminology).\n\n\nProject layout\n\n\nHADeploy project layout may be simple. For examples, in our case, it could be:\n\n\napp.yml\nfiles/\ninfra/cluster1.yml\nkeys/build_key\ntemplates/broadapp.cfg.j2\n\n\n\n\nNote the file folder is empty, as we fetch our binary resources from a repository server. We could also define a self-contained project as the following:\n\n\napp.yml\nfiles/broadapp-0.1.1.jar\ninfra/cluster1.yml\nkeys/build_key\ntemplates/broadapp.cfg.j2\n\n\n\n\nThen, the \napp_jar_url\n should be set to\n\n\nvars:\n  app_jar_url: \nfile://broadapp-${app_version}.jar\n\n\n\n\n\nLaunching HADeploy\n\n\nLet's assume we have \ninstalled HADeploy\n and have arranged to have the \n....../hadeploy/bin\n folder in our PATH (See Installation above), and we are in our project directory.\n\n\nTo launch the deployment, just type:\n\n\nhadeploy --src app.yml --src infra/cluster1.yml --action deploy\n\n\n\n\nAnd to perform the reverse action (Fully remove all application from target cluster):\n\n\nhadeploy --src app.yml --src infra/cluster1.yml --action remove\n\n\n\n\nHADeploy command line also allow direct variable definition, with the form --var name=value. For example:\n\n\nhadeploy --var app_version=0.1.2 --src app.yml --src infra/cluster1.yml --action deploy\n\n\n\n\nNote in this case, the value will be overwritten by the one provided in \napp.yml\n. So you will have to remove from the definition file a variable you intend to specify on the command line.\n\n\nThe general rule is than variable definitions are evaluated/overridden in order of which they appears. So the order of --var and --src on the command line is significant.\n\n\nThere is also another option \n--action dumpvars\n, to display all defined variables:\n\n\nhadeploy --var app_version=0.1.2 --src app.yml --src infra/cluster1.yml --action dumpvars\napp_version: 0.1.2\nfile_mode: 640\nfolder_mode: 750\nforceAll: true\nzookeeper:\n  connection_timeout: 60000\n  zkpath: /broadapp\n\n\n\n\nThis can be of great help in case you have a quite complex configuration, whith several level of included files.\n\n\nSome other option of the HADeploy command are described in other chapters:\n\n\n\n\n\n\n--workingFolder\n: Refer to \nUnder_the_hood\n chapter.\n\n\n\n\n\n\n--askVaultPassword\n and \n--vaultPasswordFile\n: Refer to \nEncrypted variables\n\n\n\n\n\n\n--scope\n and \n--noScope\n: Refer to \nAltering scope", 
            "title": "Next step"
        }, 
        {
            "location": "/getting_started/next_step/#next-step", 
            "text": "We will describe here the deployment of a simple application, 'broadapp', which will need to deploy some artifacts and resources on a Hadoop cluster, both on nodes and on HDFS.  We will not deeply enter in the detail of all items. You will find more exhaustive description in the Reference part of this document.   NOTE: This sample is intended to demonstrate the basic features of HADeploy. It is not intended to define 'Best Practices' about Hadoop Application deployment.   This example will be divided in two parts: One describing the target Hadoop cluster (Infrastructure description) and a second one the application deployment itself (Application description)", 
            "title": "Next step"
        }, 
        {
            "location": "/getting_started/next_step/#infrastructure-description", 
            "text": "There is two methods to provide HADeploy with the target infrastructure description. One is to explicitly list all hosts. Another one is to refer to an already existing Ansible Inventory.  Let's start with the first one:", 
            "title": "Infrastructure description"
        }, 
        {
            "location": "/getting_started/next_step/#explicit-description", 
            "text": "The infrastructure is described in a file with the following content:  # ------------------------------------- Infrastructure part\n\nhosts:\n# A first host expressed the standard yaml style\n- name: sr\n  ssh_host: sr.cluster1.mydomain.com\n  ssh_user: root\n  ssh_private_key_file:  keys/build_key  \n# And these others using more consise yaml 'flow style'\n- { name: en1, ssh_host: en.cluster1.mydomain.com, ssh_user: root, ssh_private_key_file:  keys/build_key  }\n- { name: nn1, ssh_host: nn1.cluster1.mydomain.com, ssh_user: root, ssh_private_key_file:  keys/build_key  }\n- { name: nn2, ssh_host: nn2.cluster1.mydomain.com, ssh_user: root, ssh_private_key_file:  keys/build_key  }\n- { name: dn1, ssh_host: dn1.cluster1.mydomain.com, ssh_user: root, ssh_private_key_file:  keys/build_key  }\n- { name: dn2, ssh_host: dn2.cluster1.mydomain.com, ssh_user: root, ssh_private_key_file:  keys/build_key  }\n- { name: dn3, ssh_host: dn3.cluster1.mydomain.com, ssh_user: root, ssh_private_key_file:  keys/build_key  }\n\nhost_groups:\n- name: data_nodes\n  hosts:    # This host list in standart YAML style\n  - dn1\n  - dn2\n  - dn3\n- name: control_nodes\n  hosts: [  sr ,  en ,  nn1 ,  nn2  ]   # And these in YAML 'flow style'\n- name: zookeepers\n  hosts: [  sr ,  nn1 ,  nn2  ]\n\nhdfs_relay:\n  host: en1\n\nkafka_relay:\n  host: br1\n  zookeeper:\n    host_group: zookeepers\n\nhbase_relay:\n  host: en1  First, we will describe the several hosts our cluster is made of. Each host as a name: entry, by which we will reference it, and others entries describing how to access it through ssh.  The specified  ssh_user:  is typically  root , as it must be a user with enough rights to perform all the required actions.  Then we will find a block of  host_groups : description. This will simply allow grouping hosts by function, and easing hosts's reference.  Then is the definition of the  hdfs_relay : specifying which host will support all HDFS related operations.  Then is the definition of the  kafka_relay : which will be relaying all topics creation, modification and removal operations. As such it need to know how to access zookeeper and to be able to access Kafka library folder as it use corresponding jar files. The simplest solution is to use a broker node for this task.  And last is the definition of  hbase_relay : which will be relaying all HBase table creation, modification and removal operation.", 
            "title": "Explicit description"
        }, 
        {
            "location": "/getting_started/next_step/#from-an-existing-ansible-inventory", 
            "text": "Those familiar to Ansible should have noted such description is very similar to the way Ansible describe its inventory.  If the target cluster is already described in an Ansible inventory file, then one may simple reference it instead of duplicate information.  In such case, the infrastructure part of the file will look like:  # ------------------------------------- Infrastructure part\n\nansible_inventories:\n- file:  .../some-ansible-build/inventory \n\nhost_groups:\n- name: control_nodes\n  hosts: [  sr1 ,  en1 ,  en2 ,  nn1 ,  nn2  ]\n- name: zookeepers\n  hosts: [  sr1 ,  nn1 ,  nn2  ]\n\nhdfs_relay:\n  host: en1\n\nkafka_relay:\n  host: br1\n  zookeeper:\n    host_group: zookeepers\n\nhbase_relay:\n  host: en1  In our case, we add some  host_groups  as we need them for the following, and there were not defined in the source Ansible inventory  Note we also have the ability to modify only some attributes of one, several or all hosts of this Ansible inventory. See  host_overrides  in the reference part.", 
            "title": "From an existing Ansible inventory."
        }, 
        {
            "location": "/getting_started/next_step/#application-description", 
            "text": "Here is the file describing the application deployment:  # ------------------------------------ Environment part\n# Except if begining with /, All path are relative to this file\nlocal_files_folders:\n-  ./files \n\nlocal_templates_folders:\n-  ./templates \n\n# ------------------------------------- Application part\n\nvars:\n  app_version:  0.1.1 \n  repository_server:  myserver.com \n  app_jar_url:  http://${repository_server}/repo/broadapp-${app_version}.jar \n  zookeeper:\n    connection_timeout: 60000\n    zkpath: /broadapp\n\n\ngroups:\n  - scope: all\n    name: broadgroup\n\nusers:\n- login: broadapp\n  comment:  Broadapp technical account \n  groups:  broadgroup \n\n- { login: broaduser, scope: control_nodes, comment:  Broadapp user account , groups:  broadgroup , can_sudo_to:  hdfs, yarn  }\n\nfolders:\n- { scope:  en1:data_nodes , path: /var/log/broadapp, owner: broadapp, group: broadgroup, mode:  0755  }\n- { scope: en1, path: /etc/broadapp, owner: root, group: root, mode:  0755  }\n- { scope: en1, path: /opt/broadapp, owner: root, group: root, mode:  0755  }\n\n- { scope: hdfs, path: /apps/broadapp, owner: broadapp, group: broadgroup, mode:  0755  }\n\nfiles:\n- { scope: en1, src:  ${app_jar_url} , dest_folder:  /opt/broadapp ,  owner: root, group: root, mode:  0644 , validate_certs: no }\n- { scope: en1, src:  tmpl://broadapp.cfg.j2 , dest_folder: /etc/broadapp, dest_name: broadapp.cfg, owner: root, group: root, mode:  0644  }\n\n- { scope: hdfs, src:  ${app_jar_url} , dest_folder:  /apps/broadapp , dest_name: broadapp.jar, owner: broadapp, group: broadgroup, mode:  0644 , validate_certs: no }\n\ntrees:\n- { scope: hdfs, src:  file://data , dest_folder:  /apps/broadapp/init_data , owner: broadapp, group: broadgroup, file_mode:  0644 , folder_mode:  0755  }\n\n\nkafka_topics:\n- name: broadapp_t1\n  partition_factor: 3\n  replication_factor: 2\n  properties:\n    retention.ms: 630720000000\n    retention.bytes: 858993459200\n\nhbase_namespaces:\n- name: broadgroup\n\nhbase_tables: \n- name: broadapp_t1\n  namespace: broadgroup\n  properties: \n    regionReplication: 1\n    durability: ASYNC_WAL\n  column_families:\n  - name: cf1\n    properties: \n      maxVersions: 12\n      minVersions: 2\n      timeToLive: 200000  Let's describe it part by part:", 
            "title": "Application description"
        }, 
        {
            "location": "/getting_started/next_step/#environment-part", 
            "text": "# ------------------------------------ Environment part\n# Except if begining with /, All path are relative to this file\nlocal_files_folders:\n-  ./files \n\nlocal_templates_folders:\n-  ./templates   This part describes the local environment, from where to find the files, which will be pushed on the target clusters.  Also allow defining a template folder, where all the template source files will be stored (Templating is a powerful mechanism for defining configuration files. HADeploy use Ansible template subsystem, based on Jinja2).", 
            "title": "Environment part"
        }, 
        {
            "location": "/getting_started/next_step/#vars", 
            "text": "vars:\n  app_version:  0.1.1 \n  repository_server:  myserver.com \n  app_jar_url:  http://${repository_server}/repo/broadapp-${app_version}.jar \n  zookeeper:\n    connection_timeout: 60000\n    zkpath: /broadapp  HADeploy allow you to defined variable which then will be substituted by surrounding the variable name with  \"${ \"and \"}\"  And, as demonstrated by the  app_jar_url:  entry, variable value can themself include other variable.  Note also than variable can be a scalar (String, int, etc...) but also a map.  There is also a method to provide variable definition on the command line when launching HADeploy. See  Launching HADeploy  below.", 
            "title": "Vars"
        }, 
        {
            "location": "/getting_started/next_step/#groups", 
            "text": "This block allows definition of Linux groups.  groups:\n  - scope: all\n    name: broadgroup  Here we create a Linux local group named broadgroup on all hosts of the clusters.  Some of the items of HADeploy definition file have a  scope:  attribute, which defines the target on which action will be performed. Such attribute may be optional and may have  all  as default value. So, the following is equivalent:  groups:\n  - name: broadgroup", 
            "title": "Groups"
        }, 
        {
            "location": "/getting_started/next_step/#users", 
            "text": "This block will allow us to define Linux account required by the application.  users:\n- login: broadapp\n  comment:  Broadapp technical account \n  groups:  broadgroup \n\n- { login: broaduser, scope: control_nodes, comment:  Broadapp user account , groups:  broadgroup ,\n      can_sudo_to:  hdfs, yarn  }  Each entry will support most of the usual user's configuration parameters. See  users  in the Reference part.  As for groups, we can define on which hosts users will be created using the  scope:  attribute.", 
            "title": "Users"
        }, 
        {
            "location": "/getting_started/next_step/#folders", 
            "text": "This block allow us to define the directory used by the application, with all associated permissions  folders:\n- { scope: en1, path: /etc/broadapp, owner: root, group: root, mode:  0755  }\n- { scope: en1, path: /opt/broadapp, owner: root, group: root, mode:  0755  }\n- { scope:  en1:data_nodes , path: /var/log/broadapp, owner: broadapp, group: broadgroup, mode:  0755  }\n\n- { scope: hdfs, path: /apps/broadapp, owner: broadapp, group: broadgroup, mode:  0755  }  Note again the  scope:  attribute; with allow specifying where the directory will be created:  For a single node of name en1:  scope: en1  For the same en node, plus all hosts belonging to the  data_nodes  group:  scope: en1:data_nodes  Here the folder will not be create on one or several hosts, but on the HDFS file system:  scope: hdfs", 
            "title": "Folders"
        }, 
        {
            "location": "/getting_started/next_step/#files", 
            "text": "Then the following block will set the application file in place, also with permission.  files:\n- { scope: en1, src:  ${app_jar_url} , dest_folder:  /opt/broadapp ,  owner: root, group: root, mode:  0644 , validate_certs: no }\n- { scope: en1, src:  tmpl://broadapp.cfg.j2 , dest_folder: /etc/broadapp, dest_name: broadapp.cfg, owner: root, group: root, mode:  0644  }\n\n- { scope: hdfs, src:  ${app_jar_url} , dest_folder:  /apps/broadapp , dest_name: broadapp.jar, owner: broadapp, group: broadgroup, mode:  0644 , validate_certs: no }  Here again, the  scope:  attribute indicates where to store the file. And the  src:  attribute is an URI, providing a choice of sources. More on that in the reference part.  Note also variable are surrounded by quotes. This is required when using 'flow style', otherwise the opening '{' will be confused when the start of map delimiter. \nSee also  alternate variable notation  for non-String variables.", 
            "title": "Files"
        }, 
        {
            "location": "/getting_started/next_step/#trees", 
            "text": "HADeploy also provide ability to copy recursively a folder with all its content.  The following definition will copy all the content of the data folder on HDFS, under the path  /apps/broadapp/init_data .  All copied files ownership and permission will be adjusted with the provided values. Also, inner subfolder will be created if any, and ownership and permission also adjusted.  trees:\n- { scope: hdfs, src:  file://data , dest_folder:  /apps/broadapp/init_data , owner: broadapp, group: broadgroup, file_mode:  0644 , folder_mode:  0755  }", 
            "title": "Trees"
        }, 
        {
            "location": "/getting_started/next_step/#kafka_topics", 
            "text": "This block define a Kafka topic which would be required by the application:  kafka_topics:\n- name: broadapp_t1\n  partition_factor: 3\n  replication_factor: 2\n  properties:\n    retention.ms: 630720000000\n    retention.bytes: 858993459200  You will find here typical parameters of a Kafka topic: name, partition and replication factor, and a set of properties.", 
            "title": "kafka_topics"
        }, 
        {
            "location": "/getting_started/next_step/#hbase-namespaces-and-tables", 
            "text": "hbase_namespaces:\n- name: broadgroup\n\nhbase_tables: \n- name: broadapp_t1\n  namespace: broadgroup\n  properties: \n    regionReplication: 1\n    durability: ASYNC_WAL\n  column_families:\n  - name: cf1\n    properties: \n      maxVersions: 12\n      minVersions: 2\n      timeToLive: 200000  This block defines one namespace broadgroup and a HBase table named broadapp_t1 in it, with specifics properties and one column family.", 
            "title": "Hbase namespaces and tables"
        }, 
        {
            "location": "/getting_started/next_step/#templates", 
            "text": "Note in the  files:  part of this sample the second file with  src:  attribute:  tmpl://broadapp.cfg.j2 .  files:\n...\n- { scope: en1, src:  tmpl://broadapp.cfg.j2 , dest_folder: /etc/broadapp, dest_name: broadapp.cfg, owner: root, group: root, mode:  0644  }  The tmpl scheme stands for 'template'. And the corresponding source template will be fetched from one of the folders defined by the  local_templates_folders  list described previously.  Now, let's dig inside this file. Here is what it could look like:  Broadapp Config file:  zookeeper.connect={% for host in groups['zookeepers'] %}{% if not loop.first %},{% endif %}{{  hostvars[host]['ansible_fqdn'] }}:2181{% endfor %}${zookeeper.zkpath}\n\nzookeeper.connection.timeout.ms={{ zookeeper.connection_timeout }}  Here we see how we can refer to the  zookeeper.connection_timeout  variable we defined in a previous  vars:  block.  The  zookeeper.connect  value is a bit more complex. It is a typical Ansible pattern used to build a zookeeper quorum value.   Important point to note is, in a template, we have access to both variable defined in the HADeploy definition file and to all the variables grabbed by Ansible from remote hosts (Called Facts in Ansible terminology).", 
            "title": "Templates"
        }, 
        {
            "location": "/getting_started/next_step/#project-layout", 
            "text": "HADeploy project layout may be simple. For examples, in our case, it could be:  app.yml\nfiles/\ninfra/cluster1.yml\nkeys/build_key\ntemplates/broadapp.cfg.j2  Note the file folder is empty, as we fetch our binary resources from a repository server. We could also define a self-contained project as the following:  app.yml\nfiles/broadapp-0.1.1.jar\ninfra/cluster1.yml\nkeys/build_key\ntemplates/broadapp.cfg.j2  Then, the  app_jar_url  should be set to  vars:\n  app_jar_url:  file://broadapp-${app_version}.jar", 
            "title": "Project layout"
        }, 
        {
            "location": "/getting_started/next_step/#launching-hadeploy", 
            "text": "Let's assume we have  installed HADeploy  and have arranged to have the  ....../hadeploy/bin  folder in our PATH (See Installation above), and we are in our project directory.  To launch the deployment, just type:  hadeploy --src app.yml --src infra/cluster1.yml --action deploy  And to perform the reverse action (Fully remove all application from target cluster):  hadeploy --src app.yml --src infra/cluster1.yml --action remove  HADeploy command line also allow direct variable definition, with the form --var name=value. For example:  hadeploy --var app_version=0.1.2 --src app.yml --src infra/cluster1.yml --action deploy  Note in this case, the value will be overwritten by the one provided in  app.yml . So you will have to remove from the definition file a variable you intend to specify on the command line.  The general rule is than variable definitions are evaluated/overridden in order of which they appears. So the order of --var and --src on the command line is significant.  There is also another option  --action dumpvars , to display all defined variables:  hadeploy --var app_version=0.1.2 --src app.yml --src infra/cluster1.yml --action dumpvars\napp_version: 0.1.2\nfile_mode: 640\nfolder_mode: 750\nforceAll: true\nzookeeper:\n  connection_timeout: 60000\n  zkpath: /broadapp  This can be of great help in case you have a quite complex configuration, whith several level of included files.  Some other option of the HADeploy command are described in other chapters:    --workingFolder : Refer to  Under_the_hood  chapter.    --askVaultPassword  and  --vaultPasswordFile : Refer to  Encrypted variables    --scope  and  --noScope : Refer to  Altering scope", 
            "title": "Launching HADeploy"
        }, 
        {
            "location": "/plugins_reference/core/encrypted_vars/", 
            "text": "encrypted_vars\n\n\nSynopsis\n\n\nHAdeploy allow most of the variable to be encrypted. This feature rely on the \nAnsible Vault\n mechanism.\n\n\nAll encrypted values must be set under one or several \nencrypted_vars\n token, and their usage must follow a specific pattern. See the example below.\n\n\nAttributes\n\n\nAttributes are variables name.\n\n\nExample\n\n\nA typical use case of encryption is to protect the\nssh_password\n in a host definition. Encryption can be achieved by provided the values as in the following sample:\n\n\n\nencrypted_vars:\n  john_password: |\n    $ANSIBLE_VAULT;1.1;AES256\n    65626166653134326137613232323336373139393036383532333863623630363662303531306539\n    6637306363343836376633353439656634613638643031660a636238323663353337313333663438\n    30306234306463626338663637623563393735653237323833323064316561653237393538303762\n    6363326232656461310a656631386135663764386565366566633537633665646562626236393462\n    6231\n\n- name: sr\n  ssh_host: sr.cluster1.mydomain.com\n  ssh_user: root\n  ssh_password: \n{{john_password}}\n\n\n\n\n\n\nNote the way the variable is provided to the ranger_password attribute: \n\"{{john_password}}\"\n (And not the usual \n${john_password}\n). This form is mandatory, \nas the variable resolution must be performed by Ansible, not by HADeploy. See \nVariables\n for more info.\n\n\nNB: As the encrypted value is directly provided to Ansible, which will decrypt it in memory, HADeploy itself does not perform any decryption. So, there is no risk to have a decrypted, clear value in some intermediate file.\n\n\nUsing this pattern, most of the values of type string can be encrypted in the deployment file.\n\n\nAnother use case of encryption is to protect the ranger admin password in the \nranger_relay\n definition:\n\n\nencrypted_vars:\n  ranger_password: |\n    $ANSIBLE_VAULT;1.1;AES256\n    34396662613462623565323936616330623661623065343033646136643635653430636238613962\n    3537343131346462343138343064313937646366363435340a633532366162623838376436366362\n    61393033343932303636653066336130616132383463373934396265306364363562613565613165\n    6163613739303430650a356136353865623534643237646166393230613933396166663963633538\n    3664\n\nranger_relay:\n  host: en1\n  ranger_url:  https://ranger.mycluster.mycompany.com:6182\n  ranger_username: admin\n  ranger_password: \n{{ranger_password}}\n  \n  ca_bundle_local_file: cert/ranger_mycluster_cert.pem\n  ca_bundle_relay_file: /etc/security/certs/ranger_mycluster_cert.pem\n\n\n\n\nEncrypting a value\n\n\nHADeploy encryption rely on the Ansible Vault capability. So, the encryption will be performed using \nansible-vault\n command.\n\n\nHere is a simple approach to achieve this:\n\n\nFirst, create a temporary file containing only the password (Here, the password is \nadmin\n):\n\n\necho -n admin \n/tmp/data.txt\n\n\n\n\nIt is important to ensure there is no leading or trailing control character, or white space:\n\n\nhexdump -C /tmp/data.txt\n00000000  61 64 6d 69 6e                                    |admin|\n00000005\n\n\n\n\nThen, you can encrypt it, using the following command:\n\n\nansible-vault encrypt \n/tmp/data.txt\nNew Vault password:\nConfirm New Vault password:\n$ANSIBLE_VAULT;1.1;AES256\n36303764663465323835653063393330393363656263356332383930363039303662663530653561\n3365366637386139333030306638633739653332336363380a623833646435393466386531616230\n36396536633064663736643931313464366166663062663165333362656262626638343532393538\n6562643836373164620a653835383665356233643835613066653261333561333533356638303963\n3266\nEncryption successful\n\n\n\n\nYou will need to provide a Vault password. This is the password you will have to provided later, on each launch of HADeploy.\n\n\nNow, you may cut and paste the result as your \nencrypted_vars.ranger_password\n value, as shown on the top of this page. And be sure:\n\n\n\n\nIndentation is the same for all lines.\n\n\nIndentation is only made of space (No tab).\n\n\nThere is no space, or white space at the end of the line.\n\n\n\n\nIf you don't follow these recommendation, we may have some cryptic error messages, such as:\n\n\nfatal: [en1]: FAILED! =\n {\nfailed\n: true, \nmsg\n: \nUnexpected templating type error occurred on ({{ rangerPassword }}): Non-hexadecimal digit found\n}\n\n\n\n\nAnd, of course, don't forget to cleanup the file which contains the password in clear text.\n\n\nrm /tmp/data.txt\n\n\n\n\nLaunching HADeploy with encrypted values\n\n\n\n\nDo not mistake this feature with the vault password you may need to provide when accessing an Ansible inventory (\nsee here\n). \nThere is no relationshipt between these two passwords. They act at different level.\n\n\n\n\nIf you launch HADeploy on file containing encrypted value, you will need to provide a password. Otherwise you will have an error like the following: \n\n\nThe offending line appears to be:\n\n  vars:\n      rangerPassword: !vault |\n                      ^ here\n\n\n\n\nFirst approach is to enter this password on each launch. For this, simply add the option \n--askVaultPassword\n on the command line.\n\n\nhadeploy --src infra/mycluster.yml --src app.yml --askVaultPassword  --action DEPLOY\n....\nVault password:\n\n\n\n\nAnother approach is to provide this password in a file. The password must be a string stored as a single line of the file.\n\n\nThen use the option \n--vaultPasswordFile\n to provide the path on this file:\n\n\nhadeploy --src infra/mycluster.yml --src app.yml --vaultPasswordFile infra/vault_password.txt  --action DEPLOY\n\n\n\n\nEnsure permissions on the file are such that no one else can access your key and do not add this file to source control.", 
            "title": "encrypted_vars"
        }, 
        {
            "location": "/plugins_reference/core/encrypted_vars/#encrypted_vars", 
            "text": "", 
            "title": "encrypted_vars"
        }, 
        {
            "location": "/plugins_reference/core/encrypted_vars/#synopsis", 
            "text": "HAdeploy allow most of the variable to be encrypted. This feature rely on the  Ansible Vault  mechanism.  All encrypted values must be set under one or several  encrypted_vars  token, and their usage must follow a specific pattern. See the example below.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/core/encrypted_vars/#attributes", 
            "text": "Attributes are variables name.", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/core/encrypted_vars/#example", 
            "text": "A typical use case of encryption is to protect the ssh_password  in a host definition. Encryption can be achieved by provided the values as in the following sample:  \nencrypted_vars:\n  john_password: |\n    $ANSIBLE_VAULT;1.1;AES256\n    65626166653134326137613232323336373139393036383532333863623630363662303531306539\n    6637306363343836376633353439656634613638643031660a636238323663353337313333663438\n    30306234306463626338663637623563393735653237323833323064316561653237393538303762\n    6363326232656461310a656631386135663764386565366566633537633665646562626236393462\n    6231\n\n- name: sr\n  ssh_host: sr.cluster1.mydomain.com\n  ssh_user: root\n  ssh_password:  {{john_password}}   Note the way the variable is provided to the ranger_password attribute:  \"{{john_password}}\"  (And not the usual  ${john_password} ). This form is mandatory, \nas the variable resolution must be performed by Ansible, not by HADeploy. See  Variables  for more info.  NB: As the encrypted value is directly provided to Ansible, which will decrypt it in memory, HADeploy itself does not perform any decryption. So, there is no risk to have a decrypted, clear value in some intermediate file.  Using this pattern, most of the values of type string can be encrypted in the deployment file.  Another use case of encryption is to protect the ranger admin password in the  ranger_relay  definition:  encrypted_vars:\n  ranger_password: |\n    $ANSIBLE_VAULT;1.1;AES256\n    34396662613462623565323936616330623661623065343033646136643635653430636238613962\n    3537343131346462343138343064313937646366363435340a633532366162623838376436366362\n    61393033343932303636653066336130616132383463373934396265306364363562613565613165\n    6163613739303430650a356136353865623534643237646166393230613933396166663963633538\n    3664\n\nranger_relay:\n  host: en1\n  ranger_url:  https://ranger.mycluster.mycompany.com:6182\n  ranger_username: admin\n  ranger_password:  {{ranger_password}}   \n  ca_bundle_local_file: cert/ranger_mycluster_cert.pem\n  ca_bundle_relay_file: /etc/security/certs/ranger_mycluster_cert.pem", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/core/encrypted_vars/#encrypting-a-value", 
            "text": "HADeploy encryption rely on the Ansible Vault capability. So, the encryption will be performed using  ansible-vault  command.  Here is a simple approach to achieve this:  First, create a temporary file containing only the password (Here, the password is  admin ):  echo -n admin  /tmp/data.txt  It is important to ensure there is no leading or trailing control character, or white space:  hexdump -C /tmp/data.txt\n00000000  61 64 6d 69 6e                                    |admin|\n00000005  Then, you can encrypt it, using the following command:  ansible-vault encrypt  /tmp/data.txt\nNew Vault password:\nConfirm New Vault password:\n$ANSIBLE_VAULT;1.1;AES256\n36303764663465323835653063393330393363656263356332383930363039303662663530653561\n3365366637386139333030306638633739653332336363380a623833646435393466386531616230\n36396536633064663736643931313464366166663062663165333362656262626638343532393538\n6562643836373164620a653835383665356233643835613066653261333561333533356638303963\n3266\nEncryption successful  You will need to provide a Vault password. This is the password you will have to provided later, on each launch of HADeploy.  Now, you may cut and paste the result as your  encrypted_vars.ranger_password  value, as shown on the top of this page. And be sure:   Indentation is the same for all lines.  Indentation is only made of space (No tab).  There is no space, or white space at the end of the line.   If you don't follow these recommendation, we may have some cryptic error messages, such as:  fatal: [en1]: FAILED! =  { failed : true,  msg :  Unexpected templating type error occurred on ({{ rangerPassword }}): Non-hexadecimal digit found }  And, of course, don't forget to cleanup the file which contains the password in clear text.  rm /tmp/data.txt", 
            "title": "Encrypting a value"
        }, 
        {
            "location": "/plugins_reference/core/encrypted_vars/#launching-hadeploy-with-encrypted-values", 
            "text": "Do not mistake this feature with the vault password you may need to provide when accessing an Ansible inventory ( see here ). \nThere is no relationshipt between these two passwords. They act at different level.   If you launch HADeploy on file containing encrypted value, you will need to provide a password. Otherwise you will have an error like the following:   The offending line appears to be:\n\n  vars:\n      rangerPassword: !vault |\n                      ^ here  First approach is to enter this password on each launch. For this, simply add the option  --askVaultPassword  on the command line.  hadeploy --src infra/mycluster.yml --src app.yml --askVaultPassword  --action DEPLOY\n....\nVault password:  Another approach is to provide this password in a file. The password must be a string stored as a single line of the file.  Then use the option  --vaultPasswordFile  to provide the path on this file:  hadeploy --src infra/mycluster.yml --src app.yml --vaultPasswordFile infra/vault_password.txt  --action DEPLOY  Ensure permissions on the file are such that no one else can access your key and do not add this file to source control.", 
            "title": "Launching HADeploy with encrypted values"
        }, 
        {
            "location": "/plugins_reference/core/excluded_scopes/", 
            "text": "excluded_scopes\n\n\nSynopsis\n\n\nAllow definition of a list of 'scope' which will be excluded of the processing.\n\n\nSame information can also be provided on the command line, using the \n--noScope\n option. In such case, the command line provided scope(s) will added to this list.\n\n\nMore information on this feature in a \nspecific chapter\n\n\nExample\n\n\nexcluded_scopes:\n- ranger \n\n\n\n\nAll ranger related operations will not be performed. This may be used to be inserted in the infrastructure fragment of a cluster without Ranger.", 
            "title": "excluded_scopes"
        }, 
        {
            "location": "/plugins_reference/core/excluded_scopes/#excluded_scopes", 
            "text": "", 
            "title": "excluded_scopes"
        }, 
        {
            "location": "/plugins_reference/core/excluded_scopes/#synopsis", 
            "text": "Allow definition of a list of 'scope' which will be excluded of the processing.  Same information can also be provided on the command line, using the  --noScope  option. In such case, the command line provided scope(s) will added to this list.  More information on this feature in a  specific chapter", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/core/excluded_scopes/#example", 
            "text": "excluded_scopes:\n- ranger   All ranger related operations will not be performed. This may be used to be inserted in the infrastructure fragment of a cluster without Ranger.", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/core/include/", 
            "text": "include\n\n\nSynopsis\n\n\nAllow inclusion of source file.\n\n\nAttributes\n\n\nCould be a list of source file to include, or a simple source file name.\n\n\nExample\n\n\ninclude: infra.yml\ninclude: app.yml\n\n\n\n\nor\n\n\ninclude:\n- infra.yml\n- app.yml\n\n\n\n\nIf paths are not absolute, they will be relative to the HADeploy embedding file location.\n\n\nThe included file name can also be build from variables: \n\n\nvars:\n  env: dev\n....\n\ninclude: ./envs/${env}.yml", 
            "title": "include"
        }, 
        {
            "location": "/plugins_reference/core/include/#include", 
            "text": "", 
            "title": "include"
        }, 
        {
            "location": "/plugins_reference/core/include/#synopsis", 
            "text": "Allow inclusion of source file.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/core/include/#attributes", 
            "text": "Could be a list of source file to include, or a simple source file name.", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/core/include/#example", 
            "text": "include: infra.yml\ninclude: app.yml  or  include:\n- infra.yml\n- app.yml  If paths are not absolute, they will be relative to the HADeploy embedding file location.  The included file name can also be build from variables:   vars:\n  env: dev\n....\n\ninclude: ./envs/${env}.yml", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/core/included_scopes/", 
            "text": "included_scopes\n\n\nSynopsis\n\n\nAllow definition of a list of 'scope' which will be the only ones included in the processing.\n\n\nAn empty, or unexisting list will be interpreted as 'all scopes', this full processing.\n\n\nSame information can also be provided on the command line, using the \n--scope\n option. In such case, the command line provided scope(s) will fully override this value.\nIn other terms, only the command line provided scopes will be included in the processing. \n\n\nMore information on this feature in a \nspecific chapter\n\n\nExample\n\n\nincluded_scopes:\n- hbase\n- hive \n\n\n\n\nOnly HBase and Hive related operations will be performed.", 
            "title": "included_scopes"
        }, 
        {
            "location": "/plugins_reference/core/included_scopes/#included_scopes", 
            "text": "", 
            "title": "included_scopes"
        }, 
        {
            "location": "/plugins_reference/core/included_scopes/#synopsis", 
            "text": "Allow definition of a list of 'scope' which will be the only ones included in the processing.  An empty, or unexisting list will be interpreted as 'all scopes', this full processing.  Same information can also be provided on the command line, using the  --scope  option. In such case, the command line provided scope(s) will fully override this value.\nIn other terms, only the command line provided scopes will be included in the processing.   More information on this feature in a  specific chapter", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/core/included_scopes/#example", 
            "text": "included_scopes:\n- hbase\n- hive   Only HBase and Hive related operations will be performed.", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/core/vars/", 
            "text": "vars\n\n\nSynopsis\n\n\nHADeploy allow you to defined variable which then will be substituted by surrounding the variable name with  \"${ \"and \"}\" (or '\n' and '\n', see below).\n\n\nThere is also a method to provide variable definition on the command line when launching HADeploy. See \nLaunching HADeploy\n.\n\n\nAttributes\n\n\nAttributes are variables name.\n\n\nAs demonstrated by the \napp_jar_url:\n in example below, entry, variable value can themself include other variable.\n\n\nNote also than variable can be a scalar (String, int, etc...) but also a map.\n\n\nExample\n\n\nvars:\n  app_version: \n0.1.1\n\n  repository_server: \nmyserver.com\n\n  app_jar_url: \nhttp://${repository_server}/repo/broadapp-${app_version}.jar\n\n  zookeeper:\n    connection_timeout: 60000\n    zkpath: /broadapp\n\n\n\n\nAlternate notation\n\n\nHADeploy added an alternate notation for variable by using '\n' and '\n'. This notation is mainly intended to be used in 'flow style' description. For example:\n\n\nfiles:\n- {  when: \nisKerberos\n, scope: all, dest_folder: \n/etc/security/keytabs\n, src= \n${appUser}.keytab\n owner: \n${appUser}\n, group: broadgroup, mode: \n0400\n }\n\n\n\n\nIf we where using \nwhen: ${isKerberos}\n, this will generate an error as the opening \n{\n will be confused with the start of the map delimiter.\n\n\nThe solution could be to write \nwhen: \"${isKerberos}\"\n. This will be parsed correctly, but the side effect will be to consider the variable as a String, \nnot as a Boolean. Thus generating an error in this case (\nwhen:\n require a boolean).\n\n\nSo, to allow usage of non-String variable in 'flow style' notation, the \n....\n delimiters as been implemented. Of course, as in the previous example, both notation can be used simultaneously.", 
            "title": "vars"
        }, 
        {
            "location": "/plugins_reference/core/vars/#vars", 
            "text": "", 
            "title": "vars"
        }, 
        {
            "location": "/plugins_reference/core/vars/#synopsis", 
            "text": "HADeploy allow you to defined variable which then will be substituted by surrounding the variable name with  \"${ \"and \"}\" (or ' ' and ' ', see below).  There is also a method to provide variable definition on the command line when launching HADeploy. See  Launching HADeploy .", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/core/vars/#attributes", 
            "text": "Attributes are variables name.  As demonstrated by the  app_jar_url:  in example below, entry, variable value can themself include other variable.  Note also than variable can be a scalar (String, int, etc...) but also a map.", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/core/vars/#example", 
            "text": "vars:\n  app_version:  0.1.1 \n  repository_server:  myserver.com \n  app_jar_url:  http://${repository_server}/repo/broadapp-${app_version}.jar \n  zookeeper:\n    connection_timeout: 60000\n    zkpath: /broadapp", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/core/vars/#alternate-notation", 
            "text": "HADeploy added an alternate notation for variable by using ' ' and ' '. This notation is mainly intended to be used in 'flow style' description. For example:  files:\n- {  when:  isKerberos , scope: all, dest_folder:  /etc/security/keytabs , src=  ${appUser}.keytab  owner:  ${appUser} , group: broadgroup, mode:  0400  }  If we where using  when: ${isKerberos} , this will generate an error as the opening  {  will be confused with the start of the map delimiter.  The solution could be to write  when: \"${isKerberos}\" . This will be parsed correctly, but the side effect will be to consider the variable as a String, \nnot as a Boolean. Thus generating an error in this case ( when:  require a boolean).  So, to allow usage of non-String variable in 'flow style' notation, the  ....  delimiters as been implemented. Of course, as in the previous example, both notation can be used simultaneously.", 
            "title": "Alternate notation"
        }, 
        {
            "location": "/plugins_reference/ansible/ansible_overview/", 
            "text": "ansible plugin: Overview\n\n\nAim of this plugin is to allow integration of ansible playbook in a deployment. \n\n\nThis can be useful to handle some resources not (yet) managed by HADeploy, or to perform some very specifics operation.\n\n\nOf course, you should be familiar with Ansible to eficiently use this plugin. And you may also be familiar with some internal HADeploy concept, such as \nexecution order\n, \nif you have some dependencies between elements, and \nvariable resolution\n, if you don't want everything to be hard-coded in your playbook\n\n\nLet's comment a small example:\n\n\nansible_playbooks:\n- for_action: deploy\n  priority: 1500\n  playbook_text: |\n    - hosts: dn1\n      vars:\n         kdescribe_rpm_url: https://github.com/Kappaware/kdescribe/releases/download/v0.2.0/kdescribe-0.2.0-1.noarch.rpm\n      roles:\n      - kdescribe\n\n\n\n\nHere, we defined the playbook directly in file. For this, we use the Yaml block scalar mode, introduced by \n|\n. Take care of indentation.\n\n\nFor this example, we launch an ansible role \nkdescribe\n, \nwhich install an \nrpm package of the same name\n, and configure it.\n\n\nThis role need some variable to be set, as the package url. Note this variable could also be set outside the playbook:\n\n\nvars:\n  kdescribe_rpm_url: https://github.com/Kappaware/kdescribe/releases/download/v0.2.0/kdescribe-0.2.0-1.noarch.rpm\n\nansible_playbooks:\n- for_action: deploy\n  priority: 1500\n  playbook_text: |\n    - hosts: dn1\n      roles:\n      - kdescribe\n\n\n\n\nActually, All HADeploy variable are also Ansible variable. See \nvariable resolution\n\n\nAnother point is where Ansible will find this \nkdescribe\n role? \n\n\nor this, HADeploy provide a method to add some entries to the Ansible role path. For example:\n\n\nroles_folders: \n- ../roles\n\n\n\n\nBut an Ansible playbook also allow definition of individual task. For example, there is a \nkdescribe\n role for installation and configuration. But not for removal. To do so, we can add:\n\n\n- for_action: remove\n  priority: 3000\n  playbook_text: |\n    - hosts: dn1\n      tasks:\n      - name: \nRemove kdescribe\n\n        yum: name=kdescribe state=absent\n\n\n\n\n\n\nAll examples provided here perform package \nyum\n management. As, so, they need to be executed with the \nssh_user\n set as \nroot\n for the target host. Note the is a constraint of this sample, not of the \nansible\n plugin.\n\n\n\n\nAction and priority\n\n\nAs we can see, a playbook is associated to an action. For example, the first example playbook will be involved only when HADeploy will be launched  with \n--action deploy\n, as stated by the \nfor_action\n attribute.\n\n\nAlso, for this action, a \npriority\n is defined. This will allow to control when the playbook will be executed, regarding other plugins. More information on this in \nExecution order\n\n\nVariables\n\n\nThe playbook itself can contain some variable, such as in the following example:\n\n\nvars:\n  kdescribe_rpm_url: https://github.com/Kappaware/kdescribe/releases/download/v0.2.0/kdescribe-0.2.0-1.noarch.rpm\n  tools_target: dn1\n\nansible_playbooks:\n- for_action: deploy\n  priority: 1500\n  playbook_text: |\n    - hosts: ${tools_target}\n      roles:\n      - kdescribe \n\n\n\n\nNote than you can also play with Ansible variable. HADeploy will not interprets them and pass through to Ansible. For example, this will works as intended:\n\n\n- for_action: remove\n  priority: 1500\n  playbook_text: |\n    - hosts: ${tools_target}\n      tasks:\n      - name: Remove tools\n        yum: name={{item}} state=absent\n        with_items:\n        - kdescribe\n        - ktail\n\n\n\n\nPlaybook in files\n\n\nIf you playbook is more than a couple of line or if you want it to be run for several target, it could be more convenient to define it in a file, and provide this file reference to HADeploy.\n\n\n- for_action: remove\n  priority: 3000\n  playbook_file: remove_kdescribe.yml\n\n\n\n\nThe file may contains:\n\n\n# ------------------------------------ Remove kdescribe.yml\n\n- hosts: {{{src.vars.tools_target}}}\n  tasks:\n  - name: \nRemove kdescribe\n\n    yum: name=kdescribe state=absent\n\n\n\n\nNote the way we access the \ntools_target\n variable. Explanation of this syntax can be found \nhere\n. \n\n\nThe key point is this file is a snippet which will be aggregated with many other to built the overall playbook, as described by \nstep 5 of the processing\n \n\n\nAnother aspect is where to store these playbook files? As usual, a specific entry will provide a list of folders where HADeply will lookup your file:\n\n\nplaybooks_folders: \n- ../playbooks\n\n\n\n\nImplements new actions\n\n\nHADeploy is currently designed to achieve 2 actions: \ndeploy\n and \nremove\n.\n\n\nWith introduction of this \nansible\n plugin, we introduced the fact target action can be defined explicitly. This is extended to the fact new action could also be defined.\n\n\nFor example, we can consider tools like \nkdescribe\n are not part of application deployment by itself, but of an auxiliary tooling deployment. So, we can define:\n\n\nansible_playbooks:\n- for_action: deployTools\n  priority: 1500\n  playbook_text: |\n    - hosts: ${tools_target}\n      vars:\n        kdescribe_rpm_url: https://github.com/Kappaware/kdescribe/releases/download/v0.2.0/kdescribe-0.2.0-1.noarch.rpm\n      roles:\n      - kdescribe\n- for_action: removeTools\n  priority: 1500\n  playbook_text: |\n    - hosts: ${tools_target}\n      tasks:\n      - name: \nRemove kdescribe\n\n        yum: name=kdescribe state=absent\n\n\n\n\nDoing so, we have introduced two new actions, \ndeployTools\n and \nremoveTools\n. Then we can provide them on the command line:\n\n\nhadeploy --src app.yml --src infra.yml --action deployTools", 
            "title": "Overview"
        }, 
        {
            "location": "/plugins_reference/ansible/ansible_overview/#ansible-plugin-overview", 
            "text": "Aim of this plugin is to allow integration of ansible playbook in a deployment.   This can be useful to handle some resources not (yet) managed by HADeploy, or to perform some very specifics operation.  Of course, you should be familiar with Ansible to eficiently use this plugin. And you may also be familiar with some internal HADeploy concept, such as  execution order , \nif you have some dependencies between elements, and  variable resolution , if you don't want everything to be hard-coded in your playbook  Let's comment a small example:  ansible_playbooks:\n- for_action: deploy\n  priority: 1500\n  playbook_text: |\n    - hosts: dn1\n      vars:\n         kdescribe_rpm_url: https://github.com/Kappaware/kdescribe/releases/download/v0.2.0/kdescribe-0.2.0-1.noarch.rpm\n      roles:\n      - kdescribe  Here, we defined the playbook directly in file. For this, we use the Yaml block scalar mode, introduced by  | . Take care of indentation.  For this example, we launch an ansible role  kdescribe , \nwhich install an  rpm package of the same name , and configure it.  This role need some variable to be set, as the package url. Note this variable could also be set outside the playbook:  vars:\n  kdescribe_rpm_url: https://github.com/Kappaware/kdescribe/releases/download/v0.2.0/kdescribe-0.2.0-1.noarch.rpm\n\nansible_playbooks:\n- for_action: deploy\n  priority: 1500\n  playbook_text: |\n    - hosts: dn1\n      roles:\n      - kdescribe  Actually, All HADeploy variable are also Ansible variable. See  variable resolution  Another point is where Ansible will find this  kdescribe  role?   or this, HADeploy provide a method to add some entries to the Ansible role path. For example:  roles_folders: \n- ../roles  But an Ansible playbook also allow definition of individual task. For example, there is a  kdescribe  role for installation and configuration. But not for removal. To do so, we can add:  - for_action: remove\n  priority: 3000\n  playbook_text: |\n    - hosts: dn1\n      tasks:\n      - name:  Remove kdescribe \n        yum: name=kdescribe state=absent   All examples provided here perform package  yum  management. As, so, they need to be executed with the  ssh_user  set as  root  for the target host. Note the is a constraint of this sample, not of the  ansible  plugin.", 
            "title": "ansible plugin: Overview"
        }, 
        {
            "location": "/plugins_reference/ansible/ansible_overview/#action-and-priority", 
            "text": "As we can see, a playbook is associated to an action. For example, the first example playbook will be involved only when HADeploy will be launched  with  --action deploy , as stated by the  for_action  attribute.  Also, for this action, a  priority  is defined. This will allow to control when the playbook will be executed, regarding other plugins. More information on this in  Execution order", 
            "title": "Action and priority"
        }, 
        {
            "location": "/plugins_reference/ansible/ansible_overview/#variables", 
            "text": "The playbook itself can contain some variable, such as in the following example:  vars:\n  kdescribe_rpm_url: https://github.com/Kappaware/kdescribe/releases/download/v0.2.0/kdescribe-0.2.0-1.noarch.rpm\n  tools_target: dn1\n\nansible_playbooks:\n- for_action: deploy\n  priority: 1500\n  playbook_text: |\n    - hosts: ${tools_target}\n      roles:\n      - kdescribe   Note than you can also play with Ansible variable. HADeploy will not interprets them and pass through to Ansible. For example, this will works as intended:  - for_action: remove\n  priority: 1500\n  playbook_text: |\n    - hosts: ${tools_target}\n      tasks:\n      - name: Remove tools\n        yum: name={{item}} state=absent\n        with_items:\n        - kdescribe\n        - ktail", 
            "title": "Variables"
        }, 
        {
            "location": "/plugins_reference/ansible/ansible_overview/#playbook-in-files", 
            "text": "If you playbook is more than a couple of line or if you want it to be run for several target, it could be more convenient to define it in a file, and provide this file reference to HADeploy.  - for_action: remove\n  priority: 3000\n  playbook_file: remove_kdescribe.yml  The file may contains:  # ------------------------------------ Remove kdescribe.yml\n\n- hosts: {{{src.vars.tools_target}}}\n  tasks:\n  - name:  Remove kdescribe \n    yum: name=kdescribe state=absent  Note the way we access the  tools_target  variable. Explanation of this syntax can be found  here .   The key point is this file is a snippet which will be aggregated with many other to built the overall playbook, as described by  step 5 of the processing    Another aspect is where to store these playbook files? As usual, a specific entry will provide a list of folders where HADeply will lookup your file:  playbooks_folders: \n- ../playbooks", 
            "title": "Playbook in files"
        }, 
        {
            "location": "/plugins_reference/ansible/ansible_overview/#implements-new-actions", 
            "text": "HADeploy is currently designed to achieve 2 actions:  deploy  and  remove .  With introduction of this  ansible  plugin, we introduced the fact target action can be defined explicitly. This is extended to the fact new action could also be defined.  For example, we can consider tools like  kdescribe  are not part of application deployment by itself, but of an auxiliary tooling deployment. So, we can define:  ansible_playbooks:\n- for_action: deployTools\n  priority: 1500\n  playbook_text: |\n    - hosts: ${tools_target}\n      vars:\n        kdescribe_rpm_url: https://github.com/Kappaware/kdescribe/releases/download/v0.2.0/kdescribe-0.2.0-1.noarch.rpm\n      roles:\n      - kdescribe\n- for_action: removeTools\n  priority: 1500\n  playbook_text: |\n    - hosts: ${tools_target}\n      tasks:\n      - name:  Remove kdescribe \n        yum: name=kdescribe state=absent  Doing so, we have introduced two new actions,  deployTools  and  removeTools . Then we can provide them on the command line:  hadeploy --src app.yml --src infra.yml --action deployTools", 
            "title": "Implements new actions"
        }, 
        {
            "location": "/plugins_reference/ansible/ansible_playbooks/", 
            "text": "ansible_playbooks\n\n\nSynopsis\n\n\nProvide a list of Ansible playbooks which will be inserted in the overall deployment/removal processing.\n\n\nRefer to \noverview\n for more information.\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nfor_action\n\n\nyes\n\n\nThis is the target action this playbook will be involved for. In must cases, will be \ndeploy\n or \nremove\n.\n\n\n\n\n\n\npriority\n\n\nyes\n\n\nThis is the priority value to control when the playbook will be executed, regarding all other plugins. More information on this in \nExecution order\n\n\n\n\n\n\nplaybook_text\n\n\nyes if \nplaybook_file\nis not defined\n\n\nThe playbook itself, in a Yaml block. See the example below.\n\n\n\n\n\n\nplaybook_file\n\n\nyes if \nplaybook_text\nis not defined\n\n\nThe name of a file hosting the playbook. This file will be searched in the folder list defined by \nplaybooks_folders\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nExample\n\n\nansible_playbooks:\n- for_action: deploy\n  priority: 5000\n  playbook_text: |\n    - hosts: edge_nodes\n      tasks:\n      - name: run this command and ignore the result\n        shell: /usr/bin/somecommand\n        ignore_errors: True\n- for_action: deploy\n  priority: 5000\n  playbook_file: my_playbook.yml", 
            "title": "ansible_playbooks"
        }, 
        {
            "location": "/plugins_reference/ansible/ansible_playbooks/#ansible_playbooks", 
            "text": "", 
            "title": "ansible_playbooks"
        }, 
        {
            "location": "/plugins_reference/ansible/ansible_playbooks/#synopsis", 
            "text": "Provide a list of Ansible playbooks which will be inserted in the overall deployment/removal processing.  Refer to  overview  for more information.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/ansible/ansible_playbooks/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      for_action  yes  This is the target action this playbook will be involved for. In must cases, will be  deploy  or  remove .    priority  yes  This is the priority value to control when the playbook will be executed, regarding all other plugins. More information on this in  Execution order    playbook_text  yes if  playbook_file is not defined  The playbook itself, in a Yaml block. See the example below.    playbook_file  yes if  playbook_text is not defined  The name of a file hosting the playbook. This file will be searched in the folder list defined by  playbooks_folders    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/ansible/ansible_playbooks/#example", 
            "text": "ansible_playbooks:\n- for_action: deploy\n  priority: 5000\n  playbook_text: |\n    - hosts: edge_nodes\n      tasks:\n      - name: run this command and ignore the result\n        shell: /usr/bin/somecommand\n        ignore_errors: True\n- for_action: deploy\n  priority: 5000\n  playbook_file: my_playbook.yml", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/ansible/playbooks_folders/", 
            "text": "playbooks_folders\n\n\nSynopsis\n\n\nAllow definition of a list of local folder where HADeploy will lookup playbook files defined in \nansible_playbooks\n entry\n\n\nRefer to \noverview\n for more information.\n\n\nAttributes\n\n\nA list of local folder path.\n\n\nIf path is not absolute, it will be relative to the HADeploy embedding file location.\n\n\nExample\n\n\nplaybooks_folders: \n- ../playbooks", 
            "title": "playbooks_folders"
        }, 
        {
            "location": "/plugins_reference/ansible/playbooks_folders/#playbooks_folders", 
            "text": "", 
            "title": "playbooks_folders"
        }, 
        {
            "location": "/plugins_reference/ansible/playbooks_folders/#synopsis", 
            "text": "Allow definition of a list of local folder where HADeploy will lookup playbook files defined in  ansible_playbooks  entry  Refer to  overview  for more information.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/ansible/playbooks_folders/#attributes", 
            "text": "A list of local folder path.  If path is not absolute, it will be relative to the HADeploy embedding file location.", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/ansible/playbooks_folders/#example", 
            "text": "playbooks_folders: \n- ../playbooks", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/ansible/roles_folders/", 
            "text": "roles_folders\n\n\nSynopsis\n\n\nAllow definition of a list of local folder where HADeploy will lookup roles used in playbooks defined in \nansible_playbooks\n entry\n\n\nRefer to \noverview\n for more information.\n\n\nAttributes\n\n\nA list of local folder path.\n\n\nIf path is not absolute, it will be relative to the HADeploy embedding file location.\n\n\nExample\n\n\n\nroles_folders: \n- ../roles", 
            "title": "roles_folders"
        }, 
        {
            "location": "/plugins_reference/ansible/roles_folders/#roles_folders", 
            "text": "", 
            "title": "roles_folders"
        }, 
        {
            "location": "/plugins_reference/ansible/roles_folders/#synopsis", 
            "text": "Allow definition of a list of local folder where HADeploy will lookup roles used in playbooks defined in  ansible_playbooks  entry  Refer to  overview  for more information.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/ansible/roles_folders/#attributes", 
            "text": "A list of local folder path.  If path is not absolute, it will be relative to the HADeploy embedding file location.", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/ansible/roles_folders/#example", 
            "text": "roles_folders: \n- ../roles", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/ansible_inventories/ansible_inventories/", 
            "text": "ansible_inventories\n\n\nSynopsis\n\n\nProvide a list of Ansible Inventory.\n\n\nThese inventories will be parsed, eventually merged with locally defined \nhosts\n and \nhost_groups\n and will allow target Ansible inventory file generation.\n\n\nAlso, most of the parameters may be modified by using \nhost_overrides\n and \nhost_group_overrides\n. \n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nfile\n\n\nyes\n\n\nThe Ansible inventory file path. If this path is not absolute, it will be relative to the HADeploy embedding file location.\n\n\n\n\n\n\nvault_password_file\n\n\nno\n\n\nIf this inventory host one or several encrypted files, one must provide a password for decryption. One method is to provide this password in a file.\nThe password must be a string stored as a single line of the file.\nIf this path is not absolute, it will be relative to the HADeploy embedding file location.\nIt can also be stored in the home folder of the sshd_user by using the ~/... notation.\nIn all cases, ensure permissions on the file are such that no one else can access your key and do not add your this file to source control.\n\n\n\n\n\n\nask_vault_password\n\n\nno\n\n\nBoolean. Another method to provide this password is to set this switch on. In this case, the user will be prompted to enter the password on each run.\n\n\n\n\n\n\nname\n\n\nno\n\n\nAllow to provide a name to this inventory.\nUseful if this list contains several entries which require a password. This name will be inserted in the prompt for the user.\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nExample\n\n\n# This simplest case, with a single inventory\nansible_inventories:\n- file: \n.../some-ansible-folder/inventory\n\n\n\n# Build our own inventory from two Ansible inventories. And request user password with decorated prompt\nansible_inventories:\n- name: \ninv1\n \n  files: \n.../some-ansible-folder/inventory\n\n  ask_vault_password: yes\n- name: \ninv2\n  \n  file: \n.../another-ansible-folder/inventory\n\n  ask_vault_password: yes\n\n\n\n\n\nInventory merging\n\n\nIf a host with same name is defined both in \nhosts\n and in an Ansible inventory, the one from the \nhosts\n list will take precedence. \nThis is same for the \nhost_groups\n. \n\n\nNote also a \nhost_groups\n  can refer to a host in Ansible inventory.", 
            "title": "ansible_inventories"
        }, 
        {
            "location": "/plugins_reference/ansible_inventories/ansible_inventories/#ansible_inventories", 
            "text": "", 
            "title": "ansible_inventories"
        }, 
        {
            "location": "/plugins_reference/ansible_inventories/ansible_inventories/#synopsis", 
            "text": "Provide a list of Ansible Inventory.  These inventories will be parsed, eventually merged with locally defined  hosts  and  host_groups  and will allow target Ansible inventory file generation.  Also, most of the parameters may be modified by using  host_overrides  and  host_group_overrides .", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/ansible_inventories/ansible_inventories/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      file  yes  The Ansible inventory file path. If this path is not absolute, it will be relative to the HADeploy embedding file location.    vault_password_file  no  If this inventory host one or several encrypted files, one must provide a password for decryption. One method is to provide this password in a file. The password must be a string stored as a single line of the file. If this path is not absolute, it will be relative to the HADeploy embedding file location. It can also be stored in the home folder of the sshd_user by using the ~/... notation. In all cases, ensure permissions on the file are such that no one else can access your key and do not add your this file to source control.    ask_vault_password  no  Boolean. Another method to provide this password is to set this switch on. In this case, the user will be prompted to enter the password on each run.    name  no  Allow to provide a name to this inventory. Useful if this list contains several entries which require a password. This name will be inserted in the prompt for the user.    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/ansible_inventories/ansible_inventories/#example", 
            "text": "# This simplest case, with a single inventory\nansible_inventories:\n- file:  .../some-ansible-folder/inventory \n\n\n# Build our own inventory from two Ansible inventories. And request user password with decorated prompt\nansible_inventories:\n- name:  inv1  \n  files:  .../some-ansible-folder/inventory \n  ask_vault_password: yes\n- name:  inv2   \n  file:  .../another-ansible-folder/inventory \n  ask_vault_password: yes", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/ansible_inventories/ansible_inventories/#inventory-merging", 
            "text": "If a host with same name is defined both in  hosts  and in an Ansible inventory, the one from the  hosts  list will take precedence. \nThis is same for the  host_groups .   Note also a  host_groups   can refer to a host in Ansible inventory.", 
            "title": "Inventory merging"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_indices/", 
            "text": "elasticsearch_indices\n\n\nSynopsis\n\n\nAllow definition of a list of Elasticsearch indices.\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nserver\n\n\nyes\n\n\nThe server this index will be created on. Refer to the logical name attribute of an \nelasticsearch_servers\n list item.\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe name of this index.\n\n\n\n\n\n\ndefinition\n\n\nyes\n\n\nIndex definition as a json string or as a YAML definition. See examples\n\n\n\n\n\n\nrecreate\n\n\nno\n\n\nBoolean: Will trigger Index destruction before recreating it. WARNING: All data will be lost!. Use with care.\nDefault: \nno\n.\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this index to be removed when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nExample\n\n\nIn the following, the definition is provided as a JSON text:\n\n\nelasticsearch_indices:\n- server: elastic1\n  name: test1a\n  definition: |\n    {\n        \nsettings\n : {\n            \nindex\n: {\n                \nnumber_of_shards\n : 3,\n                \nnumber_of_replicas\n: 3\n            }\n        },\n        \nmappings\n : {\n            \ntype1\n : {\n                \nproperties\n : {\n                    \nfield1\n : { \ntype\n : \ntext\n }\n                }\n            }\n        }\n    }  \n\n\n\n\nIt also can be provided in 'pure' YAML form:\n\n\nelasticsearch_indices:\n- server: elastic1\n  name: test1a\n  definition:\n    settings:\n      index:\n        number_of_shards: 3\n        number_of_replicas: 3\n    mappings:\n      type1:\n        properties:\n          field1:\n            type: text\n\n\n\n\n\nIdempotency\n\n\nIdempotency is one of the key features of HADeploy. In our case, this means creating an index is not just issuing a PUT on the server. We need to take current server state in account.\n\n\nSo, when required to create an Index, HADeploy will:\n\n\n\n\nFirst, try to retrieve a index of the same name.\n\n\nIf there is no such index, just create it and return successfully. This is the simplest case.\n\n\nIf there is one, compare the existing definition to the provided one. In most case, the existing definition is richer than the one used to create it, \nas Elasticsearch engine populate it with some default value or some other meta-informations. \nSo, this comparison try to figure out if all items provided by our definition are included in the current existing one.\n\n\nIf yes, this means existing index is compliant with our provided definition. So, we can stop processing and return successfully.\n\n\nIf no, the module will try to adjust if possible. For this it can use the \n\nUpdate Indices Settings\n elasticsearch API or the\n\nPut Mapping\n one.\n\n\n\n\nNote that such adjustment have some limitation. For example, one can update the \nnumber_of_replicas\n, but not the \nnumber_of_shards\n. Refer to the links to the API above for more information about what can be updated in an open index.\n\n\nIn the case where index can't be updated to fulfill provided definition, an error is generated. \n\n\nAnother point to take in account is, for the comparison to be effective, the provided definition must be defined as it will be retrieved by the \nGet index\n API.\n\n\nFor example, when retrieving an index, you will get the setting definition like:\n\n\n{\n    \nsettings\n : {\n        \nindex\n : {\n            \nnumber_of_shards\n : 3, \n            \nnumber_of_replicas\n : 2 \n        }\n    }\n}\n\n\n\n\nBut the elastic documentation state than you can also use a simplified form:\n\n\n{\n    \nsettings\n : {\n        \nnumber_of_shards\n : 3,\n        \nnumber_of_replicas\n : 2\n    }\n}\n\n\n\n\nDON'T USE THIS SIMPLIFIED FORM in you index definition. It will mislead comparison thus generating errors.\n\n\nDefault values.\n\n\nAnother common issue is default value handling. For example \n\n\n   {\n        .....\n        \nmappings\n : {\n            \ntype1\n : {\n                \nproperties\n : {\n                    \nfield1\n : { \ntype\n : \ntext\n, \nindex\n: true }\n                }\n            }\n        }\n    }  \n\n\n\n\nWill generate an error. One must write:\n\n\n   {\n        .....\n        \nmappings\n : {\n            \ntype1\n : {\n                \nproperties\n : {\n                    \nfield1\n : { \ntype\n : \ntext\n }\n                }\n            }\n        }\n    }  \n\n\n\n\nThis because this is how the mapping is defined when retrieving index definition by GET API. (\n\"index\": true\n is the default). \n\n\nOf course, if you want to prevent some fields to be indexed, you will write:\n\n\n   {\n        .....\n        \nmappings\n : {\n            \ntype1\n : {\n                \nproperties\n : {\n                    \nfield1\n : { \ntype\n : \ntext\n, \nindex\n: false }\n                }\n            }\n        }\n    }", 
            "title": "elasticsearch_indices"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_indices/#elasticsearch_indices", 
            "text": "", 
            "title": "elasticsearch_indices"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_indices/#synopsis", 
            "text": "Allow definition of a list of Elasticsearch indices.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_indices/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      server  yes  The server this index will be created on. Refer to the logical name attribute of an  elasticsearch_servers  list item.    name  yes  The name of this index.    definition  yes  Index definition as a json string or as a YAML definition. See examples    recreate  no  Boolean: Will trigger Index destruction before recreating it. WARNING: All data will be lost!. Use with care. Default:  no .    no_remove  no  Boolean: Prevent this index to be removed when HADeploy will be used in REMOVE mode. Default:  no    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_indices/#example", 
            "text": "In the following, the definition is provided as a JSON text:  elasticsearch_indices:\n- server: elastic1\n  name: test1a\n  definition: |\n    {\n         settings  : {\n             index : {\n                 number_of_shards  : 3,\n                 number_of_replicas : 3\n            }\n        },\n         mappings  : {\n             type1  : {\n                 properties  : {\n                     field1  : {  type  :  text  }\n                }\n            }\n        }\n    }    It also can be provided in 'pure' YAML form:  elasticsearch_indices:\n- server: elastic1\n  name: test1a\n  definition:\n    settings:\n      index:\n        number_of_shards: 3\n        number_of_replicas: 3\n    mappings:\n      type1:\n        properties:\n          field1:\n            type: text", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_indices/#idempotency", 
            "text": "Idempotency is one of the key features of HADeploy. In our case, this means creating an index is not just issuing a PUT on the server. We need to take current server state in account.  So, when required to create an Index, HADeploy will:   First, try to retrieve a index of the same name.  If there is no such index, just create it and return successfully. This is the simplest case.  If there is one, compare the existing definition to the provided one. In most case, the existing definition is richer than the one used to create it, \nas Elasticsearch engine populate it with some default value or some other meta-informations. \nSo, this comparison try to figure out if all items provided by our definition are included in the current existing one.  If yes, this means existing index is compliant with our provided definition. So, we can stop processing and return successfully.  If no, the module will try to adjust if possible. For this it can use the  Update Indices Settings  elasticsearch API or the Put Mapping  one.   Note that such adjustment have some limitation. For example, one can update the  number_of_replicas , but not the  number_of_shards . Refer to the links to the API above for more information about what can be updated in an open index.  In the case where index can't be updated to fulfill provided definition, an error is generated.   Another point to take in account is, for the comparison to be effective, the provided definition must be defined as it will be retrieved by the  Get index  API.  For example, when retrieving an index, you will get the setting definition like:  {\n     settings  : {\n         index  : {\n             number_of_shards  : 3, \n             number_of_replicas  : 2 \n        }\n    }\n}  But the elastic documentation state than you can also use a simplified form:  {\n     settings  : {\n         number_of_shards  : 3,\n         number_of_replicas  : 2\n    }\n}  DON'T USE THIS SIMPLIFIED FORM in you index definition. It will mislead comparison thus generating errors.", 
            "title": "Idempotency"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_indices/#default-values", 
            "text": "Another common issue is default value handling. For example      {\n        .....\n         mappings  : {\n             type1  : {\n                 properties  : {\n                     field1  : {  type  :  text ,  index : true }\n                }\n            }\n        }\n    }    Will generate an error. One must write:     {\n        .....\n         mappings  : {\n             type1  : {\n                 properties  : {\n                     field1  : {  type  :  text  }\n                }\n            }\n        }\n    }    This because this is how the mapping is defined when retrieving index definition by GET API. ( \"index\": true  is the default).   Of course, if you want to prevent some fields to be indexed, you will write:     {\n        .....\n         mappings  : {\n             type1  : {\n                 properties  : {\n                     field1  : {  type  :  text ,  index : false }\n                }\n            }\n        }\n    }", 
            "title": "Default values."
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_servers/", 
            "text": "elasticsearch_servers\n\n\nSynopsis\n\n\nAllow definition of a list of Elasticsearch servers. These servers are intended to be refenced by \nelasticsearch_indices\n and \nelasticsearch_templates\n items. \n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe logical name given to this server\n\n\n\n\n\n\nrelay_host\n\n\nyes\n\n\nFrom which host are the HTTP requests to elasticsearch server issued.\n\n\n\n\n\n\nurl\n\n\nyes\n\n\nThe base part of the url of the server. Typically: \nhttp://elastic1.myserver.mydomain.com:9200/\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\nusername\n\n\nno\n\n\nThe user name to log on this elasticsearch server. Must have enough rights to perform intended operations.\n\n\n\n\n\n\npassword\n\n\nno\n\n\nThe password associated with the \nusername\n. May be encrypted. Refer to \nencrypted variables\n\n\n\n\n\n\nvalidate_certs\n\n\nno\n\n\nUseful if the connection is using SSL. If no, SSL certificates will not be validated. This should only be used on personally controlled sites using self-signed certificates.\nDefault: \nyes\n\n\n\n\n\n\nca_bundle_relay_file\n\n\nno\n\n\nUseful if the connection is using SSL. Allow to specify a CA_BUNDLE file, a file that contains root and intermediate certificates to validate the elasticsearch server certificate in .pem format.\nThis file will be looked up on the relay host system, on which this module will be executed.\n\n\n\n\n\n\nca_bundle_local_file\n\n\nno\n\n\nSame as above, except this file will be looked up locally, relative to the main file. It will be copied on the relay host at the location defined by \nca_bundle_relay_file\n\n\n\n\n\n\n\n\nExample\n\n\nThe simplest case:\n\n\nelasticsearch_servers:\n- name: elastic1\n  relay_host: en1\n  url: http://elastic1.myserver.mydomain.com:9200/\n\n\n\n\nFor a secured elasticsearch cluster:\n\n\nelasticsearch_servers:\n- name: elastic2\n  relay_host: en1\n  url: https://elastic2.myserver.mydomain.com:9200/\n  validate_certs: false \n  username: elastic\n  password: changeme  \n\n\n\n\nCA_BUNDLE\n\n\nInternally, HADeploy use the python \nrequests\n API to access elasticsearch. The provided \nca_bundle_relay_file\n will be used as the \nverify\n parameter of all HTTP requests. More info  \nhere\n.\n\n\nIf, for encrypting communication with elasticsearch you have generated a Certificate authority with\n\n\nbin/elasticsearch-certutil ca \n\n\n\n\nas described in the \nelastic documentation\n, the following python code will allow you to generate a CA_BUNDE file \nelastic-stack-ca.crt.pem\n.   \n\n\n# Need:\n# sudo yum install pyOpenSSL\nfrom OpenSSL import crypto\n\n# Accept \n for empty password.\nwith open(\nelastic-stack-ca.p12\n, \nrb\n) as file:\n    p12 = crypto.load_pkcs12(file.read(), \ncapassword\n)\n\n# PEM formatted certificate\ncert =  crypto.dump_certificate(crypto.FILETYPE_PEM, p12.get_certificate())\nprint cert\nf = open(\nelastic-stack-ca.crt.pem\n, \nw\n)\nf.write(cert)\nf.close()", 
            "title": "elasticsearch_servers"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_servers/#elasticsearch_servers", 
            "text": "", 
            "title": "elasticsearch_servers"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_servers/#synopsis", 
            "text": "Allow definition of a list of Elasticsearch servers. These servers are intended to be refenced by  elasticsearch_indices  and  elasticsearch_templates  items.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_servers/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  The logical name given to this server    relay_host  yes  From which host are the HTTP requests to elasticsearch server issued.    url  yes  The base part of the url of the server. Typically:  http://elastic1.myserver.mydomain.com:9200/    when  no  Boolean. Allow  conditional deployment  of this item. Default  True    username  no  The user name to log on this elasticsearch server. Must have enough rights to perform intended operations.    password  no  The password associated with the  username . May be encrypted. Refer to  encrypted variables    validate_certs  no  Useful if the connection is using SSL. If no, SSL certificates will not be validated. This should only be used on personally controlled sites using self-signed certificates. Default:  yes    ca_bundle_relay_file  no  Useful if the connection is using SSL. Allow to specify a CA_BUNDLE file, a file that contains root and intermediate certificates to validate the elasticsearch server certificate in .pem format. This file will be looked up on the relay host system, on which this module will be executed.    ca_bundle_local_file  no  Same as above, except this file will be looked up locally, relative to the main file. It will be copied on the relay host at the location defined by  ca_bundle_relay_file", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_servers/#example", 
            "text": "The simplest case:  elasticsearch_servers:\n- name: elastic1\n  relay_host: en1\n  url: http://elastic1.myserver.mydomain.com:9200/  For a secured elasticsearch cluster:  elasticsearch_servers:\n- name: elastic2\n  relay_host: en1\n  url: https://elastic2.myserver.mydomain.com:9200/\n  validate_certs: false \n  username: elastic\n  password: changeme", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_servers/#ca_bundle", 
            "text": "Internally, HADeploy use the python  requests  API to access elasticsearch. The provided  ca_bundle_relay_file  will be used as the  verify  parameter of all HTTP requests. More info   here .  If, for encrypting communication with elasticsearch you have generated a Certificate authority with  bin/elasticsearch-certutil ca   as described in the  elastic documentation , the following python code will allow you to generate a CA_BUNDE file  elastic-stack-ca.crt.pem .     # Need:\n# sudo yum install pyOpenSSL\nfrom OpenSSL import crypto\n\n# Accept   for empty password.\nwith open( elastic-stack-ca.p12 ,  rb ) as file:\n    p12 = crypto.load_pkcs12(file.read(),  capassword )\n\n# PEM formatted certificate\ncert =  crypto.dump_certificate(crypto.FILETYPE_PEM, p12.get_certificate())\nprint cert\nf = open( elastic-stack-ca.crt.pem ,  w )\nf.write(cert)\nf.close()", 
            "title": "CA_BUNDLE"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_templates/", 
            "text": "elasticsearch_templates\n\n\nSynopsis\n\n\nAllow definition of a list of Elasticsearch templates.\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nserver\n\n\nyes\n\n\nThe server this index will be created on. Refer to the logical name attribute of an \nelasticsearch_servers\n list item.\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe name of this index.\n\n\n\n\n\n\ndefinition\n\n\nyes\n\n\nTemplate definition as a json string or as a YAML definition. See examples\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this index to be removed when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nExample\n\n\nIn the following, the definition is provided as a JSON text:\n\n\nelasticsearch_templates:\n- server: elastic1\n  name: testtmpl1a\n  definition: |\n        {\n          \nindex_patterns\n: [\nfoo*\n, \nbar*\n],\n          \nsettings\n: {    \n            \nindex\n: {\n                \nnumber_of_shards\n: 1\n            }\n          },\n          \nmappings\n: {\n            \ntype1\n: {\n              \n_source\n: {\n                \nenabled\n: false\n              },\n              \nproperties\n: {\n                \nhost_name\n: {\n                  \ntype\n: \nkeyword\n\n                },\n                \ncreated_at\n: {\n                  \ntype\n: \ndate\n,\n                  \nformat\n: \nEEE MMM dd HH:mm:ss Z YYYY\n\n                }\n              }\n            }\n          }\n        }      \n\n\n\n\nIt also can be provided in 'pure' YAML form:\n\n\nelasticsearch_templates:\n- server: elastic1\n  name: testtmpl1a\n  definition:\n    index_patterns: \n    - \nfoo*\n\n    - \nbar*\n\n    settings:\n      index:\n        number_of_shards: 1\n    mappings:\n      type1:\n        _source:\n          enabled: True\n        properties:\n          host_name:\n            type: text\n            index: True\n          created_at:\n            type: date\n            format: \nEEE MMM dd HH:mm:ss Z YYYY\n\n\n\n\n\n\nIdempotence\n\n\nFor Templates, there is no idempotence concern, as a template can be recreated at any time, without loosing any data.\n\n\nBut keep in mind template recreation does not affect existing indices.", 
            "title": "elasticsearch_templates"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_templates/#elasticsearch_templates", 
            "text": "", 
            "title": "elasticsearch_templates"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_templates/#synopsis", 
            "text": "Allow definition of a list of Elasticsearch templates.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_templates/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      server  yes  The server this index will be created on. Refer to the logical name attribute of an  elasticsearch_servers  list item.    name  yes  The name of this index.    definition  yes  Template definition as a json string or as a YAML definition. See examples    no_remove  no  Boolean: Prevent this index to be removed when HADeploy will be used in REMOVE mode. Default:  no    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_templates/#example", 
            "text": "In the following, the definition is provided as a JSON text:  elasticsearch_templates:\n- server: elastic1\n  name: testtmpl1a\n  definition: |\n        {\n           index_patterns : [ foo* ,  bar* ],\n           settings : {    \n             index : {\n                 number_of_shards : 1\n            }\n          },\n           mappings : {\n             type1 : {\n               _source : {\n                 enabled : false\n              },\n               properties : {\n                 host_name : {\n                   type :  keyword \n                },\n                 created_at : {\n                   type :  date ,\n                   format :  EEE MMM dd HH:mm:ss Z YYYY \n                }\n              }\n            }\n          }\n        }        It also can be provided in 'pure' YAML form:  elasticsearch_templates:\n- server: elastic1\n  name: testtmpl1a\n  definition:\n    index_patterns: \n    -  foo* \n    -  bar* \n    settings:\n      index:\n        number_of_shards: 1\n    mappings:\n      type1:\n        _source:\n          enabled: True\n        properties:\n          host_name:\n            type: text\n            index: True\n          created_at:\n            type: date\n            format:  EEE MMM dd HH:mm:ss Z YYYY", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/elastic/elasticsearch_templates/#idempotence", 
            "text": "For Templates, there is no idempotence concern, as a template can be recreated at any time, without loosing any data.  But keep in mind template recreation does not affect existing indices.", 
            "title": "Idempotence"
        }, 
        {
            "location": "/plugins_reference/files/files/", 
            "text": "files\n\n\nSynopsis\n\n\nProvide a list of files to be deployed on the cluster.\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsrc\n\n\nyes\n\n\nSource file, in the form \nscheme\n://....\n. See below for the possible \nscheme\n values.\n\n\n\n\n\n\nscope\n\n\nyes\n\n\nOn which target does this file be deployed? May be:\nA single \nhost\n name\nA single \nhost_group\n name\nSeveral \nhosts\n or \nhost_groups\n, separated by the character ':'\nthe \nhdfs\n token\n\n\n\n\n\n\ndest_folder\n\n\nyes\n\n\nTarget folder. Must exists\n\n\n\n\n\n\ndest_name\n\n\nno\n\n\nThe target file name.\nDefault: The basename of src\n\n\n\n\n\n\nowner\n\n\nyes\n\n\nThe owner of the file\n\n\n\n\n\n\ngroup\n\n\nyes\n\n\nThe group of the file\n\n\n\n\n\n\nmode\n\n\nyes\n\n\nThe permission of the file. Must be an octal representation embedded in a string (ie: \"0755\").\n\n\n\n\n\n\nvalidate_certs\n\n\nno\n\n\nBoolean; In case of \nsrc: https://...\n Setting to false, will disable strict certificate checking, thus allowing self-signed certificate.\nDefault: \nyes\n\n\n\n\n\n\nforce_basic_auth\n\n\nno\n\n\nBoolean; In case of \nsrc: http://...\n or \nsrc: https://...\n. The underlying module only sends authentication information when a webservice responds to an initial request with a 401 status. Since some basic auth services do not properly send a 401, logins will fail. This option forces the sending of the Basic authentication header upon initial request.\n\n\n\n\n\n\nurl_username\n\n\nno\n\n\nString; In case of \nsrc: http://...\n or \nsrc: https://...\n. The username for use in HTTP basic authentication. This parameter can be used without url_password for sites that allow empty passwords.\n\n\n\n\n\n\nurl_password\n\n\nno\n\n\nString; In case of \nsrc: http://...\n or \nsrc: https://...\n. The password for use in HTTP basic authentication\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this file to be removed when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\nnotify\n\n\nno\n\n\nList of strings. Allow automatic restart of one or several background tasks if the file is modified.\nCan refer to a \nsystemd_unit\n, a \nsupervisor_program\n  or a \nstorm_topology\n. See \nbelow\n\n\n\n\n\n\nranger_policy\n\n\nno\n\n\nDefinition of Apache Ranger policy bound to this file. \nParameters are same as \nhdfs_ranger_policies\n items, excepts than \npaths\n should not be defined as automatically set to the file path, and the policy is not recursive by default.\nScope must be hdfs.\nThe policy name can be explicitly defined. Otherwise, a name will be generated as \n\"_\ntargetPath\n_\"\n.\nSee example below for more information.\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\n\n\nNB: \nsrc:\n must not reference a folder. To create a folder, use the \nfolders\n definition and to copy a folder content, use the \ntrees\n definition.\n\n\n\n\nSchemes\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nfile://...\n\n\nFor fetching the file locally, from one of the folder provided by the \nlocal_files_folders:\n list.\n\n\n\n\n\n\nfile:///...\n\n\nFor fetching the file locally, with a absolute path on the HADeploy node.\n\n\n\n\n\n\ntmpl://...\n\n\nSource is a template, which will be processed by Ansible/Jinja2 mechanism. Template will be fetched locally, from one of the folders provided by the \nlocal_templates_folders:\n list.\n\n\n\n\n\n\ntmpl:///...\n\n\nSame as above, except source template will be fetched from the HADeploy node with an absolute path.\n\n\n\n\n\n\nhttp://...\n\n\nFor fetching the file from a remote http server.\n\n\n\n\n\n\nhttps://...\n\n\nFor fetching the file from a remote https server.\n\n\n\n\n\n\nnode://\nnode\n/...\n\n\nThis mode is only relevant when scope is hdfs. It allows grabbing a file from one node of the cluster and pushes it to HDFS. Useful when, for example, some application require configuration files from client nodes to be pushed on HDFS.\nPath must be absolute.\nIf kerberos is enabled on the cluster, a source host credential must be provide for the operation to be successful. See \nsource_host_credentials\n definition is this reference part.\n\n\n\n\n\n\nmvn://...\n\n\nFor fetching file from a maven artifact repository. Must be in the form:\nmvn://\nmavenRepositoryName\n/\ngroupId\n/\nartifactId\n/\nversion\n[/\nclassifier\n[/\nextension\n]]\n, where:\nmavenRepositoryName\n is the name of the repository definition in the \nmaven_repositories\n list\ngroupId\n is the artifact's group id.\nartifactId\n is the artifact id.\nversion\n is the artifact version. Or \nlatest\n.\nclassifier\n is an optional classifier, such as \ndocs\n, \nsources\n, ... Default: empty (\n//\n)\nextension\n is an optionnal extention. Default to \njar\n\n\n\n\n\n\n\n\nExample\n\n\nfiles:\n- src: https://my.download.server/repo/myapp/myapp-0.2.2.jar\n  scope: egde_nodes\n  dest_folder: /opt/myapp\n  owner: root\n  group: root\n  mode: \n0644\n\n  validate_certs: no\n\n- scope: hdfs\n  src: \ntmpl://pixo.cfg.j2\n \n  dest_folder: \n/apps/pixo/conf\n\n  dest_name: \npico.cfg\n\n  mode: \n0000\n\n  ranger_policy:\n    permissions:\n    - users:\n      - pixo\n      accesses:\n      - read\n      - write\n\n\n\n\nFectching from a public maven repository:\n\n\nmaven_repositories:\n- name: maven2\n  url: \nhttp://repo1.maven.org/maven2/\n\n\nfiles:  \n- scope: egde_nodes\n  src: \nmvn://maven2/org.slf4j/slf4j-api/1.7.21\n\n  dest_folder: \n/opt/myapp/lib\n \n  owner: root\n  group: root\n  mode: \n0644\n \n\n\n\n\n\nBackground tasks notifications\n\n\nHADeploy is aimed not only to perform initial deployment, but also to cleverly propagate application modification. \n\n\nin particular, file modification are only performed when needed. In some case, such modification need to trigger some other action, such as background process restart. This is the function of the \nnotify\n attribute.\n\n\nThis \nnotify\n attribute is a list of string where each item can be of one of three forms:\n\n\n\n\n\n\n\"systemd://\nsystemd_unit_name\n\"\n\n\n\n\n\n\n\"supervisor://\nsupervisor_name\n/\nprogram_name\n\"\n\n\n\n\n\n\n\"storm://\ntopology_name\n\"\n\n\n\n\n\n\nIn the example below, the jar is shared by two topologies. If updated on the maven repository, it will be pulled on next deployment and the topologies will be automatically restarted.\n\n\nfiles:  \n- scope: egde_nodes\n  src: \nmvn://maven2/mycompany.com/myapp/1.0.0-SNAPSHOT\n\n  dest_folder: \n/opt/myapp/lib\n \n  owner: root\n  group: root\n  mode: \n0644\n \n  notify:\n  - \nstorm://topo1\n\n  - \nstorm://topo2", 
            "title": "files"
        }, 
        {
            "location": "/plugins_reference/files/files/#files", 
            "text": "", 
            "title": "files"
        }, 
        {
            "location": "/plugins_reference/files/files/#synopsis", 
            "text": "Provide a list of files to be deployed on the cluster.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/files/files/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      src  yes  Source file, in the form  scheme ://.... . See below for the possible  scheme  values.    scope  yes  On which target does this file be deployed? May be: A single  host  name A single  host_group  name Several  hosts  or  host_groups , separated by the character ':' the  hdfs  token    dest_folder  yes  Target folder. Must exists    dest_name  no  The target file name. Default: The basename of src    owner  yes  The owner of the file    group  yes  The group of the file    mode  yes  The permission of the file. Must be an octal representation embedded in a string (ie: \"0755\").    validate_certs  no  Boolean; In case of  src: https://...  Setting to false, will disable strict certificate checking, thus allowing self-signed certificate. Default:  yes    force_basic_auth  no  Boolean; In case of  src: http://...  or  src: https://... . The underlying module only sends authentication information when a webservice responds to an initial request with a 401 status. Since some basic auth services do not properly send a 401, logins will fail. This option forces the sending of the Basic authentication header upon initial request.    url_username  no  String; In case of  src: http://...  or  src: https://... . The username for use in HTTP basic authentication. This parameter can be used without url_password for sites that allow empty passwords.    url_password  no  String; In case of  src: http://...  or  src: https://... . The password for use in HTTP basic authentication    no_remove  no  Boolean: Prevent this file to be removed when HADeploy will be used in REMOVE mode. Default:  no    notify  no  List of strings. Allow automatic restart of one or several background tasks if the file is modified. Can refer to a  systemd_unit , a  supervisor_program   or a  storm_topology . See  below    ranger_policy  no  Definition of Apache Ranger policy bound to this file.  Parameters are same as  hdfs_ranger_policies  items, excepts than  paths  should not be defined as automatically set to the file path, and the policy is not recursive by default. Scope must be hdfs. The policy name can be explicitly defined. Otherwise, a name will be generated as  \"_ targetPath _\" . See example below for more information.    when  no  Boolean. Allow  conditional deployment  of this item. Default  True      NB:  src:  must not reference a folder. To create a folder, use the  folders  definition and to copy a folder content, use the  trees  definition.", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/files/files/#schemes", 
            "text": "Name  Description      file://...  For fetching the file locally, from one of the folder provided by the  local_files_folders:  list.    file:///...  For fetching the file locally, with a absolute path on the HADeploy node.    tmpl://...  Source is a template, which will be processed by Ansible/Jinja2 mechanism. Template will be fetched locally, from one of the folders provided by the  local_templates_folders:  list.    tmpl:///...  Same as above, except source template will be fetched from the HADeploy node with an absolute path.    http://...  For fetching the file from a remote http server.    https://...  For fetching the file from a remote https server.    node:// node /...  This mode is only relevant when scope is hdfs. It allows grabbing a file from one node of the cluster and pushes it to HDFS. Useful when, for example, some application require configuration files from client nodes to be pushed on HDFS. Path must be absolute. If kerberos is enabled on the cluster, a source host credential must be provide for the operation to be successful. See  source_host_credentials  definition is this reference part.    mvn://...  For fetching file from a maven artifact repository. Must be in the form: mvn:// mavenRepositoryName / groupId / artifactId / version [/ classifier [/ extension ]] , where: mavenRepositoryName  is the name of the repository definition in the  maven_repositories  list groupId  is the artifact's group id. artifactId  is the artifact id. version  is the artifact version. Or  latest . classifier  is an optional classifier, such as  docs ,  sources , ... Default: empty ( // ) extension  is an optionnal extention. Default to  jar", 
            "title": "Schemes"
        }, 
        {
            "location": "/plugins_reference/files/files/#example", 
            "text": "files:\n- src: https://my.download.server/repo/myapp/myapp-0.2.2.jar\n  scope: egde_nodes\n  dest_folder: /opt/myapp\n  owner: root\n  group: root\n  mode:  0644 \n  validate_certs: no\n\n- scope: hdfs\n  src:  tmpl://pixo.cfg.j2  \n  dest_folder:  /apps/pixo/conf \n  dest_name:  pico.cfg \n  mode:  0000 \n  ranger_policy:\n    permissions:\n    - users:\n      - pixo\n      accesses:\n      - read\n      - write  Fectching from a public maven repository:  maven_repositories:\n- name: maven2\n  url:  http://repo1.maven.org/maven2/ \n\nfiles:  \n- scope: egde_nodes\n  src:  mvn://maven2/org.slf4j/slf4j-api/1.7.21 \n  dest_folder:  /opt/myapp/lib  \n  owner: root\n  group: root\n  mode:  0644", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/files/files/#background-tasks-notifications", 
            "text": "HADeploy is aimed not only to perform initial deployment, but also to cleverly propagate application modification.   in particular, file modification are only performed when needed. In some case, such modification need to trigger some other action, such as background process restart. This is the function of the  notify  attribute.  This  notify  attribute is a list of string where each item can be of one of three forms:    \"systemd:// systemd_unit_name \"    \"supervisor:// supervisor_name / program_name \"    \"storm:// topology_name \"    In the example below, the jar is shared by two topologies. If updated on the maven repository, it will be pulled on next deployment and the topologies will be automatically restarted.  files:  \n- scope: egde_nodes\n  src:  mvn://maven2/mycompany.com/myapp/1.0.0-SNAPSHOT \n  dest_folder:  /opt/myapp/lib  \n  owner: root\n  group: root\n  mode:  0644  \n  notify:\n  -  storm://topo1 \n  -  storm://topo2", 
            "title": "Background tasks notifications"
        }, 
        {
            "location": "/plugins_reference/files/folders/", 
            "text": "folders\n\n\nSynopsis\n\n\nProvided a list of folders to be created and configured on the target infrastructure\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npath\n\n\nyes\n\n\nThe path of the folder to create\n\n\n\n\n\n\nscope\n\n\nyes\n\n\nOn which target does this folder be create? May be:\nA single \nhost\n name\nA single \nhost_group\n name\nSeveral \nhosts\n or \nhost_groups\n, separated by the character ':'\nthe \nhdfs\n token.\n\n\n\n\n\n\nowner\n\n\nyes\n\n\nThe owner of the file\n\n\n\n\n\n\ngroup\n\n\nyes\n\n\nThe group of the file\n\n\n\n\n\n\nmode\n\n\nyes\n\n\nThe permission of the file. Must be an octal representation embedded in a string (ie: \"0755\")\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this folder to be removed when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\nranger_policy\n\n\nno\n\n\nDefinition of Apache Ranger policy bound to this folder. Parameters are same as \nhdfs_ranger_policies\n excepts than \npaths\n should not be defined as is automatically set to the folder path. Scope must be hdfs.\nThe policy name can be explicitly defined. Otherwise, a name will be generated as \"\n_\npath\n_\n\".\nSee example below for more information\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nExample\n\n\nfolders:\n- path: /var/log/myapp\n  scope: all\n  owner: myapp\n  group: myapp\n  model: \n0755\n\n\n- path: /apps/myapp\n  scope: hdfs\n  owner: myapp\n  group: myapp\n  model: \n0755\n\n  ranger_policy:\n  permissions:\n  - groups:\n    - group1\n    accesses:\n    - read\n    - execute", 
            "title": "folders"
        }, 
        {
            "location": "/plugins_reference/files/folders/#folders", 
            "text": "", 
            "title": "folders"
        }, 
        {
            "location": "/plugins_reference/files/folders/#synopsis", 
            "text": "Provided a list of folders to be created and configured on the target infrastructure", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/files/folders/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      path  yes  The path of the folder to create    scope  yes  On which target does this folder be create? May be: A single  host  name A single  host_group  name Several  hosts  or  host_groups , separated by the character ':' the  hdfs  token.    owner  yes  The owner of the file    group  yes  The group of the file    mode  yes  The permission of the file. Must be an octal representation embedded in a string (ie: \"0755\")    no_remove  no  Boolean: Prevent this folder to be removed when HADeploy will be used in REMOVE mode. Default:  no    ranger_policy  no  Definition of Apache Ranger policy bound to this folder. Parameters are same as  hdfs_ranger_policies  excepts than  paths  should not be defined as is automatically set to the folder path. Scope must be hdfs. The policy name can be explicitly defined. Otherwise, a name will be generated as \" _ path _ \". See example below for more information    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/files/folders/#example", 
            "text": "folders:\n- path: /var/log/myapp\n  scope: all\n  owner: myapp\n  group: myapp\n  model:  0755 \n\n- path: /apps/myapp\n  scope: hdfs\n  owner: myapp\n  group: myapp\n  model:  0755 \n  ranger_policy:\n  permissions:\n  - groups:\n    - group1\n    accesses:\n    - read\n    - execute", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/files/maven_repositories/", 
            "text": "maven_repositories\n\n\nSynopsis\n\n\nProvide a list maven repositories to fetch artifact from\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nLocal name of this repository. Will be used for reference in a \nfile\n  with \nsrc: \"mvn//\nrepoName\n/...\"\n\n\n\n\n\n\nurl\n\n\nno\n\n\nThe default URL of the Maven Repository to download from.\n\n\n\n\n\n\nsnapshots_url\n\n\nno\n\n\nThe URL of the Maven Repository to download from. This URL will be used if the artifact's version contains the token \nSNAPSHOT\n.\nDefault to \nurl\n.\n\n\n\n\n\n\nlatest_url\n\n\nno\n\n\nThe URL of the Maven Repository to download from. This URL will be used if the artifact's version is the token \nlatest\n.\nDefault to \nurl\n.\n\n\n\n\n\n\nreleases_url\n\n\nno\n\n\nThe URL of the Maven Repository to download from. This URL will be used if the artifact's version is not a \nSNAPSHOT\n or \nlatest\n.\nDefault to \nurl\n.\n\n\n\n\n\n\nusername\n\n\nno\n\n\nThe username to authenticate as to the Maven Repository, in case of access control.\n\n\n\n\n\n\npassword\n\n\nno\n\n\nAssociated password\n\n\n\n\n\n\ntimeout\n\n\nno\n\n\nSpecifies a timeout in seconds for the connection attempt. Default: \n10\n\n\n\n\n\n\nvalidate_certs\n\n\nno\n\n\nBoolean; In case of \nsrc: https://...\n Setting to false, will disable strict certificate checking, thus allowing self-signed certificate.\nDefault: \nyes\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nExample\n\n\nThe traditionnal public maven repository:\n\n\nmaven_repositories:\n- name: maven2\n  url: \nhttp://repo1.maven.org/maven2/\n\n\n\n\n\nA local, private repository, requiring user authentication, and accessed using SSL, but with an invalid certificate:\n\n\n- name: nexus\n  snapshots_url: https://nexus_server.local/nexus/content/repositories/snapshots/\n  releases_url: https://nexus_server.local/nexus/content/repositories/releases/\n  latest_url: https://nexus_server.local/nexus/content/repositories/releases/\n  username: john\n  password: aNicePassword\n  validate_certs: no\n\n\n\n\nNote we have defined \nlatest_url\n to the 'releases' part. This means we intend to fetch the 'latest stable' release in this case.\n\n\nAnd here, we also adopt the good practice of encrypting the password.\n\n\n\nencrypted_vars:\n  john_password: |\n    $ANSIBLE_VAULT;1.1;AES256\n    65626166653134326137613232323336373139393036383532333863623630363662303531306539\n    6637306363343836376633353439656634613638643031660a636238323663353337313333663438\n    30306234306463626338663637623563393735653237323833323064316561653237393538303762\n    6363326232656461310a656631386135663764386565366566633537633665646562626236393462\n    6231\n\n\n- name: nexus\n  snapshots_url: https://nexus_server.local/nexus/content/repositories/snapshots/\n  releases_url: https://nexus_server.local/nexus/content/repositories/releases/\n  latest_url: https://nexus_server.local/nexus/content/repositories/snapshots/\n  username: john\n  password: \n{{john_password}}\n\n  validate_certs: no\n\n\n\n\nRefer to \nencrypted variables\n for more information.\n\n\nNote also, in this last case, we are fetching 'latest' versions from the 'snapshots' part.", 
            "title": "maven_repositories"
        }, 
        {
            "location": "/plugins_reference/files/maven_repositories/#maven_repositories", 
            "text": "", 
            "title": "maven_repositories"
        }, 
        {
            "location": "/plugins_reference/files/maven_repositories/#synopsis", 
            "text": "Provide a list maven repositories to fetch artifact from", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/files/maven_repositories/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  Local name of this repository. Will be used for reference in a  file   with  src: \"mvn// repoName /...\"    url  no  The default URL of the Maven Repository to download from.    snapshots_url  no  The URL of the Maven Repository to download from. This URL will be used if the artifact's version contains the token  SNAPSHOT . Default to  url .    latest_url  no  The URL of the Maven Repository to download from. This URL will be used if the artifact's version is the token  latest . Default to  url .    releases_url  no  The URL of the Maven Repository to download from. This URL will be used if the artifact's version is not a  SNAPSHOT  or  latest . Default to  url .    username  no  The username to authenticate as to the Maven Repository, in case of access control.    password  no  Associated password    timeout  no  Specifies a timeout in seconds for the connection attempt. Default:  10    validate_certs  no  Boolean; In case of  src: https://...  Setting to false, will disable strict certificate checking, thus allowing self-signed certificate. Default:  yes    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/files/maven_repositories/#example", 
            "text": "The traditionnal public maven repository:  maven_repositories:\n- name: maven2\n  url:  http://repo1.maven.org/maven2/   A local, private repository, requiring user authentication, and accessed using SSL, but with an invalid certificate:  - name: nexus\n  snapshots_url: https://nexus_server.local/nexus/content/repositories/snapshots/\n  releases_url: https://nexus_server.local/nexus/content/repositories/releases/\n  latest_url: https://nexus_server.local/nexus/content/repositories/releases/\n  username: john\n  password: aNicePassword\n  validate_certs: no  Note we have defined  latest_url  to the 'releases' part. This means we intend to fetch the 'latest stable' release in this case.  And here, we also adopt the good practice of encrypting the password.  \nencrypted_vars:\n  john_password: |\n    $ANSIBLE_VAULT;1.1;AES256\n    65626166653134326137613232323336373139393036383532333863623630363662303531306539\n    6637306363343836376633353439656634613638643031660a636238323663353337313333663438\n    30306234306463626338663637623563393735653237323833323064316561653237393538303762\n    6363326232656461310a656631386135663764386565366566633537633665646562626236393462\n    6231\n\n\n- name: nexus\n  snapshots_url: https://nexus_server.local/nexus/content/repositories/snapshots/\n  releases_url: https://nexus_server.local/nexus/content/repositories/releases/\n  latest_url: https://nexus_server.local/nexus/content/repositories/snapshots/\n  username: john\n  password:  {{john_password}} \n  validate_certs: no  Refer to  encrypted variables  for more information.  Note also, in this last case, we are fetching 'latest' versions from the 'snapshots' part.", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/files/local_files_folders/", 
            "text": "local_files_folders\n\n\nSynopsis\n\n\nAllow definition of a list of local folder where HADeploy will lookup local files to be deployed on the target clusters.\n\n\nAttributes\n\n\nA list of local folder path.\n\n\nIf path is not absolute, it will be relative to the HADeploy embedding file location.\n\n\nExample\n\n\nlocal_files_folders:\n- \n./files", 
            "title": "local_files_folders"
        }, 
        {
            "location": "/plugins_reference/files/local_files_folders/#local_files_folders", 
            "text": "", 
            "title": "local_files_folders"
        }, 
        {
            "location": "/plugins_reference/files/local_files_folders/#synopsis", 
            "text": "Allow definition of a list of local folder where HADeploy will lookup local files to be deployed on the target clusters.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/files/local_files_folders/#attributes", 
            "text": "A list of local folder path.  If path is not absolute, it will be relative to the HADeploy embedding file location.", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/files/local_files_folders/#example", 
            "text": "local_files_folders:\n-  ./files", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/files/local_templates_folders/", 
            "text": "local_templates_folders\n\n\nSynopsis\n\n\nAllow definition of a list of local folder where HADeploy will lookup local templates to be deployed on the target clusters.\n\n\nAttributes\n\n\nA list of local folder path.\n\n\nIf path is not absolute, it will be relative to the HADeploy embedding file location.\n\n\nExample\n\n\nlocal_templates_folders:\n- \n./templates", 
            "title": "local_templates_folders"
        }, 
        {
            "location": "/plugins_reference/files/local_templates_folders/#local_templates_folders", 
            "text": "", 
            "title": "local_templates_folders"
        }, 
        {
            "location": "/plugins_reference/files/local_templates_folders/#synopsis", 
            "text": "Allow definition of a list of local folder where HADeploy will lookup local templates to be deployed on the target clusters.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/files/local_templates_folders/#attributes", 
            "text": "A list of local folder path.  If path is not absolute, it will be relative to the HADeploy embedding file location.", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/files/local_templates_folders/#example", 
            "text": "local_templates_folders:\n-  ./templates", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/files/trees/", 
            "text": "trees\n\n\nSynopsis\n\n\nProvide a list of trees to be deployed on the cluster. \n\n\nA tree is a folder with all its content. It will be recursively copied from the source to the target (Defined by the scope).\n\n\nTarget folder and all subfolder will be created with provided owner, group and folder_mode.\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsrc\n\n\nyes\n\n\nSource folder. May be in the form:\nfile://...\n for fetching the folder content locally, from one of the folder provided by the \nlocal_files_folders:\n list\nfile:///...\n for fetching the folder content locally, with an absolute path on the HADeploy node.\ntmpl://...\n  Source is a folder containing template files, which will be processed by Ansible/Jinja2 mechanism. Template files will be fetched locally, from one of the folders provided by the \nlocal_templates_folders:\n list\ntmpl:///...\n Same as above, except source template folder will be fetched from the HADeploy node with an absolute path.\nnode://\nnode\n/...\n This mode is only relevant when scope is \nhdfs\n. It allows grabbing a folder from one node of the cluster and pushes it to HDFS. Path must be absolute.\n\n\n\n\n\n\nscope\n\n\nyes\n\n\nOn which target does this file be deployed? May be:\nA single \nhost\n name\nA single \nhost_group\n name\nSeveral \nhosts\n or \nhost_groups\n, separated by the character ':'\nthe \nhdfs\n token\n\n\n\n\n\n\ndest_folder\n\n\nyes\n\n\nTarget folder. Will be created on deployment if not existing. Must be an absolute path\nWARNING: this folder will be deleted with all its content in REMOVE mode.\n\n\n\n\n\n\nowner\n\n\nyes\n\n\nThe owner of all the target files\n\n\n\n\n\n\ngroup\n\n\nyes\n\n\nThe group of all the target files\n\n\n\n\n\n\nfile_mode\n\n\nyes\n\n\nThe permission of all the target files. Must be an octal representation embedded in a string (ie: \"0644\")\n\n\n\n\n\n\nfolder_mode\n\n\nyes\n\n\nThe permission of all the target folders. Must be an octal representation embedded in a string (ie: \"0755\")\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this folder to be removed from the target when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\nranger_policy\n\n\nno\n\n\nDefinition of Apache Ranger policy bound to this tree. Parameters are same as \nhdfs_ranger_policies\n excepts than \npaths\n should not be defined as is automatically set to the folder path. Scope must be \nhdfs\n.\nThe policy name can be explicitly defined. Otherwise, a name will be generated as \n\"_\ndest_folder\n_\"\n.\nSee example below for more information.\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\n\n\nsrc: must reference a folder. To copy a single file, use the files definition.\n\n\n\n\nExample\n\n\ntrees:\n- scope: hdfs\n  src: \nfile://data\n\n  dest_folder: \n/apps/broadapp/init_data\n\n  owner: broadapp\n  group: broadgroup\n  file_mode: \n0644\n\n  folder_mode: \n0755\n\n\n- scope: hdfs\n  src: \nfile://mytree\n\n  dest_folder: \n/apps/broadapp/thetree\n\n  file_mode: \n0000\n\n  folder_mode: \n0000\n\n  ranger_policy:\n    audit: no\n    permissions:\n    - users:\n      - broadapp\n      accesses:\n      - read\n      - write\n      - execute\n    - groups:\n      - broadgroup\n      accesses:\n      - read\n      - execute", 
            "title": "trees"
        }, 
        {
            "location": "/plugins_reference/files/trees/#trees", 
            "text": "", 
            "title": "trees"
        }, 
        {
            "location": "/plugins_reference/files/trees/#synopsis", 
            "text": "Provide a list of trees to be deployed on the cluster.   A tree is a folder with all its content. It will be recursively copied from the source to the target (Defined by the scope).  Target folder and all subfolder will be created with provided owner, group and folder_mode.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/files/trees/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      src  yes  Source folder. May be in the form: file://...  for fetching the folder content locally, from one of the folder provided by the  local_files_folders:  list file:///...  for fetching the folder content locally, with an absolute path on the HADeploy node. tmpl://...   Source is a folder containing template files, which will be processed by Ansible/Jinja2 mechanism. Template files will be fetched locally, from one of the folders provided by the  local_templates_folders:  list tmpl:///...  Same as above, except source template folder will be fetched from the HADeploy node with an absolute path. node:// node /...  This mode is only relevant when scope is  hdfs . It allows grabbing a folder from one node of the cluster and pushes it to HDFS. Path must be absolute.    scope  yes  On which target does this file be deployed? May be: A single  host  name A single  host_group  name Several  hosts  or  host_groups , separated by the character ':' the  hdfs  token    dest_folder  yes  Target folder. Will be created on deployment if not existing. Must be an absolute path WARNING: this folder will be deleted with all its content in REMOVE mode.    owner  yes  The owner of all the target files    group  yes  The group of all the target files    file_mode  yes  The permission of all the target files. Must be an octal representation embedded in a string (ie: \"0644\")    folder_mode  yes  The permission of all the target folders. Must be an octal representation embedded in a string (ie: \"0755\")    no_remove  no  Boolean: Prevent this folder to be removed from the target when HADeploy will be used in REMOVE mode. Default:  no    ranger_policy  no  Definition of Apache Ranger policy bound to this tree. Parameters are same as  hdfs_ranger_policies  excepts than  paths  should not be defined as is automatically set to the folder path. Scope must be  hdfs . The policy name can be explicitly defined. Otherwise, a name will be generated as  \"_ dest_folder _\" . See example below for more information.    when  no  Boolean. Allow  conditional deployment  of this item. Default  True      src: must reference a folder. To copy a single file, use the files definition.", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/files/trees/#example", 
            "text": "trees:\n- scope: hdfs\n  src:  file://data \n  dest_folder:  /apps/broadapp/init_data \n  owner: broadapp\n  group: broadgroup\n  file_mode:  0644 \n  folder_mode:  0755 \n\n- scope: hdfs\n  src:  file://mytree \n  dest_folder:  /apps/broadapp/thetree \n  file_mode:  0000 \n  folder_mode:  0000 \n  ranger_policy:\n    audit: no\n    permissions:\n    - users:\n      - broadapp\n      accesses:\n      - read\n      - write\n      - execute\n    - groups:\n      - broadgroup\n      accesses:\n      - read\n      - execute", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_datasets/", 
            "text": "hbase_datasets\n\n\nSynopsis\n\n\nProvide a list of datasets to upload in HBase tables \n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntable\n\n\nyes\n\n\nThe table to populate\n\n\n\n\n\n\nnamespace\n\n\nyes\n\n\nThe name of the HBase namespace where the table is.\n\n\n\n\n\n\nsrc\n\n\n\n\nThe file containing the data to load in the table. May be in the form:\nhttp://....\n for fetching the file from a remote http server\nhttps://...\n for fetching the file from a remote https server\nfile://...\n for fetching the file locally, from one of the folder provided by the \nlocal_files_folders:\n list\nfile:///...\n for fetching the file locally, with a absolute path on the HADeploy node.\ntmpl://...\n  Source is a template, which will be processed by Ansible/Jinja2 mechanism. Template will be fetched locally, from one of the folders provided by the \nlocal_templates_folders:\n list\ntmpl:///...\n Same as above, except source template will be fetched from the HADeploy node with an absolute path.\nThe file format is described below\n\n\n\n\n\n\ndelRows\n\n\nno\n\n\nSee below.\nDefault: \nno\n\n\n\n\n\n\ndelValues\n\n\nno\n\n\nSee below.\nDefault: \nno\n\n\n\n\n\n\ndontAddRow\n\n\nno\n\n\nSee below.\nDefault: \nno\n\n\n\n\n\n\ndontAddValue\n\n\nno\n\n\nSee below.\nDefault: \nno\n\n\n\n\n\n\nupdValues\n\n\nno\n\n\nSee below.\nDefault: \nno\n\n\n\n\n\n\nvalidate_certs\n\n\nno\n\n\nBoolean; In case of src: \nhttps://...\n Setting to false, will disable strict certificate checking, thus allowing self-signed certificate.\nDefault: \nyes\n\n\n\n\n\n\nforce_basic_auth\n\n\nno\n\n\nBoolean; In case of \nsrc: http[s]://...\n . The underlying module only sends authentication information when a webservice responds to an initial request with a 401 status. Since some basic auth services do not properly send a 401, logins will fail. This option forces the sending of the Basic authentication header upon initial request.\n\n\n\n\n\n\nurl_username\n\n\nno\n\n\nString; In case of \nsrc: http[s]://...\n. The username for use in HTTP basic authentication. This parameter can be used without url_password for sites that allow empty passwords.\n\n\n\n\n\n\nurl_password\n\n\nno\n\n\nString; In case of \nsrc: http[s]://...\n. The password for use in HTTP basic authentication\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this dataset to be deleted from the table when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nExample\n\n\nhbase_datasets:\n  - table: hbload1\n    namespace: default\n    src: file://dataset1.json\n    updValues: yes\n\n\n\n\nAction on pre-existing data\n\n\nIf the HBase table already contains some data, the default behavior is 'Just add, don't delete, don't modify'. Options are provided to modify such behavior.\n\n\n\n\nUn-existing row will be added. (Un-existing means there is no row with this rowKey present in the table). If you want to prevent this, set dontAddRow.\n\n\nRow existing in the table but not in the dataset file will NOT be deleted. If you want to delete them, set delRows.\n\n\nIf a row already exists and is identical, it will not be modified.\n\n\nIf a row exists in the table, but the input dataset provide more columns:values, the missing column:value will be added. If you want to prevent this, set dontAddValue\n\n\nIf a row exists, but the input file provide less column:value, the column:value will NOT be deleted. If you want to delete them, set delValues.\n\n\nIf a row exists but some value for an existing qualifier differs from the one in the input file, value will NOT be updated. If you want to update them, set updValues.\n\n\n\n\nIf you set this switches combination:\n\n\ndontAddRow: yes\ndontAddValue: yes\n\n\n\n\nThen, this operation will not modify the table\n\n\nIf you set this switches combination:\n\n\ndelRows: yes\ndelValues: yes\nupdValues: yes\n\n\n\n\nThen, the content of the table will be adjusted to be fully identical to the content of the dataset file.\n\n\nFile format\n\n\nThe file format used by hbase_datasets is a json with the following form:\n\n\n{\n    { \nrowKey1\n: { \ncolFamily1\n: { \nqualifier1\n: \nvalue1\n, ...}, \ncolFamily2\n: { ... }, ...}, \n    { \nrowKey2\n: { \ncolFamily1\n: { \nqualifier1\n: \nvalue1\n, ...}, \ncolFamily2\n: { ... }, ...},\n    ...\n} \n\n\n\n\nFor example:\n\n\n{\n    \n000000\n: { \nid\n: { \nfname\n: \nDelpha\n, \nlname\n: \nDickinson\n, \nprefix\n: \nMr.\n, \nreg\n: \n000000\n }, \njob\n: { \ncpny\n: \nBarton, Barton and Barton\n, \ntitle\n: \nHuman Branding Officer\n } }\n   ,\n000001\n: { \nid\n: { \nfname\n: \nAlvina\n, \nlname\n: \nSchulist\n, \nprefix\n: \nDr.\n, \nreg\n: \n000001\n }, \njob\n: { \ncpny\n: \nHilll, Hilll and Hilll\n, \ntitle\n: \nInvestor Brand Coordinator\n } }\n   ,\n000002\n: { \nid\n: { \nfname\n: \nBerniece\n, \nlname\n: \nBahringer\n, \nprefix\n: \nMrs.\n, \nreg\n: \n000002\n }, \njob\n: { \ncpny\n: \nEichmann-Eichmann\n, \ntitle\n: \nDistrict Paradigm Coordinator\n } }\n}\n\n\n\n\nBinary representation\n\n\nInternally, HBase does not handle String, but byte[]. So, there is a need to represent binary data in string representation. Choice has been made to use the escape convention of the HBase function \nBytes.toByteBinary()\n and \nBytes.toStringBinary()\n, using the \n'\\xXX'\n notation, where \nXX\n is the hexadecimal code of the byte.\nNote, the \n'\\'\n itself must be escaped. For example, the binary code 1 will be represented by \n\"\\\\0x01\"\n\n\nFetch dataset from an existing file\n\n\nIt can be useful to create an initial dataset file from data already existing in an HBase file.\n\n\nYou can use the \nhbdump\n tool you will find at \n\n\nhttps://github.com/BROADSoftware/hbtools\n\n\nNote than, internally, HADeploy uses the \nhbload\n tool to load these dataset.\n\n\nAlso, note the \nhbdump\n output will not escape printable characters, so, for example \n\"7000003\"\n and \n\"\\\\x37000003\"\n represents the same byte, as \nx37\n is the hexadecimal code of the character \n'7'\n. Same for \n\"\\\\x2E000007\"\n and \n\".000007\"\n\n\nLimitations\n\n\n\n\nhbase_datasets\n does not dynamically create HBase column family. All column family referenced in the input file must be already defined in the HBase table. Or an error will be generated.\n\n\nhbase_datasets\n does not handle HBase cell's timestamp.\n\n\nhbase_datasets\n is not intended to work on 'big' dataset. It is intended to be use in application deployment where some tables need to be populated with an initial bunch of data. It will fully load its dataset in memory, thus limiting the volume it will be able to manage.", 
            "title": "hbase_datasets"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_datasets/#hbase_datasets", 
            "text": "", 
            "title": "hbase_datasets"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_datasets/#synopsis", 
            "text": "Provide a list of datasets to upload in HBase tables", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_datasets/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      table  yes  The table to populate    namespace  yes  The name of the HBase namespace where the table is.    src   The file containing the data to load in the table. May be in the form: http://....  for fetching the file from a remote http server https://...  for fetching the file from a remote https server file://...  for fetching the file locally, from one of the folder provided by the  local_files_folders:  list file:///...  for fetching the file locally, with a absolute path on the HADeploy node. tmpl://...   Source is a template, which will be processed by Ansible/Jinja2 mechanism. Template will be fetched locally, from one of the folders provided by the  local_templates_folders:  list tmpl:///...  Same as above, except source template will be fetched from the HADeploy node with an absolute path. The file format is described below    delRows  no  See below. Default:  no    delValues  no  See below. Default:  no    dontAddRow  no  See below. Default:  no    dontAddValue  no  See below. Default:  no    updValues  no  See below. Default:  no    validate_certs  no  Boolean; In case of src:  https://...  Setting to false, will disable strict certificate checking, thus allowing self-signed certificate. Default:  yes    force_basic_auth  no  Boolean; In case of  src: http[s]://...  . The underlying module only sends authentication information when a webservice responds to an initial request with a 401 status. Since some basic auth services do not properly send a 401, logins will fail. This option forces the sending of the Basic authentication header upon initial request.    url_username  no  String; In case of  src: http[s]://... . The username for use in HTTP basic authentication. This parameter can be used without url_password for sites that allow empty passwords.    url_password  no  String; In case of  src: http[s]://... . The password for use in HTTP basic authentication    no_remove  no  Boolean: Prevent this dataset to be deleted from the table when HADeploy will be used in REMOVE mode. Default:  no    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_datasets/#example", 
            "text": "hbase_datasets:\n  - table: hbload1\n    namespace: default\n    src: file://dataset1.json\n    updValues: yes", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_datasets/#action-on-pre-existing-data", 
            "text": "If the HBase table already contains some data, the default behavior is 'Just add, don't delete, don't modify'. Options are provided to modify such behavior.   Un-existing row will be added. (Un-existing means there is no row with this rowKey present in the table). If you want to prevent this, set dontAddRow.  Row existing in the table but not in the dataset file will NOT be deleted. If you want to delete them, set delRows.  If a row already exists and is identical, it will not be modified.  If a row exists in the table, but the input dataset provide more columns:values, the missing column:value will be added. If you want to prevent this, set dontAddValue  If a row exists, but the input file provide less column:value, the column:value will NOT be deleted. If you want to delete them, set delValues.  If a row exists but some value for an existing qualifier differs from the one in the input file, value will NOT be updated. If you want to update them, set updValues.   If you set this switches combination:  dontAddRow: yes\ndontAddValue: yes  Then, this operation will not modify the table  If you set this switches combination:  delRows: yes\ndelValues: yes\nupdValues: yes  Then, the content of the table will be adjusted to be fully identical to the content of the dataset file.", 
            "title": "Action on pre-existing data"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_datasets/#file-format", 
            "text": "The file format used by hbase_datasets is a json with the following form:  {\n    {  rowKey1 : {  colFamily1 : {  qualifier1 :  value1 , ...},  colFamily2 : { ... }, ...}, \n    {  rowKey2 : {  colFamily1 : {  qualifier1 :  value1 , ...},  colFamily2 : { ... }, ...},\n    ...\n}   For example:  {\n     000000 : {  id : {  fname :  Delpha ,  lname :  Dickinson ,  prefix :  Mr. ,  reg :  000000  },  job : {  cpny :  Barton, Barton and Barton ,  title :  Human Branding Officer  } }\n   , 000001 : {  id : {  fname :  Alvina ,  lname :  Schulist ,  prefix :  Dr. ,  reg :  000001  },  job : {  cpny :  Hilll, Hilll and Hilll ,  title :  Investor Brand Coordinator  } }\n   , 000002 : {  id : {  fname :  Berniece ,  lname :  Bahringer ,  prefix :  Mrs. ,  reg :  000002  },  job : {  cpny :  Eichmann-Eichmann ,  title :  District Paradigm Coordinator  } }\n}", 
            "title": "File format"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_datasets/#binary-representation", 
            "text": "Internally, HBase does not handle String, but byte[]. So, there is a need to represent binary data in string representation. Choice has been made to use the escape convention of the HBase function  Bytes.toByteBinary()  and  Bytes.toStringBinary() , using the  '\\xXX'  notation, where  XX  is the hexadecimal code of the byte.\nNote, the  '\\'  itself must be escaped. For example, the binary code 1 will be represented by  \"\\\\0x01\"", 
            "title": "Binary representation"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_datasets/#fetch-dataset-from-an-existing-file", 
            "text": "It can be useful to create an initial dataset file from data already existing in an HBase file.  You can use the  hbdump  tool you will find at   https://github.com/BROADSoftware/hbtools  Note than, internally, HADeploy uses the  hbload  tool to load these dataset.  Also, note the  hbdump  output will not escape printable characters, so, for example  \"7000003\"  and  \"\\\\x37000003\"  represents the same byte, as  x37  is the hexadecimal code of the character  '7' . Same for  \"\\\\x2E000007\"  and  \".000007\"", 
            "title": "Fetch dataset from an existing file"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_datasets/#limitations", 
            "text": "hbase_datasets  does not dynamically create HBase column family. All column family referenced in the input file must be already defined in the HBase table. Or an error will be generated.  hbase_datasets  does not handle HBase cell's timestamp.  hbase_datasets  is not intended to work on 'big' dataset. It is intended to be use in application deployment where some tables need to be populated with an initial bunch of data. It will fully load its dataset in memory, thus limiting the volume it will be able to manage.", 
            "title": "Limitations"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_namespaces/", 
            "text": "hbase_namespaces\n\n\nSynopsis\n\n\nProvide a list of HBase namespaces required by the application. They will be created (or destroyed) by HADeploy, except if managed flag is set to no.\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe name of the HBase namespace.\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this namespace to be removed when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\nmanaged\n\n\nno\n\n\nIf no, this namespace is not handled by HADeploy. It will not be created nor deleted. But HADeploy can create and delete tables in this namespace.\nDefault: \nyes\n\n\n\n\n\n\nranger_policy\n\n\nno\n\n\nDefinition of Apache Ranger policy bound to this namespace. Parameters are same as \nhbase_ranger_policy\n except than \ntables\n, \ncolumns_families\n and \ncolumns\n should not be defined. The policy will apply on all column families and all columns of all tables in this namespace.\nThe policy name can be explicitly defined. Otherwise, a name will be generated as \"\n_\nnamespace\n_\n\".\nSee example below for more information\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nExample\n\n\nhbase_namespaces:\n- name: broadns\n\n\n\n\nIf you want to use the 'default' namespace for some of your table, you may declare it.\n\n\nhbase_namespaces:\n- name: broadns\n\n- name: default\n  no_remove: True\n  managed: no\n\n\n\n\nIn this case, you need to set no_remove: True, as the default namespace can't of course be removed. Otherwise, an error will be generated. And, of course managed: False.\n\n\nNote this is optional, as HADeploy internally create such definition in this case (Only for the default namespace)\n\n\nAn example with a bound \nApache Ranger policy\n:\n\n\nhbase_namespaces:\n- name: broadns\n  ranger_policy:\n    permissions:\n    - groups:\n      - users\n      accesses:\n      - read\n    - groups:\n      - hbadmin\n      accesses:\n      - read\n      - write\n      - create\n      - admin", 
            "title": "hbase_namespaces"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_namespaces/#hbase_namespaces", 
            "text": "", 
            "title": "hbase_namespaces"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_namespaces/#synopsis", 
            "text": "Provide a list of HBase namespaces required by the application. They will be created (or destroyed) by HADeploy, except if managed flag is set to no.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_namespaces/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  The name of the HBase namespace.    no_remove  no  Boolean: Prevent this namespace to be removed when HADeploy will be used in REMOVE mode. Default:  no    managed  no  If no, this namespace is not handled by HADeploy. It will not be created nor deleted. But HADeploy can create and delete tables in this namespace. Default:  yes    ranger_policy  no  Definition of Apache Ranger policy bound to this namespace. Parameters are same as  hbase_ranger_policy  except than  tables ,  columns_families  and  columns  should not be defined. The policy will apply on all column families and all columns of all tables in this namespace. The policy name can be explicitly defined. Otherwise, a name will be generated as \" _ namespace _ \". See example below for more information    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_namespaces/#example", 
            "text": "hbase_namespaces:\n- name: broadns  If you want to use the 'default' namespace for some of your table, you may declare it.  hbase_namespaces:\n- name: broadns\n\n- name: default\n  no_remove: True\n  managed: no  In this case, you need to set no_remove: True, as the default namespace can't of course be removed. Otherwise, an error will be generated. And, of course managed: False.  Note this is optional, as HADeploy internally create such definition in this case (Only for the default namespace)  An example with a bound  Apache Ranger policy :  hbase_namespaces:\n- name: broadns\n  ranger_policy:\n    permissions:\n    - groups:\n      - users\n      accesses:\n      - read\n    - groups:\n      - hbadmin\n      accesses:\n      - read\n      - write\n      - create\n      - admin", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_relay/", 
            "text": "hbase_relay\n\n\nSynopsis\n\n\nIssuing some commands to specifics subsystem, such as HBase require a quite complex client configuration.\n\n\nTo avoid this, HADeploy will not issue such command directly, but push the command on one of the cluster node, called \u2019Relay node'.\nAn edge node of the cluster would typically assume this function.\n\n\nhbase_relay\n will define which host will be used to relay operations for HBase, and also how these operations will be performed.\n\n\nThere should be only one entry of this type in the HADeploy definition file.\n\n\nAttributes\n\n\nhbase_relay\n is a map with the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nhost\n\n\nyes\n\n\nThe host on which all hbase commands will be pushed for execution. Must be fully configured as HBase client.\n\n\n\n\n\n\ntools_folder\n\n\nno\n\n\nFolder used by HADeploy to install some tools for HBase management.\nDefault: \n/tmp/hadeploy_\nuser\n/\n where \nuser\n is the \nssh_user\n defined for this relay host.\n\n\n\n\n\n\nprincipal\n\n\nno\n\n\nA Kerberos principal allowing all HBase related operation to be performed. See \nbelow\n\n\n\n\n\n\nlocal_keytab_path\n\n\nno\n\n\nA local path to the associated keytab file. This path is relative to the embeding file. See \nbelow\n\n\n\n\n\n\nrelay_keytab_path\n\n\nno\n\n\nA path to the associated keytab file on the relay host. See \nbelow\n\n\n\n\n\n\nbecome_user\n\n\nno\n\n\nA user account under which all namespace and table operations will be performed. Only used on non-Kerberos cluster.\nNote: The \nssh_user\n defined for this relay host must have enough rights to switch to this \nbecome_user\n using the \nbecome_method\n below.\nDefault: No user switch, so the \nssh_user\n defined for this relay host will be used.\n\n\n\n\n\n\nbecome_method\n\n\nno\n\n\nThe method used to swith to this user. Refer to the Ansible documentation on this parameter.\nDefault: Ansible default (\nsudo\n).\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nKerberos authentication\n\n\nWhen \nprincipal\n and \n..._keytab_path\n variables are defined, Kerberos authentication will be activated for all HBase operations.\n\n\n\n\n\n\nAll HBase operations will be performed on behalf of the user defined by the provided principal. \n\n\n\n\n\n\nThis principal must have enough rights to be able to create namespaces and HBase table. \n\n\n\n\n\n\nRegarding the keytab file, two cases:\n\n\n\n\n\n\nThis keytab file already exists on the relay host. In such case, the \nrelay_keytab_path\n must be set to the location of this file. And the relay host's \nssh_user\n must have read access on it.\n\nNormally, for HBase, there should be at least one principal and keytab file created with full privileges by the system during Kerberos setup.\n\n\n\n\n\n\nThis keytab file is not present on the relay host. In such case the \nlocal_keytab_path\n must be set to its local location. HADeploy will take care of copying it on the remote relay host, \nin a location under \ntools_folder\n. Note you can also modify this target location by setting also the \nrelay_keytab_path\n parameter. In this case, \nit must be the full path, including the keytab file name. And the containing folder must exists.\n\n\n\n\n\n\nExample\n\n\nhbase_relay:\n  host: en1\n\n\n\n\nWith Kerberos activated:\n\n\nhbase_relay:\n  host: en1\n  principal: hbase-mycluster\n  relay_keytab_path: /etc/security/keytabs/hbase.headless.keytab", 
            "title": "hbase_relay"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_relay/#hbase_relay", 
            "text": "", 
            "title": "hbase_relay"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_relay/#synopsis", 
            "text": "Issuing some commands to specifics subsystem, such as HBase require a quite complex client configuration.  To avoid this, HADeploy will not issue such command directly, but push the command on one of the cluster node, called \u2019Relay node'.\nAn edge node of the cluster would typically assume this function.  hbase_relay  will define which host will be used to relay operations for HBase, and also how these operations will be performed.  There should be only one entry of this type in the HADeploy definition file.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_relay/#attributes", 
            "text": "hbase_relay  is a map with the following attributes:     Name  req?  Description      host  yes  The host on which all hbase commands will be pushed for execution. Must be fully configured as HBase client.    tools_folder  no  Folder used by HADeploy to install some tools for HBase management. Default:  /tmp/hadeploy_ user /  where  user  is the  ssh_user  defined for this relay host.    principal  no  A Kerberos principal allowing all HBase related operation to be performed. See  below    local_keytab_path  no  A local path to the associated keytab file. This path is relative to the embeding file. See  below    relay_keytab_path  no  A path to the associated keytab file on the relay host. See  below    become_user  no  A user account under which all namespace and table operations will be performed. Only used on non-Kerberos cluster. Note: The  ssh_user  defined for this relay host must have enough rights to switch to this  become_user  using the  become_method  below. Default: No user switch, so the  ssh_user  defined for this relay host will be used.    become_method  no  The method used to swith to this user. Refer to the Ansible documentation on this parameter. Default: Ansible default ( sudo ).    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_relay/#kerberos-authentication", 
            "text": "When  principal  and  ..._keytab_path  variables are defined, Kerberos authentication will be activated for all HBase operations.    All HBase operations will be performed on behalf of the user defined by the provided principal.     This principal must have enough rights to be able to create namespaces and HBase table.     Regarding the keytab file, two cases:    This keytab file already exists on the relay host. In such case, the  relay_keytab_path  must be set to the location of this file. And the relay host's  ssh_user  must have read access on it. Normally, for HBase, there should be at least one principal and keytab file created with full privileges by the system during Kerberos setup.    This keytab file is not present on the relay host. In such case the  local_keytab_path  must be set to its local location. HADeploy will take care of copying it on the remote relay host, \nin a location under  tools_folder . Note you can also modify this target location by setting also the  relay_keytab_path  parameter. In this case, \nit must be the full path, including the keytab file name. And the containing folder must exists.", 
            "title": "Kerberos authentication"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_relay/#example", 
            "text": "hbase_relay:\n  host: en1  With Kerberos activated:  hbase_relay:\n  host: en1\n  principal: hbase-mycluster\n  relay_keytab_path: /etc/security/keytabs/hbase.headless.keytab", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_tables/", 
            "text": "hbase_tables\n\n\nSynopsis\n\n\nProvide a list of HBase tables required by the application \n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nnamespace\n\n\nyes\n\n\nThe HBase namespace hosting this table. May be defined in the hbase_namespaces list defined above, or will be assumed as already existing.\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe name of this table\n\n\n\n\n\n\nproperties\n\n\nno\n\n\nA list of properties, allowing definition of table properties.\nYou will find table properties definition is HBase documentation.\nFor a complete list, please refer to the Javadoc of the class \norg.apache.hadoop.hbase.HTableDescriptor\n of your HBase version.\n\n\n\n\n\n\ncolumn_families\n\n\nyes\n\n\nProvide a list of one or several column families. See below\n\n\n\n\n\n\npresplit\n\n\nno\n\n\nAllow an initial region split schema to be defined. See below\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this table to be removed when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\nranger_policy\n\n\nno\n\n\nDefinition of Apache Ranger policy bound to this table. Parameters are same as \nhbase_ranger_policies\n except than \ntables\n, \ncolumns_families\n and \ncolumns\n should not be defined. The policy will apply on all column families and all column of the table.\nThe policy name can be explicitly defined. Otherwise, a name will be generated as \"\n_\nnamespace\n:\ntable\n_\n\".\nSee example below for more information\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\ncolumn_families\n\n\nEach item of the column_families list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\n\n\nThe name of this column family.\n\n\n\n\n\n\nproperties\n\n\nno\n\n\n\n\nA list of properties, allowing definition of this column family properties.\nYou will find table properties definition is HBase documentation.\nFor a complete list, please refer to the Javadoc of the class \norg.apache.hadoop.hbase.HColumnDescriptor\n of your HBase version.\n\n\n\n\n\n\n\n\npresplit\n\n\n\n\nNB: Please, note than unlike all other definition, presplitting is only effective at the initial table creation. If the table already exists, no modification is performed and the \npresplit:\n attribute is ignored.\n\n\n\n\nPresplitting can be expressed with one of the following 2 methods:\n\n\n  presplit:\n    splits: ['BENJAMIN', 'JULIA', 'MARTIN', 'PAUL', 'VALENTIN']\n\n\n\n\nor:\n\n\n  presplit:\n    start_key: \n10000\n\n    end_key: \n90000\n\n    num_region: 10\n\n\n\n\nNote that internally HBase works with byte[], not String. So, in some case, the splitting must be expressed using hexadecimal notation. For example:\n\n\n  presplit:\n    start_key: '\\x00'\n    end_key: '\\xFF'\n    num_region: 5\n\n\n\n\nor:\n\n\n  presplit:\n    splits:\n    - '\\x33'\n    - '\\x66'\n    - '\\x99'\n    - '\\xCC'\n\n\n\n\nNote these two last presplit expressions will result in the same layout.\n\n\nSome aspect to take care using such notation:\n\n\n\n\nString must be surrounded by simple quotes. If you want to use double quote, you will need to escape the backslash character (i.e: \n\"\\\\x33\"\n).\n\n\nLetters used for hexadecimal notation must always be upper case.\n\n\nIf you need more information about this format, note than, internally, all theses strings are parsed using the function \norg.apache.hadoop.hbase.util.Bytes.toBytesBinary()\n\n\n\n\nExample\n\n\nhbase_tables: \n- name: broadapp_t1\n  namespace: broadgroup\n  properties: \n    regionReplication: 1\n    durability: ASYNC_WAL\n  column_families:\n  - name: cf1\n    properties: \n      maxVersions: 12\n      minVersions: 2\n      timeToLive: 200000\n\n\n\n\nCreate a table with a single column family (\ncf1\n), without presplit\n\n\nhbase_tables: \n- name: testtable1\n  namespace: ns1\n  properties: \n    regionReplication: 1\n    durability: ASYNC_WAL\n  column_families:\n  - name: cf1\n    properties: \n      maxVersions: 12\n      minVersions: 2\n      timeToLive: 200000\n      cacheBloomsOnWrite: true\n      compressionType: NONE\n  - name: cf2\n    properties:\n      maxVersions: 12\n      minVersions: 2\n      timeToLive: 200000\n\n\n\n\nCreate a table with two column families, with different set of properties\n\n\nhbase_tables: \n- name: testtable2\n  namespace: ns2\n  column_families:\n  - name: cf1\n  - name: cf2\n  presplit:\n    splits: ['BENJAMIN', 'JULIA', 'MARTIN', 'PAUL', 'VALENTIN']\nno_remove: yes\n\n\n\n\nCreate a table with two column families, with all properties to default value. Table is presplitted on an estimated distribution of first names. This table will not be removed by the application removal processing.\n\n\nFollowing will create a table with two columns families and grant read access to members of \nusers\n group through Apache Ranger.\n\n\nhbase_tables: \n- name: testtable3\n  namespace: ns1\n  column_families:\n  - name: cf1\n  - name: cf2\n  ranger_policy:\n    permissions:\n    - groups:\n      - users\n      accesses:\n      - read", 
            "title": "hbase_tables"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_tables/#hbase_tables", 
            "text": "", 
            "title": "hbase_tables"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_tables/#synopsis", 
            "text": "Provide a list of HBase tables required by the application", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_tables/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      namespace  yes  The HBase namespace hosting this table. May be defined in the hbase_namespaces list defined above, or will be assumed as already existing.    name  yes  The name of this table    properties  no  A list of properties, allowing definition of table properties. You will find table properties definition is HBase documentation. For a complete list, please refer to the Javadoc of the class  org.apache.hadoop.hbase.HTableDescriptor  of your HBase version.    column_families  yes  Provide a list of one or several column families. See below    presplit  no  Allow an initial region split schema to be defined. See below    no_remove  no  Boolean: Prevent this table to be removed when HADeploy will be used in REMOVE mode. Default:  no    ranger_policy  no  Definition of Apache Ranger policy bound to this table. Parameters are same as  hbase_ranger_policies  except than  tables ,  columns_families  and  columns  should not be defined. The policy will apply on all column families and all column of the table. The policy name can be explicitly defined. Otherwise, a name will be generated as \" _ namespace : table _ \". See example below for more information    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_tables/#column_families", 
            "text": "Each item of the column_families list has the following attributes:     Name  req?  Default  Description      name  yes   The name of this column family.    properties  no   A list of properties, allowing definition of this column family properties. You will find table properties definition is HBase documentation. For a complete list, please refer to the Javadoc of the class  org.apache.hadoop.hbase.HColumnDescriptor  of your HBase version.", 
            "title": "column_families"
        }, 
        {
            "location": "/plugins_reference/hbase/hbase_tables/#presplit", 
            "text": "NB: Please, note than unlike all other definition, presplitting is only effective at the initial table creation. If the table already exists, no modification is performed and the  presplit:  attribute is ignored.   Presplitting can be expressed with one of the following 2 methods:    presplit:\n    splits: ['BENJAMIN', 'JULIA', 'MARTIN', 'PAUL', 'VALENTIN']  or:    presplit:\n    start_key:  10000 \n    end_key:  90000 \n    num_region: 10  Note that internally HBase works with byte[], not String. So, in some case, the splitting must be expressed using hexadecimal notation. For example:    presplit:\n    start_key: '\\x00'\n    end_key: '\\xFF'\n    num_region: 5  or:    presplit:\n    splits:\n    - '\\x33'\n    - '\\x66'\n    - '\\x99'\n    - '\\xCC'  Note these two last presplit expressions will result in the same layout.  Some aspect to take care using such notation:   String must be surrounded by simple quotes. If you want to use double quote, you will need to escape the backslash character (i.e:  \"\\\\x33\" ).  Letters used for hexadecimal notation must always be upper case.  If you need more information about this format, note than, internally, all theses strings are parsed using the function  org.apache.hadoop.hbase.util.Bytes.toBytesBinary()   Example  hbase_tables: \n- name: broadapp_t1\n  namespace: broadgroup\n  properties: \n    regionReplication: 1\n    durability: ASYNC_WAL\n  column_families:\n  - name: cf1\n    properties: \n      maxVersions: 12\n      minVersions: 2\n      timeToLive: 200000  Create a table with a single column family ( cf1 ), without presplit  hbase_tables: \n- name: testtable1\n  namespace: ns1\n  properties: \n    regionReplication: 1\n    durability: ASYNC_WAL\n  column_families:\n  - name: cf1\n    properties: \n      maxVersions: 12\n      minVersions: 2\n      timeToLive: 200000\n      cacheBloomsOnWrite: true\n      compressionType: NONE\n  - name: cf2\n    properties:\n      maxVersions: 12\n      minVersions: 2\n      timeToLive: 200000  Create a table with two column families, with different set of properties  hbase_tables: \n- name: testtable2\n  namespace: ns2\n  column_families:\n  - name: cf1\n  - name: cf2\n  presplit:\n    splits: ['BENJAMIN', 'JULIA', 'MARTIN', 'PAUL', 'VALENTIN']\nno_remove: yes  Create a table with two column families, with all properties to default value. Table is presplitted on an estimated distribution of first names. This table will not be removed by the application removal processing.  Following will create a table with two columns families and grant read access to members of  users  group through Apache Ranger.  hbase_tables: \n- name: testtable3\n  namespace: ns1\n  column_families:\n  - name: cf1\n  - name: cf2\n  ranger_policy:\n    permissions:\n    - groups:\n      - users\n      accesses:\n      - read", 
            "title": "presplit"
        }, 
        {
            "location": "/plugins_reference/hdfs/hdfs_relay/", 
            "text": "hdfs_relay\n\n\nSynopsis\n\n\nIssuing some commands to specifics subsystem, such as HDFS require a quite complex client configuration.\n\n\nTo avoid this, HADeploy will not issue such command directly, but push the command on one of the cluster node, called \u2019Relay node'.\nAn edge node of the cluster would typically assume this function.\n\n\nhdfs_relay\n will define which host will be used to relay operations for HDFS, and also how these operations will be performed.\n\n\nThere should be only one entry of this type in the HADeploy definition file.\n\n\nAttributes\n\n\nhdfs_relay is a map with the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nhost\n\n\nyes\n\n\nThe host on which all HDFS commands will be pushed for execution.\n\n\n\n\n\n\ncache_folder\n\n\nno\n\n\nA folder on this host, which will be used by HADeploy as cache storage. Mainly, all files targeted to HDFS will be first pushed in this cache. And will remains in it, to optimize idempotence.\nDefault: \n{{ansible_user_dir}}/.hadeploy/files\n, where \n{{ansible_user_dir}}\n is substitued by the home folder of the \nssh_user\n defined for this relay host.\n\n\n\n\n\n\nuser\n\n\nno\n\n\nThe user account HADeploy will use to perform all HDFS related operation. Must have enough rights to do so.\nNot to be defined when using Kerberos authentication.\nDefault: The \nssh_user\n defined for this relay host or \nhdfs\n if this user is \nroot\n.\n\n\n\n\n\n\nprincipal\n\n\nno\n\n\nA Kerberos principal allowing all HDFS related operation to be performed. See \nbelow\n\n\n\n\n\n\nlocal_keytab_path\n\n\nno\n\n\nA local path to the associated keytab file. This path is relative to the embeding file. See \nbelow\n\n\n\n\n\n\nrelay_keytab_path\n\n\nno\n\n\nA path to the associated keytab file on the relay host. See \nbelow\n\n\n\n\n\n\nhadoop_conf_dir\n\n\nno\n\n\nWhere HADeploy will lookup Hadoop configuration file.\nDefault: \n/etc/hadoop/conf\n\n\n\n\n\n\nwebhdfs_endpoint\n\n\nno\n\n\nHADeploy will perform several actions through WebHDFS REST interface. You can specify corresponding endpoint, if it is not defined in the usual configuration way.\nDefault: The value found in \nhadoop_conf_dir\n/hdfs-site.xml\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nHadoop configuration lookup\n\n\nIf this \nhdfs_relay\n host is properly configured as an Hadoop client, there should be no need to provide value to \nhadoop_conf_dir\n and/or \nwebhdfs_endpoint\n, \nas HADeploy will be able to lookup the WebHDFS URL by using default values. \n\n\nKerberos authentication\n\n\nWhen \nprincipal\n and \n..._keytab_path\n variables are defined, Kerberos authentication will be activated for all HDFS folders, files and trees operations. This means a \nkinit\n will be issued with provided values before any HDFS access, and a \nkdestroy\n issued after. This has the following consequences:\n\n\n\n\n\n\nAll HDFS operations will be performed on behalf of the user defined by the provided principal. The user parameter become irrelevant and providing it is an error.\n\n\n\n\n\n\nThe \nkinit\n will be issued under the relay host \nssh_user\n account. This means any previous ticket own by this user on this node will be lost. \n\n\n\n\n\n\nRegarding the keytab file, two cases:\n\n\n\n\n\n\nThis keytab file already exists on the relay host. In such case, the \nrelay_keytab_path\n must be set to the location of this file. And the relay host's \nssh_user\n must have read access on it.\n\n\n\n\n\n\nThis keytab file is not present on the relay host. In such case the \nlocal_keytab_path\n must be set to its local location. HADeploy will take care of copying it on the remote relay host, in a location under \ntools_folder\n. Note you can also modify this target location by setting also the \nrelay_keytab_path\n parameter. In this case, \nit must be the full path, including the keytab file name. And the containing folder must exists.\n\n\n\n\n\n\nExample\n\n\nThe simplest case:\n\n\nhdfs_relay:\n  host: en1\n\n\n\n\nSame, with default value sets:\n\n\nhdfs_relay:\n  host: en1\n  user: hdfs\n  hadoop_conf_dir: \n/etc/hadoop/conf\n\n  webhdfs_endpoint: \nnamenode.mycluster.myinfra.com:50070\n\n\n\n\n\nThe simplest case with Kerberos activated:\n\n\nhdfs_relay:\n  host: en1\n  principal: hdfs-mycluster\n  relay_keytab_path: /etc/security/keytabs/hdfs.headless.keytab", 
            "title": "hdfs_relay"
        }, 
        {
            "location": "/plugins_reference/hdfs/hdfs_relay/#hdfs_relay", 
            "text": "", 
            "title": "hdfs_relay"
        }, 
        {
            "location": "/plugins_reference/hdfs/hdfs_relay/#synopsis", 
            "text": "Issuing some commands to specifics subsystem, such as HDFS require a quite complex client configuration.  To avoid this, HADeploy will not issue such command directly, but push the command on one of the cluster node, called \u2019Relay node'.\nAn edge node of the cluster would typically assume this function.  hdfs_relay  will define which host will be used to relay operations for HDFS, and also how these operations will be performed.  There should be only one entry of this type in the HADeploy definition file.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/hdfs/hdfs_relay/#attributes", 
            "text": "hdfs_relay is a map with the following attributes:     Name  req?  Description      host  yes  The host on which all HDFS commands will be pushed for execution.    cache_folder  no  A folder on this host, which will be used by HADeploy as cache storage. Mainly, all files targeted to HDFS will be first pushed in this cache. And will remains in it, to optimize idempotence. Default:  {{ansible_user_dir}}/.hadeploy/files , where  {{ansible_user_dir}}  is substitued by the home folder of the  ssh_user  defined for this relay host.    user  no  The user account HADeploy will use to perform all HDFS related operation. Must have enough rights to do so. Not to be defined when using Kerberos authentication. Default: The  ssh_user  defined for this relay host or  hdfs  if this user is  root .    principal  no  A Kerberos principal allowing all HDFS related operation to be performed. See  below    local_keytab_path  no  A local path to the associated keytab file. This path is relative to the embeding file. See  below    relay_keytab_path  no  A path to the associated keytab file on the relay host. See  below    hadoop_conf_dir  no  Where HADeploy will lookup Hadoop configuration file. Default:  /etc/hadoop/conf    webhdfs_endpoint  no  HADeploy will perform several actions through WebHDFS REST interface. You can specify corresponding endpoint, if it is not defined in the usual configuration way. Default: The value found in  hadoop_conf_dir /hdfs-site.xml    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/hdfs/hdfs_relay/#hadoop-configuration-lookup", 
            "text": "If this  hdfs_relay  host is properly configured as an Hadoop client, there should be no need to provide value to  hadoop_conf_dir  and/or  webhdfs_endpoint , \nas HADeploy will be able to lookup the WebHDFS URL by using default values.", 
            "title": "Hadoop configuration lookup"
        }, 
        {
            "location": "/plugins_reference/hdfs/hdfs_relay/#kerberos-authentication", 
            "text": "When  principal  and  ..._keytab_path  variables are defined, Kerberos authentication will be activated for all HDFS folders, files and trees operations. This means a  kinit  will be issued with provided values before any HDFS access, and a  kdestroy  issued after. This has the following consequences:    All HDFS operations will be performed on behalf of the user defined by the provided principal. The user parameter become irrelevant and providing it is an error.    The  kinit  will be issued under the relay host  ssh_user  account. This means any previous ticket own by this user on this node will be lost.     Regarding the keytab file, two cases:    This keytab file already exists on the relay host. In such case, the  relay_keytab_path  must be set to the location of this file. And the relay host's  ssh_user  must have read access on it.    This keytab file is not present on the relay host. In such case the  local_keytab_path  must be set to its local location. HADeploy will take care of copying it on the remote relay host, in a location under  tools_folder . Note you can also modify this target location by setting also the  relay_keytab_path  parameter. In this case, \nit must be the full path, including the keytab file name. And the containing folder must exists.", 
            "title": "Kerberos authentication"
        }, 
        {
            "location": "/plugins_reference/hdfs/hdfs_relay/#example", 
            "text": "The simplest case:  hdfs_relay:\n  host: en1  Same, with default value sets:  hdfs_relay:\n  host: en1\n  user: hdfs\n  hadoop_conf_dir:  /etc/hadoop/conf \n  webhdfs_endpoint:  namenode.mycluster.myinfra.com:50070   The simplest case with Kerberos activated:  hdfs_relay:\n  host: en1\n  principal: hdfs-mycluster\n  relay_keytab_path: /etc/security/keytabs/hdfs.headless.keytab", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/hdfs/source_host_credentials/", 
            "text": "source_host_credentials\n\n\nSynopsis\n\n\nProvide a list of credentials to allow access to HDFS from host other than \nhdfs_relay\n.\n\n\nThis is used by files or trees operation when the source is a cluster node (\nsrc: node:///...\n)\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nhost\n\n\nyes\n\n\nThe source host\n\n\n\n\n\n\nprincipal\n\n\nyes\n\n\nA Kerberos principal allowing all HDFS related operation to be performed. See below\n\n\n\n\n\n\nlocal_keytab_path\n\n\nyes if\nnode_keytab_path\nis not defined\n\n\nA local path to the associated keytab file. This path is relative to the embeding file. See \nbelow\n\n\n\n\n\n\nnode_keytab_path\n\n\nyes if\nlocal_keytab_path\nis not defined\n\n\nA path to the associated keytab file on the node. See \nbelow\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nKerberos authentication\n\n\nWhen performing a copy operation (files or trees) from a cluster's host to HDFS, if a \nprincipal\n and \n..._keytab_path\n variables are defined for this host, Kerberos authentication will be activated before issuing the operation. \n\n\nThis means a \nkinit\n will be issued with provided values on this host before any HDFS access, and a kdestroy issued after. This has the following consequences:\n\n\n\n\n\n\nAll HDFS operations will be performed on behalf of the user defined by the provided principal. \n\n\n\n\n\n\nThe \nkinit\n will be issued under this host \nssh_user\n account. This means any previous ticket own by this user on this node will be lost. \n\n\n\n\n\n\nRegarding the keytab file, two cases:\n\n\n\n\n\n\nThis keytab file already exists on this host. In such case, the \nnode_keytab_path\n must be set to the location of this file. And this host's \nssh_user\n must have read access on it.\n\n\n\n\n\n\nThis keytab file is not present on this host. In such case the \nlocal_keytab_path\n must be set to its local location. HADeploy will take care of copying it on the remote host, in a location under \ntools_folder\n. Note you can also modify this target location by setting also the \nnode_keytab_path\n parameter. In this case, \nit must be the full path, including the keytab file name. And the containing folder must exists.\n\n\n\n\n\n\nExample\n\n\nsource_host_credentials:\n- host: sr1\n  principal: hdfs-mycluster\n  node_keytab_path: /etc/security/keytabs/hdfs.headless.keytab", 
            "title": "source_host_credentials"
        }, 
        {
            "location": "/plugins_reference/hdfs/source_host_credentials/#source_host_credentials", 
            "text": "", 
            "title": "source_host_credentials"
        }, 
        {
            "location": "/plugins_reference/hdfs/source_host_credentials/#synopsis", 
            "text": "Provide a list of credentials to allow access to HDFS from host other than  hdfs_relay .  This is used by files or trees operation when the source is a cluster node ( src: node:///... )", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/hdfs/source_host_credentials/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      host  yes  The source host    principal  yes  A Kerberos principal allowing all HDFS related operation to be performed. See below    local_keytab_path  yes if node_keytab_path is not defined  A local path to the associated keytab file. This path is relative to the embeding file. See  below    node_keytab_path  yes if local_keytab_path is not defined  A path to the associated keytab file on the node. See  below    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/hdfs/source_host_credentials/#kerberos-authentication", 
            "text": "When performing a copy operation (files or trees) from a cluster's host to HDFS, if a  principal  and  ..._keytab_path  variables are defined for this host, Kerberos authentication will be activated before issuing the operation.   This means a  kinit  will be issued with provided values on this host before any HDFS access, and a kdestroy issued after. This has the following consequences:    All HDFS operations will be performed on behalf of the user defined by the provided principal.     The  kinit  will be issued under this host  ssh_user  account. This means any previous ticket own by this user on this node will be lost.     Regarding the keytab file, two cases:    This keytab file already exists on this host. In such case, the  node_keytab_path  must be set to the location of this file. And this host's  ssh_user  must have read access on it.    This keytab file is not present on this host. In such case the  local_keytab_path  must be set to its local location. HADeploy will take care of copying it on the remote host, in a location under  tools_folder . Note you can also modify this target location by setting also the  node_keytab_path  parameter. In this case, \nit must be the full path, including the keytab file name. And the containing folder must exists.", 
            "title": "Kerberos authentication"
        }, 
        {
            "location": "/plugins_reference/hdfs/source_host_credentials/#example", 
            "text": "source_host_credentials:\n- host: sr1\n  principal: hdfs-mycluster\n  node_keytab_path: /etc/security/keytabs/hdfs.headless.keytab", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/hive/hive_databases/", 
            "text": "hive_databases\n\n\nSynopsis\n\n\nProvide a list of HIVE databases required by the application. They will be created (or destroyed) by HADeploy.\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe name of the HIVE database.\n\n\n\n\n\n\nproperties\n\n\nno\n\n\nA map of properties. Equivalent to WITH DBPROPERTIES Hive DDL clause\n\n\n\n\n\n\nlocation\n\n\nno\n\n\nEquivalent to LOCATION Hive DDL clause\n\n\n\n\n\n\nowner\n\n\nno\n\n\nThe owner of the database. May be a user account or a group.\n\n\n\n\n\n\nowner_type\n\n\nno\n\n\nUSER or ROLE. Specify what represent the owner attribute\n\n\n\n\n\n\ncomment\n\n\nno\n\n\nEquivalent to the COMMENT Hive DDL close\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this database to be removed when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\nranger_policy\n\n\nno\n\n\nDefinition of Apache Ranger policy bound to this database.\nParameters are same as \nhive_ranger_policies\n except than \ndatabases\n, \ntables\n and \ncolumns\n should not be defined. The policy will apply on all columns of all tables of this database.\nThe policy name can be explicitly defined. Otherwise, a name will be generated as \"\n_\ndatabase\n_\n\".\nSee example below for more information\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nExample:\n\n\nhive_databases:\n- name: jdctest1\n\n\n\n\nWill internally generate the command:\n\n\nCREATE DATABASE jdctest1\n\n\n\n\nAnd\n\n\nhive_databases:\n- name: jdctest2\n  location: /user/apphive/db/testtables1\n  owner_type: USER\n  owner: apphive\n  comment: \nFor jdchive table testing\n\n\n\n\n\nWill internally generate the commands:\n\n\nCREATE DATABASE jdctest2 COMMENT 'For jdchive table testing' LOCATION 'hdfs://clusterid/user/apphive/db/testtables1'\nALTER DATABASE jdctest2 SET OWNER USER apphive\n\n\n\n\nThe following example illustrate the association with a Ranger policy, granting full rights on all table of this databases for the 'admin' user:\n\n\nhive_databases:\n- name: jdctest3\n  owner_type: USER\n  owner: apphive\n  ranger_policy:\n    permissions:\n    - users:\n      - admin\n      accesses:\n      - all\n      delegate_admin: yes\n\n\n\n\nDatabase Ownership\n\n\nDatabase owner can be explicitly set by the \nowner\n attribute defined above. If this attribute is not present, then the owner will be:\n\n\n\n\n\n\nIf kerberos is enabled, the account defined by the \nhive_relay.principal\n in the \nhive_relay\n definition.\n\n\n\n\n\n\nIf kerberos is not enabled, the account defined by the \nssh_user\n set for the \nhost\n used as \nhive_relay\n\n\n\n\n\n\nOnce created, one may change owner by setting the corresponding attribute. Launching HADeploy with another ansible_user/principal will have no effect", 
            "title": "hive_databases"
        }, 
        {
            "location": "/plugins_reference/hive/hive_databases/#hive_databases", 
            "text": "", 
            "title": "hive_databases"
        }, 
        {
            "location": "/plugins_reference/hive/hive_databases/#synopsis", 
            "text": "Provide a list of HIVE databases required by the application. They will be created (or destroyed) by HADeploy.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/hive/hive_databases/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  The name of the HIVE database.    properties  no  A map of properties. Equivalent to WITH DBPROPERTIES Hive DDL clause    location  no  Equivalent to LOCATION Hive DDL clause    owner  no  The owner of the database. May be a user account or a group.    owner_type  no  USER or ROLE. Specify what represent the owner attribute    comment  no  Equivalent to the COMMENT Hive DDL close    no_remove  no  Boolean: Prevent this database to be removed when HADeploy will be used in REMOVE mode. Default:  no    ranger_policy  no  Definition of Apache Ranger policy bound to this database. Parameters are same as  hive_ranger_policies  except than  databases ,  tables  and  columns  should not be defined. The policy will apply on all columns of all tables of this database. The policy name can be explicitly defined. Otherwise, a name will be generated as \" _ database _ \". See example below for more information    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/hive/hive_databases/#example", 
            "text": "hive_databases:\n- name: jdctest1  Will internally generate the command:  CREATE DATABASE jdctest1  And  hive_databases:\n- name: jdctest2\n  location: /user/apphive/db/testtables1\n  owner_type: USER\n  owner: apphive\n  comment:  For jdchive table testing   Will internally generate the commands:  CREATE DATABASE jdctest2 COMMENT 'For jdchive table testing' LOCATION 'hdfs://clusterid/user/apphive/db/testtables1'\nALTER DATABASE jdctest2 SET OWNER USER apphive  The following example illustrate the association with a Ranger policy, granting full rights on all table of this databases for the 'admin' user:  hive_databases:\n- name: jdctest3\n  owner_type: USER\n  owner: apphive\n  ranger_policy:\n    permissions:\n    - users:\n      - admin\n      accesses:\n      - all\n      delegate_admin: yes", 
            "title": "Example:"
        }, 
        {
            "location": "/plugins_reference/hive/hive_databases/#database-ownership", 
            "text": "Database owner can be explicitly set by the  owner  attribute defined above. If this attribute is not present, then the owner will be:    If kerberos is enabled, the account defined by the  hive_relay.principal  in the  hive_relay  definition.    If kerberos is not enabled, the account defined by the  ssh_user  set for the  host  used as  hive_relay    Once created, one may change owner by setting the corresponding attribute. Launching HADeploy with another ansible_user/principal will have no effect", 
            "title": "Database Ownership"
        }, 
        {
            "location": "/plugins_reference/hive/hive_relay/", 
            "text": "hive_relay\n\n\nSynopsis\n\n\nIssuing some commands to specifics subsystem, such as HIVE require a quite complex client configuration.\n\n\nTo avoid this, HADeploy will not issue such command directly, but push the command on one of the cluster node, called \u2019Relay node'.\nAn edge node of the cluster would typically assume this function.\n\n\nhive_relay\n will define which host will be used to relay operations for HIVE, and also how these operations will be performed.\n\n\nThere should be only one entry of this type in the HADeploy definition file.\n\n\nAttributes\n\n\nhive_relay\n is a map with the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nhost\n\n\nyes\n\n\nThe host on which all hive commands will be pushed for execution. Must be a fully configured as HIVE client.\n\n\n\n\n\n\ntools_folder\n\n\nno\n\n\nFolder used by HADeploy to install some tools for HIVE management.\nDefault: \n/tmp/hadeploy_\nuser\n/\n where \nuser\n is the \nssh_user\n defined for this relay host.\n\n\n\n\n\n\nprincipal\n\n\nno\n\n\nA Kerberos principal allowing all HIVE related operations to be performed. See \nbelow\n\n\n\n\n\n\nlocal_keytab_path\n\n\nno\n\n\nA local path to the associated keytab file. This path is relative to the embeding file. See \nbelow\n\n\n\n\n\n\nrelay_keytab_path\n\n\nno\n\n\nA path to the associated keytab file on the relay host. See \nbelow\n\n\n\n\n\n\nbecome_user\n\n\nno\n\n\nA user account under which all database and table operations will be performed. Only used on non-Kerberos cluster. It will determine table ownership.\nNote: The \nssh_user\n defined for this relay host must have enough rights to switch to this \nbecome_user\n using the \nbecome_method\n below.\nDefault: No user switch, so the \nssh_user\n defined for this relay host will be used.\n\n\n\n\n\n\nbecome_method\n\n\nno\n\n\nThe method used to swith to this user. Refer to the Ansible documentation on this parameter.\nDefault: Ansible default (\nsudo\n).\n\n\n\n\n\n\nreport_file\n\n\nno\n\n\nLocal path for a report file which will be (re)generated on each run.\nThis YAML file describe all performed operation and eventually un-achievable operation. It could be a starting point for a more sophisticated database migration processing.\nUnder the hood, all HIVE operation are performed by a special tools: \njdchive\n. You will find more information about the schema of this report file \nhere\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nKerberos authentication\n\n\nWhen \nprincipal\n and \n..._keytab_path\n variables are defined, Kerberos authentication will be activated for all HIVE operations.\n\n\n\n\n\n\nAll HIVE operations will be performed on behalf of the user defined by the provided principal. \n\n\n\n\n\n\nThis principal must have enough rights to be able to create HIVE databases and table. \n\n\n\n\n\n\nRegarding the keytab file, two cases:\n\n\n\n\n\n\nThis keytab file already exists on the relay host. In such case, the \nrelay_keytab_path\n must be set to the location of this file. And the relay host's \nssh_user\n must have read access on it.\n\n\n\n\n\n\nThis keytab file is not present on the relay host. In such case the \nlocal_keytab_path\n must be set to its local location. HADeploy will take care of copying it on the remote relay host, \nin a location under \ntools_folder\n. Note you can also modify this target location by setting also the \nrelay_keytab_path\n parameter. In this case, \nit must be the full path, including the keytab file name. And the containing folder must exists.\n\n\n\n\n\n\nExample\n\n\nhive_relay:\n  host: en1\n\n\n\n\nWith Kerberos activated:\n\n\nhive_relay:\n  host: en1\n  principal: hive_admin\n  relay_keytab_path: /etc/security/keytabs/hive_admin.keytab", 
            "title": "hive_relay"
        }, 
        {
            "location": "/plugins_reference/hive/hive_relay/#hive_relay", 
            "text": "", 
            "title": "hive_relay"
        }, 
        {
            "location": "/plugins_reference/hive/hive_relay/#synopsis", 
            "text": "Issuing some commands to specifics subsystem, such as HIVE require a quite complex client configuration.  To avoid this, HADeploy will not issue such command directly, but push the command on one of the cluster node, called \u2019Relay node'.\nAn edge node of the cluster would typically assume this function.  hive_relay  will define which host will be used to relay operations for HIVE, and also how these operations will be performed.  There should be only one entry of this type in the HADeploy definition file.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/hive/hive_relay/#attributes", 
            "text": "hive_relay  is a map with the following attributes:     Name  req?  Description      host  yes  The host on which all hive commands will be pushed for execution. Must be a fully configured as HIVE client.    tools_folder  no  Folder used by HADeploy to install some tools for HIVE management. Default:  /tmp/hadeploy_ user /  where  user  is the  ssh_user  defined for this relay host.    principal  no  A Kerberos principal allowing all HIVE related operations to be performed. See  below    local_keytab_path  no  A local path to the associated keytab file. This path is relative to the embeding file. See  below    relay_keytab_path  no  A path to the associated keytab file on the relay host. See  below    become_user  no  A user account under which all database and table operations will be performed. Only used on non-Kerberos cluster. It will determine table ownership. Note: The  ssh_user  defined for this relay host must have enough rights to switch to this  become_user  using the  become_method  below. Default: No user switch, so the  ssh_user  defined for this relay host will be used.    become_method  no  The method used to swith to this user. Refer to the Ansible documentation on this parameter. Default: Ansible default ( sudo ).    report_file  no  Local path for a report file which will be (re)generated on each run. This YAML file describe all performed operation and eventually un-achievable operation. It could be a starting point for a more sophisticated database migration processing. Under the hood, all HIVE operation are performed by a special tools:  jdchive . You will find more information about the schema of this report file  here    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/hive/hive_relay/#kerberos-authentication", 
            "text": "When  principal  and  ..._keytab_path  variables are defined, Kerberos authentication will be activated for all HIVE operations.    All HIVE operations will be performed on behalf of the user defined by the provided principal.     This principal must have enough rights to be able to create HIVE databases and table.     Regarding the keytab file, two cases:    This keytab file already exists on the relay host. In such case, the  relay_keytab_path  must be set to the location of this file. And the relay host's  ssh_user  must have read access on it.    This keytab file is not present on the relay host. In such case the  local_keytab_path  must be set to its local location. HADeploy will take care of copying it on the remote relay host, \nin a location under  tools_folder . Note you can also modify this target location by setting also the  relay_keytab_path  parameter. In this case, \nit must be the full path, including the keytab file name. And the containing folder must exists.", 
            "title": "Kerberos authentication"
        }, 
        {
            "location": "/plugins_reference/hive/hive_relay/#example", 
            "text": "hive_relay:\n  host: en1  With Kerberos activated:  hive_relay:\n  host: en1\n  principal: hive_admin\n  relay_keytab_path: /etc/security/keytabs/hive_admin.keytab", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/hive/hive_tables/", 
            "text": "hive_tables\n\n\nSynopsis\n\n\nProvide a list of HIVE tables required by the application. They will be created (or destroyed) by HADeploy.\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe name of the table\n\n\n\n\n\n\ndatabase\n\n\nyes\n\n\nThe database this table belong to\n\n\n\n\n\n\nexternal\n\n\nno\n\n\nBoolean. Equivalent to the EXTERNAL Hive DDL clause\n\n\n\n\n\n\nfields\n\n\nno\n\n\nA list of fields definition describing the table's column. Refer to \nField definition\n below\n\n\n\n\n\n\ncomment\n\n\nno\n\n\nAn optional comment associated to the table\n\n\n\n\n\n\nlocation\n\n\nno\n\n\nEquivalent to the LOCATION Hive DDL clause\n\n\n\n\n\n\nproperties\n\n\nno\n\n\nA map of properties. Equivalent to TBLPROPERTIES Hive DDL clause\n\n\n\n\n\n\nstored_as\n\n\nno\n\n\nSpecify the file format, such as SEQUENCEFILE, TEXTEFILE, RCFILE, ....\nEquivalent to STORED AS Hive DDL clause [1]\n\n\n\n\n\n\ninput_format\n\n\nno\n\n\nEquivalent to STORED AS INPUT FORMAT '....' Hive DDL clause [1][2]\n\n\n\n\n\n\noutput_format\n\n\nno\n\n\nEquivalent to STORED AS OUTPUT FORMAT '....' Hive DDL clause [1][2]\n\n\n\n\n\n\ndelimited\n\n\nno\n\n\nA map of delimiter character. Equivalent to ROW FORMAT DELIMITED Hive DDL clause. Refer to \nDelimited row format\n below [3]\n\n\n\n\n\n\nserde\n\n\nno\n\n\nAllow explicit definition of a \nserde\n'. Equivalent to ROW FORMAT SERDE Hive DDL clause [3]\n\n\n\n\n\n\nserde_properties\n\n\nno\n\n\nA map of properties associated to the \nserde\n. Equivalent to WITH SERDEPROPERTIES Hive DDL clause\n\n\n\n\n\n\nstorage_handler\n\n\nno\n\n\nAllow definition of the storage handler. Equivalent to STORED BY Hive DDL clause\n\n\n\n\n\n\npartitions\n\n\nno\n\n\nA list of fields definition describing the table's partitioning. Refer to \nField definition\n  below\n\n\n\n\n\n\nclustered_by\n\n\nno\n\n\nAllow definition of a CLUSTERED BY Hive DDL clause. Refer to \nTable clustering\n below\n\n\n\n\n\n\nskewed_by\n\n\nno\n\n\nAllow definition of a SKEWED BY Hive DDL Clause. Refer to \nSkewed values\n\n\n\n\n\n\nalterable\n\n\nno\n\n\nBoolean. Allow most of ALTER TABLE commands to be automatically issued for table modification. Refer to \nAltering table\n below.\nDefault: \nno\n.\n\n\n\n\n\n\ndroppable\n\n\nno\n\n\nBoolean. Allow this table to be dropped and recreated if definition is modified.\nDefault value is \nyes\n if the table is external, \nno\nfor all other cases\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this database to be removed when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\nranger_policy\n\n\nno\n\n\nDefinition of Apache Ranger policy bound to this table.\nParameters are same as \nhive_ranger_policies\n except than \ndatabase\n, \ntables\n and \ncolumns\n should not be defined. The policy will apply on all columns of this table.\nThe policy name can be explicitly defined. Otherwise, a name will be generated as \"\n_\ndatabase.table\n_\n\".\nSee example below for more information\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\n[1]: Storage format can be defined using two methods:\n\n\n\n\nUse \nstored_by\n. This will define implicitly \ninput_format\n, \noutput_format\n and, for some value the \nserde\n.\n\n\nExplictly define \ninput_format\n, \noutput_format\n and eventually \nserde\n.\n\n\n\n\nThe two approaches are exclusive. Defining both \nstored_by\n and \ninput/output_format\n will generate an error.\n\n\n[2] \ninput_format\n and \noutput_format\n must be defined both if used.\n\n\n[3] \ndelimited\n and \nserde\n are exclusive and can't be defined both.\n\n\nField definition:\n\n\nHere is the definition of a \nfield\n element:\n\n\n\n\n\n\n\n\nField Attribute\n\n\nReq.\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe name of the field\n\n\n\n\n\n\ntype\n\n\nyes\n\n\nThe type of the field. Note this file if used as-is, without interpretation from HADeploy.\nThis will allow to define Hive Complex Types here (arrays, maps, structs,union). See examples below.\n\n\n\n\n\n\ncomment\n\n\nno\n\n\nAn optional comment\n\n\n\n\n\n\n\n\nDelimited row format\n\n\nThe \ndelimited\n map can hold the following values:\n\n\nfields_terminated_by:\nfields_escaped_by:\ncollection_items_terminated_by:\nmap_keys_terminated_by:\nlines_terminated_by:\nnull_defined_as:\n\n\n\n\nThe characters must be expressed between single quote, and can be a regular character, an usual backslash escape sequence, or a unicode value. For example:\n\n\n...\ndelimited:\n  fields_terminated_by: ','\n  map_keys_terminated_by: '\\u0009'  # Same as '\\t'\n  lines_terminated_by: '\\n'\n  null_defined_as: '\\u0001'\n\n\n\n\n\n\nusing octal notation (i.e. '\\001') is not supported.\n\n\n\n\nTable clustering\n\n\nHere is the definition of a \nclustered_by\n element:\n\n\n\n\n\n\n\n\nAttribute\n\n\nReq.\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncolumns\n\n\nyes\n\n\nThis list of columns to CLUSTER BY\n\n\n\n\n\n\nnbr_buckets\n\n\nyes\n\n\nThe number of buckets\n\n\n\n\n\n\nsorted_by\n\n\nno\n\n\nA list of sort item element, as defined just below\n\n\n\n\n\n\n\n\nInner sort item element:\n\n\n\n\n\n\n\n\nAttribute\n\n\nReq.\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncolumns\n\n\nyes\n\n\nA list of column\n\n\n\n\n\n\ndirection\n\n\nno\n\n\nThe direction: \nASC\n or \nDESC\n. Default is \nASC\n\n\n\n\n\n\n\n\nExample:\n\n\n  ...\n  clustered_by:\n    columns:\n    - userid\n    - page_url\n    sorted_by:\n    - { column: userid, direction: asc }\n    - { column: page_url, direction: desc }\n    nbr_buckets: 16\n\n\n\n\nWill be interpreted as:\n\n\n CLUSTERED BY(userid,page_url) SORTED BY (userid asc, page_url desc) INTO 16 BUCKETS\n\n\n\n\nSkewed values\n\n\nHere is the definition of the \nskewed_by\n element:\n\n\n\n\n\n\n\n\nAttribute\n\n\nReq.\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncolumns\n\n\nyes\n\n\nA list of column\n\n\n\n\n\n\nvalues\n\n\nyes\n\n\nA list of list of values\n\n\n\n\n\n\nstored_as_directories\n\n\nno\n\n\nBoolean. Is skewed value stored as directory.\nDefault \nno\n\n\n\n\n\n\n\n\nExample:\n\n\n  ...\n  skewed_by:\n    columns:\n    - key\n    values:\n    - [ 1 ]\n    - [ 5 ]\n    - [ 6 ]\n    stored_as_directories: true\n\n\n\n\nwill be interpreted as:\n\n\nSKEWED BY(key) ON(('1'),('5'),('6')) STORED AS DIRECTORIES\n\n\n\n\nAnd:\n\n\n  skewed_by:\n    columns:\n    - col1\n    - col2\n    values:\n    - [ \ns1\n, 1 ]\n    - [ \ns3\n, 3 ]\n    - [ \ns13\n, 13 ]\n    - [ \ns78\n, 78 ]\n\n\n\n\nwill be interpreted as:\n\n\nSKEWED BY(col1,col2) ON(('s1', 1),('s3', 3),('s13', 13),('s78',78))\n\n\n\n\nAltering table\n\n\nThe must tricky operation with tools like \nHADeploy\n is not table creation, but how it must behave on existing table evolution, specially if theses table already contains some data. \n\n\nIn case of table schema update, operation can be classified in several categories:\n\n\n\n\n\n\nModification which can be performed by issuing one or several ALTER TABLE command and which are independent of data layout. For example, changing a comment. These operations are automatically performed.\n\n\n\n\n\n\nModification which can be performed by issuing one or several ALTER TABLE command, but may introduce a shift between the effective stored data and the new table definition definition. Such operation need to be allowed by setting the \nalterable\nflag to \nyes\n. \n\n\n\n\n\n\n\n\nMost if not all ALTER TABLE commands will only modify Hive's metadata, and will not modify data. Users should make sure the actual data layout of the table/partition conforms with the new metadata definition.\n\n\n\n\n\n\nModification which occurs on table which can be freely dropped without deleting the data. This is the case for example for EXTERNAL tables. In such case, the table is dropped and recreated in case of schema modification. This can be controlled using the \ndroppable\n flag.\n\n\nModification which can't be performed as too complex or there is no corresponding ALTER TABLE command. Such operation should be performed by an external, user defined, script. See below for more information.\n\n\n\n\nDatabase migration.\n\n\nDatabase migration is a complex subject, as it involve not only modifying the database schema, but also adjusting existing data to comply for the new schema.\n\n\nIn some case, like adding a new column, it could be quite simple. But it is generally more involving. \n\n\nJust think of a simple use case: Breaking a \nfull_name\n field (content: i.e. 'Sylvester STALLONE') in two fields: \nfirst_name\n ('Sylvester') and \nlast_name\n ('STALLONE'). It is obvious you will need some application specific code to be executed to perform this.\n\n\nThis problem has been addressed by some tools in the RDBMS fields. But there is no miracle. These tools are in fact more frameworks which globally act the following way:\n\n\n\n\nVersion database schema\n\n\nRequest the user to provide a set of migration scripts to transform database version X to version Y\n\n\nFor each migration, lookup source and target version. And try to find a appropriate sequence of user's migration scripts\n\n\n\n\nCurrently, HADeploy do not provide such solution yet. But, under the hood, all HIVE operation are performed by a special tools: \njdchive\n. This tool has been designed with this migraton pattern in mind. Check \nhere\n\n\nTable ownership\n\n\nAs there is no command such as 'ALTER TABLE SET USER...' the database owner will be the account under which the table creation commands was launched during database creation.\n\n\nThis account can be set be the \nuser\nattribute of the \nhive_relay\n. If not, it will be the \nssh_user\n attached to the host used for relaying (\nhive_relay.host\n).\n\n\nOnce the table is created, there is no way to change this table ownership.\n\n\nExample:\n\n\nhive_tables:\n- name: testSimple\n  database: jdctest1\n  comment: \nA first, simple test table\n\n  location: \n/tmp/xxx\n\n  fields:\n  - name: fname\n    type: string\n    comment: \nFirst name\n\n  - name: lname\n    type: string\n    comment: \nLast name\n\n- name: testRcFile\n  database: jdctest1\n  comment: \nA RCFILE table\n\n  fields: [ { name: fname, type: string, comment: 'The first name' }, { name: lname, type: string } ]\n  stored_as: RCFILE\n\n\n\n\nWill be interpreted, for creation as:\n\n\nCREATE  TABLE jdctest1.testSimple ( fname string COMMENT 'First name', lname string COMMENT 'Last name' ) COMMENT 'A first, simple test table' LOCATION 'hdfs://mycluster/tmp/xxx'\nCREATE  TABLE jdctest1.testRcFile ( fname string COMMENT 'The first name', lname string ) COMMENT 'A RCFILE table' STORED AS RCFILE\n\n\n\n\nAnd:\n\n\nhive_tables:\n- name: testPartitions\n  database: jdctest1\n  comment: \nA table with partitions\n\n  fields:\n  - name: viewTime\n    type: INT\n  - name: userid\n    type: BIGINT\n  - name: page_url\n    type: STRING\n  - name: referrer_url\n    type: STRING\n  - name: ip\n    type: STRING\n    comment: \nIP Address of the User\n\n  partitions:\n  - name: dt\n    type: STRING\n  - name: country\n    type: STRING\n  stored_as: SEQUENCEFILE\n  state: present\n\n\n\n\nWill be interpreted, for creation as:\n\n\nCREATE  TABLE jdctest1.testPartitions ( viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING COMMENT 'IP Address of the User' ) COMMENT 'A table with partitions' PARTITIONED BY ( dt STRING, country STRING ) STORED AS SEQUENCEFILE\n\n\n\n\n\nAnd:\n\n\nhive_tables:\n- name: testSerde\n  database: jdctest1\n  comment: \nSerde test\n\n  fields:\n  - { name: host, type: STRING }\n  - { name: identity, type: STRING }\n  - { name: theuser, type: STRING }\n  serde: \norg.apache.hadoop.hive.serde2.RegexSerDe\n\n  serde_properties:\n    input.regex: \n([^ ]*) ([^ ]*) ([^ ]*)\n\n  state: present\n  alterable: true\n\n\n\n\nWill be interpreted, for creation as:\n\n\nCREATE  TABLE jdctest1.testSerde ( host STRING, identity STRING, theuser STRING) COMMENT 'Serde test' ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe' WITH SERDEPROPERTIES ( 'input.regex'='([^ ]*) ([^ ]*) ([^ ]*)')\n\n\n\n\nComplex Types example\n\n\nHere, the field \nvalue\n is not a simple scalar one:\n\n\n- name: testComplex\n  database: jdctest1\n  external: true\n  fields:\n  - { name: compressed, type: boolean }\n  - { name: value, type: \nstruct\ncontentType:string, message:string, sender:string, properties: array\nstruct\nkey:string,value:int\n, type:string\n }\n  - { name: timestamp, type: string }\n\n\n\n\nAnd same example, but expressed differently:\n\n\n- name: testComplex\n  database: jdctest1\n  external: true\n  fields:\n  - { name: compressed, type: boolean }\n  - name: value\n    type: |\n      struct\n\n        contentType:string, \n        message:string, \n        sender:string, \n        properties: array\n\n          struct\n\n            key:string,\n            value:int\n          \n\n        \n, \n        type:string\n      \n\n  - { name: timestamp, type: string }\n\n\n\n\nApache Ranger example\n\n\nFollowing is an illustration of Apache Ranger policy association: The table is created with select and update permissions for all users of the 'users' group. And user 'sa' can also create new indexes.\n\n\nhive_tables:\n- name: testranger\n  database: jdctest1\n  comment: \nA first, simple test table\n\n  location: \n/tmp/xxx\n\n  fields:\n  - name: fname\n    type: string\n    comment: \nFirst name\n\n  - name: lname\n    type: string\n    comment: \nLast name\n\n  ranger_policy:\n    permissions:\n    - groups:\n      - users\n      accesses: \n      - select\n      - update\n    - users:\n      - sa\n      accesses:\n      - index\n\n\n\n\nHBase table mapping\n\n\nA frequent use case is the mapping of HBase table by HIVE, to ease querying. This can be achieved using HADeploy. \n\n\nExample:\n\n\nProvided the following HBase table definition:\n\n\nhbase_namespaces:\n- name: test2\n\nhbase_tables:\n  - name: test2a\n    namespace: test2\n    column_families:\n    - name: id\n    - name: job\n\n\n\n\n(Refer to \nhbase_namespaces\n and \nhbase_tables\n for more informations\n\n\nOne can easely map an external HIVE table: \n\n\nhive_tables\n- name: testHBase\n  database: jdctest1\n  fields:\n  - { name: rowkey, type: string, comment: \nThe row key\n }\n  - { name: number, type: int }\n  - { name: prefix, type: string }\n  - { name: fname, type: string }\n  - { name: lname, type: string }\n  - { name: company, type: string }\n  - { name: title, type: string }\n  external: true\n  storage_handler: \norg.apache.hadoop.hive.hbase.HBaseStorageHandler\n\n  properties:\n    hbase.table.name: \ntest2:test2a\n\n  serde_properties:\n    hbase.columns.mapping: \n:key,id:reg,id:prefix,id:fname,id:lname,job:cpny,job:title\n\n  state: present\n\n\n\n\nWill be interpreted, for creation as:\n\n\nCREATE EXTERNAL TABLE jdctest1.testHBase ( rowkey string COMMENT 'The row key', number int, prefix string, fname string, lname string, company string, title string ) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ( 'hbase.columns.mapping'=':key,id:reg,id:prefix,id:fname,id:lname,job:cpny,job:title') TBLPROPERTIES ( 'hbase.table.name'='test2:test2a')", 
            "title": "hive_tables"
        }, 
        {
            "location": "/plugins_reference/hive/hive_tables/#hive_tables", 
            "text": "", 
            "title": "hive_tables"
        }, 
        {
            "location": "/plugins_reference/hive/hive_tables/#synopsis", 
            "text": "Provide a list of HIVE tables required by the application. They will be created (or destroyed) by HADeploy.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/hive/hive_tables/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  The name of the table    database  yes  The database this table belong to    external  no  Boolean. Equivalent to the EXTERNAL Hive DDL clause    fields  no  A list of fields definition describing the table's column. Refer to  Field definition  below    comment  no  An optional comment associated to the table    location  no  Equivalent to the LOCATION Hive DDL clause    properties  no  A map of properties. Equivalent to TBLPROPERTIES Hive DDL clause    stored_as  no  Specify the file format, such as SEQUENCEFILE, TEXTEFILE, RCFILE, .... Equivalent to STORED AS Hive DDL clause [1]    input_format  no  Equivalent to STORED AS INPUT FORMAT '....' Hive DDL clause [1][2]    output_format  no  Equivalent to STORED AS OUTPUT FORMAT '....' Hive DDL clause [1][2]    delimited  no  A map of delimiter character. Equivalent to ROW FORMAT DELIMITED Hive DDL clause. Refer to  Delimited row format  below [3]    serde  no  Allow explicit definition of a  serde '. Equivalent to ROW FORMAT SERDE Hive DDL clause [3]    serde_properties  no  A map of properties associated to the  serde . Equivalent to WITH SERDEPROPERTIES Hive DDL clause    storage_handler  no  Allow definition of the storage handler. Equivalent to STORED BY Hive DDL clause    partitions  no  A list of fields definition describing the table's partitioning. Refer to  Field definition   below    clustered_by  no  Allow definition of a CLUSTERED BY Hive DDL clause. Refer to  Table clustering  below    skewed_by  no  Allow definition of a SKEWED BY Hive DDL Clause. Refer to  Skewed values    alterable  no  Boolean. Allow most of ALTER TABLE commands to be automatically issued for table modification. Refer to  Altering table  below. Default:  no .    droppable  no  Boolean. Allow this table to be dropped and recreated if definition is modified. Default value is  yes  if the table is external,  no for all other cases    no_remove  no  Boolean: Prevent this database to be removed when HADeploy will be used in REMOVE mode. Default:  no    ranger_policy  no  Definition of Apache Ranger policy bound to this table. Parameters are same as  hive_ranger_policies  except than  database ,  tables  and  columns  should not be defined. The policy will apply on all columns of this table. The policy name can be explicitly defined. Otherwise, a name will be generated as \" _ database.table _ \". See example below for more information    when  no  Boolean. Allow  conditional deployment  of this item. Default  True     [1]: Storage format can be defined using two methods:   Use  stored_by . This will define implicitly  input_format ,  output_format  and, for some value the  serde .  Explictly define  input_format ,  output_format  and eventually  serde .   The two approaches are exclusive. Defining both  stored_by  and  input/output_format  will generate an error.  [2]  input_format  and  output_format  must be defined both if used.  [3]  delimited  and  serde  are exclusive and can't be defined both.", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/hive/hive_tables/#field-definition", 
            "text": "Here is the definition of a  field  element:     Field Attribute  Req.  Description      name  yes  The name of the field    type  yes  The type of the field. Note this file if used as-is, without interpretation from HADeploy. This will allow to define Hive Complex Types here (arrays, maps, structs,union). See examples below.    comment  no  An optional comment", 
            "title": "Field definition:"
        }, 
        {
            "location": "/plugins_reference/hive/hive_tables/#delimited-row-format", 
            "text": "The  delimited  map can hold the following values:  fields_terminated_by:\nfields_escaped_by:\ncollection_items_terminated_by:\nmap_keys_terminated_by:\nlines_terminated_by:\nnull_defined_as:  The characters must be expressed between single quote, and can be a regular character, an usual backslash escape sequence, or a unicode value. For example:  ...\ndelimited:\n  fields_terminated_by: ','\n  map_keys_terminated_by: '\\u0009'  # Same as '\\t'\n  lines_terminated_by: '\\n'\n  null_defined_as: '\\u0001'   using octal notation (i.e. '\\001') is not supported.", 
            "title": "Delimited row format"
        }, 
        {
            "location": "/plugins_reference/hive/hive_tables/#table-clustering", 
            "text": "Here is the definition of a  clustered_by  element:     Attribute  Req.  Description      columns  yes  This list of columns to CLUSTER BY    nbr_buckets  yes  The number of buckets    sorted_by  no  A list of sort item element, as defined just below     Inner sort item element:     Attribute  Req.  Description      columns  yes  A list of column    direction  no  The direction:  ASC  or  DESC . Default is  ASC     Example:    ...\n  clustered_by:\n    columns:\n    - userid\n    - page_url\n    sorted_by:\n    - { column: userid, direction: asc }\n    - { column: page_url, direction: desc }\n    nbr_buckets: 16  Will be interpreted as:   CLUSTERED BY(userid,page_url) SORTED BY (userid asc, page_url desc) INTO 16 BUCKETS", 
            "title": "Table clustering"
        }, 
        {
            "location": "/plugins_reference/hive/hive_tables/#skewed-values", 
            "text": "Here is the definition of the  skewed_by  element:     Attribute  Req.  Description      columns  yes  A list of column    values  yes  A list of list of values    stored_as_directories  no  Boolean. Is skewed value stored as directory. Default  no     Example:    ...\n  skewed_by:\n    columns:\n    - key\n    values:\n    - [ 1 ]\n    - [ 5 ]\n    - [ 6 ]\n    stored_as_directories: true  will be interpreted as:  SKEWED BY(key) ON(('1'),('5'),('6')) STORED AS DIRECTORIES  And:    skewed_by:\n    columns:\n    - col1\n    - col2\n    values:\n    - [  s1 , 1 ]\n    - [  s3 , 3 ]\n    - [  s13 , 13 ]\n    - [  s78 , 78 ]  will be interpreted as:  SKEWED BY(col1,col2) ON(('s1', 1),('s3', 3),('s13', 13),('s78',78))", 
            "title": "Skewed values"
        }, 
        {
            "location": "/plugins_reference/hive/hive_tables/#altering-table", 
            "text": "The must tricky operation with tools like  HADeploy  is not table creation, but how it must behave on existing table evolution, specially if theses table already contains some data.   In case of table schema update, operation can be classified in several categories:    Modification which can be performed by issuing one or several ALTER TABLE command and which are independent of data layout. For example, changing a comment. These operations are automatically performed.    Modification which can be performed by issuing one or several ALTER TABLE command, but may introduce a shift between the effective stored data and the new table definition definition. Such operation need to be allowed by setting the  alterable flag to  yes .      Most if not all ALTER TABLE commands will only modify Hive's metadata, and will not modify data. Users should make sure the actual data layout of the table/partition conforms with the new metadata definition.    Modification which occurs on table which can be freely dropped without deleting the data. This is the case for example for EXTERNAL tables. In such case, the table is dropped and recreated in case of schema modification. This can be controlled using the  droppable  flag.  Modification which can't be performed as too complex or there is no corresponding ALTER TABLE command. Such operation should be performed by an external, user defined, script. See below for more information.", 
            "title": "Altering table"
        }, 
        {
            "location": "/plugins_reference/hive/hive_tables/#database-migration", 
            "text": "Database migration is a complex subject, as it involve not only modifying the database schema, but also adjusting existing data to comply for the new schema.  In some case, like adding a new column, it could be quite simple. But it is generally more involving.   Just think of a simple use case: Breaking a  full_name  field (content: i.e. 'Sylvester STALLONE') in two fields:  first_name  ('Sylvester') and  last_name  ('STALLONE'). It is obvious you will need some application specific code to be executed to perform this.  This problem has been addressed by some tools in the RDBMS fields. But there is no miracle. These tools are in fact more frameworks which globally act the following way:   Version database schema  Request the user to provide a set of migration scripts to transform database version X to version Y  For each migration, lookup source and target version. And try to find a appropriate sequence of user's migration scripts   Currently, HADeploy do not provide such solution yet. But, under the hood, all HIVE operation are performed by a special tools:  jdchive . This tool has been designed with this migraton pattern in mind. Check  here", 
            "title": "Database migration."
        }, 
        {
            "location": "/plugins_reference/hive/hive_tables/#table-ownership", 
            "text": "As there is no command such as 'ALTER TABLE SET USER...' the database owner will be the account under which the table creation commands was launched during database creation.  This account can be set be the  user attribute of the  hive_relay . If not, it will be the  ssh_user  attached to the host used for relaying ( hive_relay.host ).  Once the table is created, there is no way to change this table ownership.", 
            "title": "Table ownership"
        }, 
        {
            "location": "/plugins_reference/hive/hive_tables/#example", 
            "text": "hive_tables:\n- name: testSimple\n  database: jdctest1\n  comment:  A first, simple test table \n  location:  /tmp/xxx \n  fields:\n  - name: fname\n    type: string\n    comment:  First name \n  - name: lname\n    type: string\n    comment:  Last name \n- name: testRcFile\n  database: jdctest1\n  comment:  A RCFILE table \n  fields: [ { name: fname, type: string, comment: 'The first name' }, { name: lname, type: string } ]\n  stored_as: RCFILE  Will be interpreted, for creation as:  CREATE  TABLE jdctest1.testSimple ( fname string COMMENT 'First name', lname string COMMENT 'Last name' ) COMMENT 'A first, simple test table' LOCATION 'hdfs://mycluster/tmp/xxx'\nCREATE  TABLE jdctest1.testRcFile ( fname string COMMENT 'The first name', lname string ) COMMENT 'A RCFILE table' STORED AS RCFILE  And:  hive_tables:\n- name: testPartitions\n  database: jdctest1\n  comment:  A table with partitions \n  fields:\n  - name: viewTime\n    type: INT\n  - name: userid\n    type: BIGINT\n  - name: page_url\n    type: STRING\n  - name: referrer_url\n    type: STRING\n  - name: ip\n    type: STRING\n    comment:  IP Address of the User \n  partitions:\n  - name: dt\n    type: STRING\n  - name: country\n    type: STRING\n  stored_as: SEQUENCEFILE\n  state: present  Will be interpreted, for creation as:  CREATE  TABLE jdctest1.testPartitions ( viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING COMMENT 'IP Address of the User' ) COMMENT 'A table with partitions' PARTITIONED BY ( dt STRING, country STRING ) STORED AS SEQUENCEFILE  And:  hive_tables:\n- name: testSerde\n  database: jdctest1\n  comment:  Serde test \n  fields:\n  - { name: host, type: STRING }\n  - { name: identity, type: STRING }\n  - { name: theuser, type: STRING }\n  serde:  org.apache.hadoop.hive.serde2.RegexSerDe \n  serde_properties:\n    input.regex:  ([^ ]*) ([^ ]*) ([^ ]*) \n  state: present\n  alterable: true  Will be interpreted, for creation as:  CREATE  TABLE jdctest1.testSerde ( host STRING, identity STRING, theuser STRING) COMMENT 'Serde test' ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe' WITH SERDEPROPERTIES ( 'input.regex'='([^ ]*) ([^ ]*) ([^ ]*)')", 
            "title": "Example:"
        }, 
        {
            "location": "/plugins_reference/hive/hive_tables/#complex-types-example", 
            "text": "Here, the field  value  is not a simple scalar one:  - name: testComplex\n  database: jdctest1\n  external: true\n  fields:\n  - { name: compressed, type: boolean }\n  - { name: value, type:  struct contentType:string, message:string, sender:string, properties: array struct key:string,value:int , type:string  }\n  - { name: timestamp, type: string }  And same example, but expressed differently:  - name: testComplex\n  database: jdctest1\n  external: true\n  fields:\n  - { name: compressed, type: boolean }\n  - name: value\n    type: |\n      struct \n        contentType:string, \n        message:string, \n        sender:string, \n        properties: array \n          struct \n            key:string,\n            value:int\n           \n         , \n        type:string\n       \n  - { name: timestamp, type: string }", 
            "title": "Complex Types example"
        }, 
        {
            "location": "/plugins_reference/hive/hive_tables/#apache-ranger-example", 
            "text": "Following is an illustration of Apache Ranger policy association: The table is created with select and update permissions for all users of the 'users' group. And user 'sa' can also create new indexes.  hive_tables:\n- name: testranger\n  database: jdctest1\n  comment:  A first, simple test table \n  location:  /tmp/xxx \n  fields:\n  - name: fname\n    type: string\n    comment:  First name \n  - name: lname\n    type: string\n    comment:  Last name \n  ranger_policy:\n    permissions:\n    - groups:\n      - users\n      accesses: \n      - select\n      - update\n    - users:\n      - sa\n      accesses:\n      - index", 
            "title": "Apache Ranger example"
        }, 
        {
            "location": "/plugins_reference/hive/hive_tables/#hbase-table-mapping", 
            "text": "A frequent use case is the mapping of HBase table by HIVE, to ease querying. This can be achieved using HADeploy.", 
            "title": "HBase table mapping"
        }, 
        {
            "location": "/plugins_reference/hive/hive_tables/#example_1", 
            "text": "Provided the following HBase table definition:  hbase_namespaces:\n- name: test2\n\nhbase_tables:\n  - name: test2a\n    namespace: test2\n    column_families:\n    - name: id\n    - name: job  (Refer to  hbase_namespaces  and  hbase_tables  for more informations  One can easely map an external HIVE table:   hive_tables\n- name: testHBase\n  database: jdctest1\n  fields:\n  - { name: rowkey, type: string, comment:  The row key  }\n  - { name: number, type: int }\n  - { name: prefix, type: string }\n  - { name: fname, type: string }\n  - { name: lname, type: string }\n  - { name: company, type: string }\n  - { name: title, type: string }\n  external: true\n  storage_handler:  org.apache.hadoop.hive.hbase.HBaseStorageHandler \n  properties:\n    hbase.table.name:  test2:test2a \n  serde_properties:\n    hbase.columns.mapping:  :key,id:reg,id:prefix,id:fname,id:lname,job:cpny,job:title \n  state: present  Will be interpreted, for creation as:  CREATE EXTERNAL TABLE jdctest1.testHBase ( rowkey string COMMENT 'The row key', number int, prefix string, fname string, lname string, company string, title string ) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ( 'hbase.columns.mapping'=':key,id:reg,id:prefix,id:fname,id:lname,job:cpny,job:title') TBLPROPERTIES ( 'hbase.table.name'='test2:test2a')", 
            "title": "Example:"
        }, 
        {
            "location": "/plugins_reference/inventory/exit_on_fail/", 
            "text": "exit_on_fail\n\n\nSynopsis\n\n\nA boolean flag. \n\n\n\n\nIf \nyes\n, all processing will exit on first failure.\n\n\nIf \nno\n, if an host fail, it will be excluded. But the processing will continue on other hosts.\n\n\n\n\nDefault value: `yes\n\n\nExample\n\n\nexit_on_fail: No", 
            "title": "exit_on_fail"
        }, 
        {
            "location": "/plugins_reference/inventory/exit_on_fail/#exit_on_fail", 
            "text": "", 
            "title": "exit_on_fail"
        }, 
        {
            "location": "/plugins_reference/inventory/exit_on_fail/#synopsis", 
            "text": "A boolean flag.    If  yes , all processing will exit on first failure.  If  no , if an host fail, it will be excluded. But the processing will continue on other hosts.   Default value: `yes", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/inventory/exit_on_fail/#example", 
            "text": "exit_on_fail: No", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/inventory/hosts/", 
            "text": "hosts\n\n\nSynopsis\n\n\nProvide a list of hosts describing the target cluster.\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe host name. May be whatever you want. HADeploy will always refer to this host by this name.\n\n\n\n\n\n\nforce_setup\n\n\nno\n\n\nA common Ansible problem is when referencing host info for a host which has not being accessed, so there is no fact grabbed for it.\nSetting this flag to \nyes\n will trigger an access to this host at the begining of the play.\nDefault: \nno\n\n\n\n\n\n\nssh_host\n\n\nyes\n\n\nHow to reach this host using ssh from the HADeploy node. Typically the FQDN. May also be the IP address.\n\n\n\n\n\n\nssh_user\n\n\nyes\n\n\nThis user account under which HADeploy will perform its operation. Typically root.\n\n\n\n\n\n\nssh_private_key_file\n\n\nno\n\n\nThe path to the private key file granting no password access to this host. If this path is not absolute, it will be relative to the HADeploy embedding file location.\n\n\n\n\n\n\nssh_password\n\n\nno\n\n\nThe password to provide to access this host. This may be encrypted. Refer to \nencrypted variables\n\n\n\n\n\n\nssh_extra_args\n\n\nno\n\n\nThis setting is always appended to the default ssh command line.\n\n\n\n\n\n\nssh_port\n\n\nno\n\n\nThe ssh port number, if not 22\n\n\n\n\n\n\nssh_common_args\n\n\nno\n\n\nThis setting is always appended to the default command line for sftp, scp, and ssh. Useful to configure a ProxyCommand for a certain host (or group).\n\n\n\n\n\n\nsftp_extra_args\n\n\nno\n\n\nThis setting is always appended to the default sftp command line.\n\n\n\n\n\n\nscp_extra_args\n\n\nno\n\n\nThis setting is always appended to the default scp command line.\n\n\n\n\n\n\nssh_pipelining\n\n\nno\n\n\nDetermines whether or not to use SSH pipelining.\n\n\n\n\n\n\nssh_executable\n\n\nno\n\n\nThis setting overrides the default behavior to use the system ssh.\n\n\n\n\n\n\nbecome\n\n\nno\n\n\nAllows to force privilege escalation\n\n\n\n\n\n\nbecome_method\n\n\nno\n\n\nAllows to set privilege escalation method\n\n\n\n\n\n\nbecome_user\n\n\nno\n\n\nAllows to set the user you become through privilege escalation\n\n\n\n\n\n\nbecome_pass\n\n\nno\n\n\nAllows you to set the privilege escalation password. This may be encrypted. Refer to \nencrypted variables\n\n\n\n\n\n\nbecome_exe\n\n\nno\n\n\nAllows you to set the executable for the escalation method selected\n\n\n\n\n\n\nbecome_flags\n\n\nno\n\n\nAllows you to set the flags passed to the selected escalation method.\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\n\n\nIf the user launching HADeploy have itself a private key granting access to all the hosts, there is no need to define \nssh_private_key_file\n and \nssh_password\n in any Ansible or HADeploy file.\n\n\n\n\nExample\n\n\nhosts:\n- name: sr\n  ssh_host: sr.cluster1.mydomain.com\n  ssh_user: root\n  ssh_private_key_file: \nkeys/build_key\n \n  ssh_extra_args: \n-o UserKnownHostsFile=/dev/null\n\n\n\n\n\nTricks\n\n\nIf, when running HADeploy you encounter error like:\n\n\nfatal: [dn1]: FAILED! =\n {\nchanged\n: false, \nfailed\n: true, \nmsg\n: \nAnsibleUndefinedVariable: 'dict object' has no attribute 'ansible_fqdn'\n}\n\n\n\n\nit is most likely that you need to set \nforce_setup\n on some host_group or host.", 
            "title": "hosts"
        }, 
        {
            "location": "/plugins_reference/inventory/hosts/#hosts", 
            "text": "", 
            "title": "hosts"
        }, 
        {
            "location": "/plugins_reference/inventory/hosts/#synopsis", 
            "text": "Provide a list of hosts describing the target cluster.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/inventory/hosts/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  The host name. May be whatever you want. HADeploy will always refer to this host by this name.    force_setup  no  A common Ansible problem is when referencing host info for a host which has not being accessed, so there is no fact grabbed for it. Setting this flag to  yes  will trigger an access to this host at the begining of the play. Default:  no    ssh_host  yes  How to reach this host using ssh from the HADeploy node. Typically the FQDN. May also be the IP address.    ssh_user  yes  This user account under which HADeploy will perform its operation. Typically root.    ssh_private_key_file  no  The path to the private key file granting no password access to this host. If this path is not absolute, it will be relative to the HADeploy embedding file location.    ssh_password  no  The password to provide to access this host. This may be encrypted. Refer to  encrypted variables    ssh_extra_args  no  This setting is always appended to the default ssh command line.    ssh_port  no  The ssh port number, if not 22    ssh_common_args  no  This setting is always appended to the default command line for sftp, scp, and ssh. Useful to configure a ProxyCommand for a certain host (or group).    sftp_extra_args  no  This setting is always appended to the default sftp command line.    scp_extra_args  no  This setting is always appended to the default scp command line.    ssh_pipelining  no  Determines whether or not to use SSH pipelining.    ssh_executable  no  This setting overrides the default behavior to use the system ssh.    become  no  Allows to force privilege escalation    become_method  no  Allows to set privilege escalation method    become_user  no  Allows to set the user you become through privilege escalation    become_pass  no  Allows you to set the privilege escalation password. This may be encrypted. Refer to  encrypted variables    become_exe  no  Allows you to set the executable for the escalation method selected    become_flags  no  Allows you to set the flags passed to the selected escalation method.    when  no  Boolean. Allow  conditional deployment  of this item. Default  True      If the user launching HADeploy have itself a private key granting access to all the hosts, there is no need to define  ssh_private_key_file  and  ssh_password  in any Ansible or HADeploy file.", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/inventory/hosts/#example", 
            "text": "hosts:\n- name: sr\n  ssh_host: sr.cluster1.mydomain.com\n  ssh_user: root\n  ssh_private_key_file:  keys/build_key  \n  ssh_extra_args:  -o UserKnownHostsFile=/dev/null", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/inventory/hosts/#tricks", 
            "text": "If, when running HADeploy you encounter error like:  fatal: [dn1]: FAILED! =  { changed : false,  failed : true,  msg :  AnsibleUndefinedVariable: 'dict object' has no attribute 'ansible_fqdn' }  it is most likely that you need to set  force_setup  on some host_group or host.", 
            "title": "Tricks"
        }, 
        {
            "location": "/plugins_reference/inventory/host_groups/", 
            "text": "host_groups\n\n\nSynopsis\n\n\nProvide a list of group of hosts.\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe name of the host group\n\n\n\n\n\n\nhosts\n\n\nno\n\n\nA list of hosts included in this group. Each host must be defined in the \nhosts:\n part (Or the associated Ansible inventory).\n\n\n\n\n\n\ngroups\n\n\nno\n\n\nA list of \nhost_group\n from wich all hosts will be included in this group. Allow group composition, and also group renaming.\n\n\n\n\n\n\nforce_setup\n\n\nno\n\n\nA common Ansible problem is when referencing host info for a host which has not being accessed, so there is no fact grabbed for it.\nSetting this flag to \nyes\n will trigger an access to all hosts of this group at the begining of the play.\nDefault: \nno\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nNote than same host can belong to several groups.\n\n\nExample\n\n\n\nhost_groups:\n- name: data_nodes\n  hosts:    # This host list in standard YAML style\n  - dn1\n  - dn2\n  - dn3\n\n- name: control_nodes\n  hosts: [ \nsr\n, \nen\n, \nnn1\n, \nnn2\n ]   # And these in YAML 'flow style\n\n- name: empty_group\n\n# Group renaming\n- name: brokers\n  groups:\n  - kafka_brokers\n\n\n\n\nTricks\n\n\nIf, when running HADeploy you encounter error like:\n\n\nfatal: [dn1]: FAILED! =\n {\nchanged\n: false, \nfailed\n: true, \nmsg\n: \nAnsibleUndefinedVariable: 'dict object' has no attribute 'ansible_fqdn'\n}\n\n\n\n\nit is most likely that you need to set \nforce_setup\n on some host_group or host.", 
            "title": "host_groups"
        }, 
        {
            "location": "/plugins_reference/inventory/host_groups/#host_groups", 
            "text": "", 
            "title": "host_groups"
        }, 
        {
            "location": "/plugins_reference/inventory/host_groups/#synopsis", 
            "text": "Provide a list of group of hosts.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/inventory/host_groups/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  The name of the host group    hosts  no  A list of hosts included in this group. Each host must be defined in the  hosts:  part (Or the associated Ansible inventory).    groups  no  A list of  host_group  from wich all hosts will be included in this group. Allow group composition, and also group renaming.    force_setup  no  A common Ansible problem is when referencing host info for a host which has not being accessed, so there is no fact grabbed for it. Setting this flag to  yes  will trigger an access to all hosts of this group at the begining of the play. Default:  no    when  no  Boolean. Allow  conditional deployment  of this item. Default  True     Note than same host can belong to several groups.", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/inventory/host_groups/#example", 
            "text": "host_groups:\n- name: data_nodes\n  hosts:    # This host list in standard YAML style\n  - dn1\n  - dn2\n  - dn3\n\n- name: control_nodes\n  hosts: [  sr ,  en ,  nn1 ,  nn2  ]   # And these in YAML 'flow style\n\n- name: empty_group\n\n# Group renaming\n- name: brokers\n  groups:\n  - kafka_brokers", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/inventory/host_groups/#tricks", 
            "text": "If, when running HADeploy you encounter error like:  fatal: [dn1]: FAILED! =  { changed : false,  failed : true,  msg :  AnsibleUndefinedVariable: 'dict object' has no attribute 'ansible_fqdn' }  it is most likely that you need to set  force_setup  on some host_group or host.", 
            "title": "Tricks"
        }, 
        {
            "location": "/plugins_reference/inventory/host_group_overrides/", 
            "text": "host_group_overrides\n\n\nSynopsis\n\n\nProvide a list of host group overriding values, to modify an existing inventory.\n\n\nTypically used to patch an Ansible inventory (see \nansible_inventory_files\n) with some new values\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe name of the host to patch. Must be an existing host. Also may be 'all' or '*' to patch all hosts.\n\n\n\n\n\n\nhosts\n\n\nno\n\n\nAllow to override the original hosts list\n\n\n\n\n\n\nforce_setup\n\n\nno\n\n\nA common Ansible problem is when referencing host info for a host which has not being accessed, so there is no fact grabbed for it.\nSetting this flag to \nyes\n will trigger an access to all hosts of this group at the begining of the play.\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nExample\n\n\nA common pitfall is the infamous \nAnsibleUndefinedVariable: 'dict object' has no attribute 'ansible_fqdn'\n message. \nThis is due to the fact we are referencing some variables (\nansible_fqdn\n) of hosts which are not part of the playbook.\n\n\nThe following example force all hosts of the \nzookeepers\n group to be included at the top of the playbook \n\n\nhost_group_overrides:\n- name: zookeepers\n  force_setup: yes\n\n\n\n\nTricks\n\n\nIf, when running HADeploy you encounter error like:\n\n\nfatal: [dn1]: FAILED! =\n {\nchanged\n: false, \nfailed\n: true, \nmsg\n: \nAnsibleUndefinedVariable: 'dict object' has no attribute 'ansible_fqdn'\n}\n\n\n\n\nit is most likely that you need to set \nforce_setup\n on some host_group or host.", 
            "title": "host_group_overrides"
        }, 
        {
            "location": "/plugins_reference/inventory/host_group_overrides/#host_group_overrides", 
            "text": "", 
            "title": "host_group_overrides"
        }, 
        {
            "location": "/plugins_reference/inventory/host_group_overrides/#synopsis", 
            "text": "Provide a list of host group overriding values, to modify an existing inventory.  Typically used to patch an Ansible inventory (see  ansible_inventory_files ) with some new values", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/inventory/host_group_overrides/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  The name of the host to patch. Must be an existing host. Also may be 'all' or '*' to patch all hosts.    hosts  no  Allow to override the original hosts list    force_setup  no  A common Ansible problem is when referencing host info for a host which has not being accessed, so there is no fact grabbed for it. Setting this flag to  yes  will trigger an access to all hosts of this group at the begining of the play.    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/inventory/host_group_overrides/#example", 
            "text": "A common pitfall is the infamous  AnsibleUndefinedVariable: 'dict object' has no attribute 'ansible_fqdn'  message. \nThis is due to the fact we are referencing some variables ( ansible_fqdn ) of hosts which are not part of the playbook.  The following example force all hosts of the  zookeepers  group to be included at the top of the playbook   host_group_overrides:\n- name: zookeepers\n  force_setup: yes", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/inventory/host_group_overrides/#tricks", 
            "text": "If, when running HADeploy you encounter error like:  fatal: [dn1]: FAILED! =  { changed : false,  failed : true,  msg :  AnsibleUndefinedVariable: 'dict object' has no attribute 'ansible_fqdn' }  it is most likely that you need to set  force_setup  on some host_group or host.", 
            "title": "Tricks"
        }, 
        {
            "location": "/plugins_reference/inventory/host_overrides/", 
            "text": "host_overrides\n\n\nSynopsis\n\n\nProvide a list of host overriding values, to modify an existing inventory.\n\n\nTypically used to patch an Ansible inventory (see ansible_inventory_file) with some new values\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe name of the host to patch. Must be an existing host. Also may be 'all' or '*' to patch all hosts.\n\n\n\n\n\n\nforce_setup\n\n\nno\n\n\nA common Ansible problem is when referencing host info for a host which has not being accessed, so there is no fact grabbed for it.\nSetting this flag to \nyes\n will trigger an access to this host at the begining of the play.\n\n\n\n\n\n\nssh_host\n\n\nno\n\n\nAllow to override ssh_host\n\n\n\n\n\n\nssh_user\n\n\nno\n\n\nAllow to override ssh_user\n\n\n\n\n\n\nssh_private_key_file\n\n\nno\n\n\nAllow to override ssh_private_key_file\n\n\n\n\n\n\nssh_password\n\n\nno\n\n\nAllow to override ssh_password. This may be encrypted. Refer to \nencrypted variables\n\n\n\n\n\n\nssh_port\n\n\nno\n\n\nAllow to override ssh_port\n\n\n\n\n\n\nssh_extra_args\n\n\nno\n\n\nAllow to override ssh_extra_args\n\n\n\n\n\n\nssh_common_args\n\n\nno\n\n\nAllow to override ssh_common_args\n\n\n\n\n\n\nsftp_extra_args\n\n\nno\n\n\nAllow to override sftp_extra_args\n\n\n\n\n\n\nscp_extra_args\n\n\nno\n\n\nAllow to override scp_extra_args\n\n\n\n\n\n\nssh_pipelining\n\n\nno\n\n\nAllow to override ssh_pipelining\n\n\n\n\n\n\nssh_executable\n\n\nno\n\n\nAllow to override ssh_executable\n\n\n\n\n\n\nbecome\n\n\nno\n\n\nAllow to override become\n\n\n\n\n\n\nbecome_method\n\n\nno\n\n\nAllow to override become_method\n\n\n\n\n\n\nbecome_user\n\n\nno\n\n\nAllow to override become_user\n\n\n\n\n\n\nbecome_pass\n\n\nno\n\n\nAllow to override become_pass. This may be encrypted. Refer to \nencrypted variables\n\n\n\n\n\n\nbecome_exe\n\n\nno\n\n\nAllow to override become_exe\n\n\n\n\n\n\npriority\n\n\nno\n\n\nAn integer number allowing to order overriding in case of multiple \nhost_override\n on the same host(s). See example below.\nDefault: 100\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\n\n\nTo suppress an attribute, set it to empty string: ''\n\n\n\n\nExamples\n\n\nWe want to address a VM built using Vagrant. For this, we can use the Vagrant generated ansible inventory. Such inventory instruct HADeploy to access the VM under the \nvagrant\naccount. \nSo, we need to instruct HADeploy to become \nroot\nin order to perform privileged operation. \n\n\nansible_inventories: \n- file: \n../../iac/vgrvm1/.vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory\n\n\nhost_overrides:\n- name: all\n  become_user: root\n\n\n\n\nOf course, this works because the \nvagrant\n account has been granted by vagrant with appropriate sudo rights. \n\n\nThis other example change the user for all HADeploy action for all hosts. And authenticate with a (encrypted) password instead of a key:\n\n\n\nencrypted_vars:\n  deployer_password: |\n    $ANSIBLE_VAULT;1.1;AES256\n    65626166653134326137613232323336373139393036383532333863623630363662303531306539\n    6637306363343836376633353439656634613638643031660a636238323663353337313333663438\n    30306234306463626338663637623563393735653237323833323064316561653237393538303762\n    6363326232656461310a656631386135663764386565366566633537633665646562626236393462\n    6231\n\nhost_overrides:\n- name: all\n  ssh_user: deployer      \n  ssh_password: \n{{deployer_password}}\n\n  ssh_private_key_file: ''\n\n\n\n\nRefer to \nencrypted variables\n for more information.\n\n\nPriority\n\n\nHere is an illustration of the \npriority\n attribute usage: \n\n\nhost_overrides:\n- name: all\n  become_user: root\n\n....  \n\nhost_overrides:\n- name: all\n  become_user: ''\n  priority: 110\n\n\n\n\nThis will result with \nbecome_user\n not set. \n\n\nWhile:\n\n\nhost_overrides:\n- name: all\n  become_user: root\n\n....  \n\nhost_overrides:\n- name: all\n  become_user: ''\n  priority: 90\n\n\n\n\nWill result with \nbecome_user\n set to \nroot\n. (Default value is 100, so the first one take precedence).\n\n\nThis feature may be usefull when building complex configuration by merging several inventory files.\n\n\nTricks\n\n\nIf, when running HADeploy you encounter error like:\n\n\nfatal: [dn1]: FAILED! =\n {\nchanged\n: false, \nfailed\n: true, \nmsg\n: \nAnsibleUndefinedVariable: 'dict object' has no attribute 'ansible_fqdn'\n}\n\n\n\n\nit is most likely that you need to set \nforce_setup\n on some host_group or host.", 
            "title": "host_overrides"
        }, 
        {
            "location": "/plugins_reference/inventory/host_overrides/#host_overrides", 
            "text": "", 
            "title": "host_overrides"
        }, 
        {
            "location": "/plugins_reference/inventory/host_overrides/#synopsis", 
            "text": "Provide a list of host overriding values, to modify an existing inventory.  Typically used to patch an Ansible inventory (see ansible_inventory_file) with some new values", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/inventory/host_overrides/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  The name of the host to patch. Must be an existing host. Also may be 'all' or '*' to patch all hosts.    force_setup  no  A common Ansible problem is when referencing host info for a host which has not being accessed, so there is no fact grabbed for it. Setting this flag to  yes  will trigger an access to this host at the begining of the play.    ssh_host  no  Allow to override ssh_host    ssh_user  no  Allow to override ssh_user    ssh_private_key_file  no  Allow to override ssh_private_key_file    ssh_password  no  Allow to override ssh_password. This may be encrypted. Refer to  encrypted variables    ssh_port  no  Allow to override ssh_port    ssh_extra_args  no  Allow to override ssh_extra_args    ssh_common_args  no  Allow to override ssh_common_args    sftp_extra_args  no  Allow to override sftp_extra_args    scp_extra_args  no  Allow to override scp_extra_args    ssh_pipelining  no  Allow to override ssh_pipelining    ssh_executable  no  Allow to override ssh_executable    become  no  Allow to override become    become_method  no  Allow to override become_method    become_user  no  Allow to override become_user    become_pass  no  Allow to override become_pass. This may be encrypted. Refer to  encrypted variables    become_exe  no  Allow to override become_exe    priority  no  An integer number allowing to order overriding in case of multiple  host_override  on the same host(s). See example below. Default: 100    when  no  Boolean. Allow  conditional deployment  of this item. Default  True      To suppress an attribute, set it to empty string: ''", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/inventory/host_overrides/#examples", 
            "text": "We want to address a VM built using Vagrant. For this, we can use the Vagrant generated ansible inventory. Such inventory instruct HADeploy to access the VM under the  vagrant account. \nSo, we need to instruct HADeploy to become  root in order to perform privileged operation.   ansible_inventories: \n- file:  ../../iac/vgrvm1/.vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory \n\nhost_overrides:\n- name: all\n  become_user: root  Of course, this works because the  vagrant  account has been granted by vagrant with appropriate sudo rights.   This other example change the user for all HADeploy action for all hosts. And authenticate with a (encrypted) password instead of a key:  \nencrypted_vars:\n  deployer_password: |\n    $ANSIBLE_VAULT;1.1;AES256\n    65626166653134326137613232323336373139393036383532333863623630363662303531306539\n    6637306363343836376633353439656634613638643031660a636238323663353337313333663438\n    30306234306463626338663637623563393735653237323833323064316561653237393538303762\n    6363326232656461310a656631386135663764386565366566633537633665646562626236393462\n    6231\n\nhost_overrides:\n- name: all\n  ssh_user: deployer      \n  ssh_password:  {{deployer_password}} \n  ssh_private_key_file: ''  Refer to  encrypted variables  for more information.", 
            "title": "Examples"
        }, 
        {
            "location": "/plugins_reference/inventory/host_overrides/#priority", 
            "text": "Here is an illustration of the  priority  attribute usage:   host_overrides:\n- name: all\n  become_user: root\n\n....  \n\nhost_overrides:\n- name: all\n  become_user: ''\n  priority: 110  This will result with  become_user  not set.   While:  host_overrides:\n- name: all\n  become_user: root\n\n....  \n\nhost_overrides:\n- name: all\n  become_user: ''\n  priority: 90  Will result with  become_user  set to  root . (Default value is 100, so the first one take precedence).  This feature may be usefull when building complex configuration by merging several inventory files.", 
            "title": "Priority"
        }, 
        {
            "location": "/plugins_reference/inventory/host_overrides/#tricks", 
            "text": "If, when running HADeploy you encounter error like:  fatal: [dn1]: FAILED! =  { changed : false,  failed : true,  msg :  AnsibleUndefinedVariable: 'dict object' has no attribute 'ansible_fqdn' }  it is most likely that you need to set  force_setup  on some host_group or host.", 
            "title": "Tricks"
        }, 
        {
            "location": "/plugins_reference/kafka/kafka_relay/", 
            "text": "kafka_relay\n\n\nSynopsis\n\n\nIssuing some commands to specifics subsystem, such as Apache Kafka require a quite complex client configuration.\n\n\nTo avoid this, HADeploy will not issue such command directly, but push the command on one of the cluster node, called \u2019Relay node'.\n\n\nkafka_relay\n will define which host will be used to relay operations for Kafka, and also how these operations will be performed.\n\n\nThere should be only one entry of this type in the HADeploy definition file.\n\n\nAttributes\n\n\nkafka_relay\n is a map with the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nhost\n\n\nyes\n\n\nThe host on which all Kafka commands will be pushed for execution. THIS HOST MUST HAVE KAFKA INSTALLED ON. This can be validated by trying to locate and use commanes such as \nkafka-topic.sh\n.\n\n\n\n\n\n\nzk_host_group\n\n\nyes\n\n\nThe \nhost_group\n representing the zookeeper quorum. This group must contain the hosts acting as zookeeper servers.\nThis group should have the \nforce_setup\nflag set to `yes\n\n\n\n\n\n\nkafka_version\n\n\nyes\n\n\nSpecify the Kafka version. May be \n0.10.0\n, \n1.0.0\n, \n1.1.1\n or \n2.0.0\n. If your current version does not strictly match one of theses, you may try to select immediate previous version (i.e. use \n0.10.0\n for \n0.11.0\n).\n\n\n\n\n\n\nzk_port\n\n\nno\n\n\nThe zookeeper client port.\nDefault: \n2181\n\n\n\n\n\n\nzk_path\n\n\nno\n\n\nThe root path of Kafka in zookeeper.\nDefault:\n'/'\n\n\n\n\n\n\nbroker_id_map\n\n\nno\n\n\nWith Kafka, each broker is identified with an id. When creating a Topic, one can let Kafka distribute partition's replica across the cluster. But, we may also need to specify explicitly the distribution of replica, with strict location rules.\nIn such case, we need to specify brokers at topic creation, using \nbroker_id\n. As these \nbroker_id\n are infrastructure dependent, our application deployment description would be tightly coupled to the target infrastructure.\nTo prevent, this, we introduce here a level of indirection, by a map where each key is a virtual \nbroker_id\n (used in \nassignment\n in topic definition) and the value is the effective one.\nIf this map is not defined, then the virtual \nbroker_id\n are same as the effective ones.\n\n\n\n\n\n\ntools_folder\n\n\nno\n\n\nFolder used by HADeploy to install some tools for Kafka management.\nDefault: \n/tmp/hadeploy_\nuser\n/\n where \nuser\n is the \nssh_user\n defined for this relay host.\n\n\n\n\n\n\nbecome_user\n\n\nno\n\n\nA user account under which all kafka operations will be performed..\nNote: The \nssh_user\n defined for this relay host must have enough rights to switch to this \nbecome_user\n using the \nbecome_method\n below.\nDefault: No user switch, so the \nssh_user\n defined for this relay host will be used.\n\n\n\n\n\n\nbecome_method\n\n\nno\n\n\nThe method used to swith to this user. Refer to the Ansible documentation on this parameter.\nDefault: Ansible default (\nsudo\n).\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nKerberos authentication\n\n\nHADeploy Kafka topics management need access to Zookeeper. When Kerberos is activated on the target cluster, such access may be protected and forbidden for your deployment user. \n\n\nIn such case, solution is to act as \nkafka\n user, by using the \nbecome_user\n attribute.\n\n\nBut keep in mind The \nssh_user\n defined for this relay host must have enough rights to switch to \nkafka\n user account using the \nbecome_method\n.\n\n\nExample\n\n\nThe simplest case:\n\n\nkafka_relay:\n  host: br1\n  zk_host_group: zookeepers\n  kafka_version: \n1.0.0\n\n\n\n\n\nThe simplest case when Kerberos is activated:\n\n\nkafka_relay:\n  host: br1\n  zk_host_group: zookeepers\n  kafka_version: \n1.0.0\n\n  become_user: kafka\n\n\n\n\nA more complex, with default value set and a \nbroker_id\n mapping (Typical of an Hortonworks Kafka deployment).\n\n\nkafka_relay:\n  host: br1\n  zk_host_group: zookeepers\n  kafka_version: \n0.10.0\n\n  zk_port: 2181\n  broker_id_map:\n    1: 1001\n    2: 1002\n    3: 1003\n\n\n\n\nTricks\n\n\nkdescribe\n\n\nTo find the \nbroker_ids\n values, one may use the \nkdescribe\n tool:\n\n\nAnsibleUndefinedVariable\n\n\nIf, when running HADeploy you encounter error like:\n\n\nfatal: [dn1]: FAILED! =\n {\nchanged\n: false, \nfailed\n: true, \nmsg\n: \nAnsibleUndefinedVariable: 'dict object' has no attribute 'ansible_fqdn'\n}\n\n\n\n\nit is most likely that you have not set \nforce_setup\n on the \nzk_host_group\n group", 
            "title": "kafka_relay"
        }, 
        {
            "location": "/plugins_reference/kafka/kafka_relay/#kafka_relay", 
            "text": "", 
            "title": "kafka_relay"
        }, 
        {
            "location": "/plugins_reference/kafka/kafka_relay/#synopsis", 
            "text": "Issuing some commands to specifics subsystem, such as Apache Kafka require a quite complex client configuration.  To avoid this, HADeploy will not issue such command directly, but push the command on one of the cluster node, called \u2019Relay node'.  kafka_relay  will define which host will be used to relay operations for Kafka, and also how these operations will be performed.  There should be only one entry of this type in the HADeploy definition file.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/kafka/kafka_relay/#attributes", 
            "text": "kafka_relay  is a map with the following attributes:     Name  req?  Description      host  yes  The host on which all Kafka commands will be pushed for execution. THIS HOST MUST HAVE KAFKA INSTALLED ON. This can be validated by trying to locate and use commanes such as  kafka-topic.sh .    zk_host_group  yes  The  host_group  representing the zookeeper quorum. This group must contain the hosts acting as zookeeper servers. This group should have the  force_setup flag set to `yes    kafka_version  yes  Specify the Kafka version. May be  0.10.0 ,  1.0.0 ,  1.1.1  or  2.0.0 . If your current version does not strictly match one of theses, you may try to select immediate previous version (i.e. use  0.10.0  for  0.11.0 ).    zk_port  no  The zookeeper client port. Default:  2181    zk_path  no  The root path of Kafka in zookeeper. Default: '/'    broker_id_map  no  With Kafka, each broker is identified with an id. When creating a Topic, one can let Kafka distribute partition's replica across the cluster. But, we may also need to specify explicitly the distribution of replica, with strict location rules. In such case, we need to specify brokers at topic creation, using  broker_id . As these  broker_id  are infrastructure dependent, our application deployment description would be tightly coupled to the target infrastructure. To prevent, this, we introduce here a level of indirection, by a map where each key is a virtual  broker_id  (used in  assignment  in topic definition) and the value is the effective one. If this map is not defined, then the virtual  broker_id  are same as the effective ones.    tools_folder  no  Folder used by HADeploy to install some tools for Kafka management. Default:  /tmp/hadeploy_ user /  where  user  is the  ssh_user  defined for this relay host.    become_user  no  A user account under which all kafka operations will be performed.. Note: The  ssh_user  defined for this relay host must have enough rights to switch to this  become_user  using the  become_method  below. Default: No user switch, so the  ssh_user  defined for this relay host will be used.    become_method  no  The method used to swith to this user. Refer to the Ansible documentation on this parameter. Default: Ansible default ( sudo ).    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/kafka/kafka_relay/#kerberos-authentication", 
            "text": "HADeploy Kafka topics management need access to Zookeeper. When Kerberos is activated on the target cluster, such access may be protected and forbidden for your deployment user.   In such case, solution is to act as  kafka  user, by using the  become_user  attribute.  But keep in mind The  ssh_user  defined for this relay host must have enough rights to switch to  kafka  user account using the  become_method .", 
            "title": "Kerberos authentication"
        }, 
        {
            "location": "/plugins_reference/kafka/kafka_relay/#example", 
            "text": "The simplest case:  kafka_relay:\n  host: br1\n  zk_host_group: zookeepers\n  kafka_version:  1.0.0   The simplest case when Kerberos is activated:  kafka_relay:\n  host: br1\n  zk_host_group: zookeepers\n  kafka_version:  1.0.0 \n  become_user: kafka  A more complex, with default value set and a  broker_id  mapping (Typical of an Hortonworks Kafka deployment).  kafka_relay:\n  host: br1\n  zk_host_group: zookeepers\n  kafka_version:  0.10.0 \n  zk_port: 2181\n  broker_id_map:\n    1: 1001\n    2: 1002\n    3: 1003", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/kafka/kafka_relay/#tricks", 
            "text": "", 
            "title": "Tricks"
        }, 
        {
            "location": "/plugins_reference/kafka/kafka_relay/#kdescribe", 
            "text": "To find the  broker_ids  values, one may use the  kdescribe  tool:", 
            "title": "kdescribe"
        }, 
        {
            "location": "/plugins_reference/kafka/kafka_relay/#ansibleundefinedvariable", 
            "text": "If, when running HADeploy you encounter error like:  fatal: [dn1]: FAILED! =  { changed : false,  failed : true,  msg :  AnsibleUndefinedVariable: 'dict object' has no attribute 'ansible_fqdn' }  it is most likely that you have not set  force_setup  on the  zk_host_group  group", 
            "title": "AnsibleUndefinedVariable"
        }, 
        {
            "location": "/plugins_reference/kafka/kafka_topics/", 
            "text": "kafka_topics\n\n\nSynopsis\n\n\nProvide a list of Kafka topics, which will be managed by HADeploy\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe name of the topic\n\n\n\n\n\n\nproperties\n\n\nno\n\n\nA map of properties associated to the topic. Refer to the kafka documentation for a complete list of available properties.\n\n\n\n\n\n\npartition_factor\n\n\nyes if assignments is not defined\n\n\nSpecify the number of partition of the topic.\n\n\n\n\n\n\nreplication_factor\n\n\nyes if assignments is not defined\n\n\nSpecify the number of replica for each partition.\n\n\n\n\n\n\nassignments\n\n\nyes if rep/part factors are not specified\n\n\nA Map where the key is the partition# and the value a list of \nbroker_id\n.\nThis allow to manual definition of the distribution of partition's replica, with strict location rules.\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this group to be removed when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\nranger_policy\n\n\nno\n\n\nDefinition of Apache Ranger policy bound to this topic. Parameters are same as \nkafka_ranger_policy\n except than \ntopics\n should not be defined.\nThe policy name can be explicitly defined. Otherwise, a name will be generated as \"\n_\ntopic\n_\n\".\nSee example below for more information\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nExample\n\n\nSimple case. We let Kafka decide on which brokers our replica will be set:\n\n\nkafka_topics:\n- name: broadapp_t1\n  partition_factor: 3\n  replication_factor: 2\n  properties:\n    retention.ms: 630720000000\n    retention.bytes: 858993459200\n\n\n\n\nThe same, topic, but we specify explicitly our placement of replica:\n\n\nkafka_topics:\n- name: broadapp_t1\n  assignments:\n    0: [ 1, 2 ]\n    1: [ 2, 3 ]\n    2: [ 3, 1 ]\n  properties:\n    retention.ms: 630720000000\n    retention.bytes: 858993459200\n\n\n\n\nIf \nkafka_relay\n host a \nbroker_id_map\n as the following:\n\n\nkafka_relay:\n  ...\n  broker_id_map:\n    1: 1001\n    2: 1002\n    3: 1003\n\n\n\n\nThen the first partition (#0) will have two replicas, placed on brokers of id 1001 and 1002.\n\n\nIf \nhdfs_relay\n does not contains a \nbroker_id_map\n, then the first partition (#0) will have two replicas, placed on brokers of id 1 and 2. \n\n\n\n\nNB: Recent version of Kafka introduced a 'Rack awareness' capability which ensure a good distribution of replica amongst several racks. This explicit partition assignment may now be used only on very specifics cases.\n\n\nNB: Partition re-assignment on topic modification is not supported. One may use the kafka provided partition reassignment tool (kafka-reassign-partitions.sh) for this.\n\n\n\n\nAnother example, with a Apache Ranger policy granting Publish and Consume rights to the users of group users:\n\n\nkafka_topics:\n- name: broadapp_t1\n  partition_factor: 3\n  replication_factor: 2\n  ranger_policy:\n    audit: no\n    permissions:\n    - groups:\n      - users\n      accesses:\n    - Consume\n    - Publish\n\n\n\n\nTrick\n\n\nTo find the \nbroker_ids\n values, one may use the \nkdescribe\n tool:\n\n\nhttps://github.com/Kappaware/kdescribe", 
            "title": "kafka_topic"
        }, 
        {
            "location": "/plugins_reference/kafka/kafka_topics/#kafka_topics", 
            "text": "", 
            "title": "kafka_topics"
        }, 
        {
            "location": "/plugins_reference/kafka/kafka_topics/#synopsis", 
            "text": "Provide a list of Kafka topics, which will be managed by HADeploy", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/kafka/kafka_topics/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  The name of the topic    properties  no  A map of properties associated to the topic. Refer to the kafka documentation for a complete list of available properties.    partition_factor  yes if assignments is not defined  Specify the number of partition of the topic.    replication_factor  yes if assignments is not defined  Specify the number of replica for each partition.    assignments  yes if rep/part factors are not specified  A Map where the key is the partition# and the value a list of  broker_id . This allow to manual definition of the distribution of partition's replica, with strict location rules.    no_remove  no  Boolean: Prevent this group to be removed when HADeploy will be used in REMOVE mode. Default:  no    ranger_policy  no  Definition of Apache Ranger policy bound to this topic. Parameters are same as  kafka_ranger_policy  except than  topics  should not be defined. The policy name can be explicitly defined. Otherwise, a name will be generated as \" _ topic _ \". See example below for more information    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/kafka/kafka_topics/#example", 
            "text": "Simple case. We let Kafka decide on which brokers our replica will be set:  kafka_topics:\n- name: broadapp_t1\n  partition_factor: 3\n  replication_factor: 2\n  properties:\n    retention.ms: 630720000000\n    retention.bytes: 858993459200  The same, topic, but we specify explicitly our placement of replica:  kafka_topics:\n- name: broadapp_t1\n  assignments:\n    0: [ 1, 2 ]\n    1: [ 2, 3 ]\n    2: [ 3, 1 ]\n  properties:\n    retention.ms: 630720000000\n    retention.bytes: 858993459200  If  kafka_relay  host a  broker_id_map  as the following:  kafka_relay:\n  ...\n  broker_id_map:\n    1: 1001\n    2: 1002\n    3: 1003  Then the first partition (#0) will have two replicas, placed on brokers of id 1001 and 1002.  If  hdfs_relay  does not contains a  broker_id_map , then the first partition (#0) will have two replicas, placed on brokers of id 1 and 2.    NB: Recent version of Kafka introduced a 'Rack awareness' capability which ensure a good distribution of replica amongst several racks. This explicit partition assignment may now be used only on very specifics cases.  NB: Partition re-assignment on topic modification is not supported. One may use the kafka provided partition reassignment tool (kafka-reassign-partitions.sh) for this.   Another example, with a Apache Ranger policy granting Publish and Consume rights to the users of group users:  kafka_topics:\n- name: broadapp_t1\n  partition_factor: 3\n  replication_factor: 2\n  ranger_policy:\n    audit: no\n    permissions:\n    - groups:\n      - users\n      accesses:\n    - Consume\n    - Publish", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/kafka/kafka_topics/#trick", 
            "text": "To find the  broker_ids  values, one may use the  kdescribe  tool:  https://github.com/Kappaware/kdescribe", 
            "title": "Trick"
        }, 
        {
            "location": "/plugins_reference/ranger/hbase_ranger_policies/", 
            "text": "hbase_ranger_policies\n\n\nSynopsis\n\n\nAllow definition of a list of Apache Ranger policies for setting HBase permissions\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe policy name. Will be decorated to mark it as managed by HADeploy, as described in \nranger_relay\n.\n\n\n\n\n\n\ntables\n\n\nyes\n\n\nA list of HBase tables on which this policy will apply. Table should be expressed as \nnamespace\n:\ntablename\n. Accept wildcard characters '*' and '?'.\n\n\n\n\n\n\ncolumn_families\n\n\nno\n\n\nA list of column families on which the policy will apply.\nDefault: \n\"*\"\n\n\n\n\n\n\ncolumns\n\n\nno\n\n\nA list of columns on which the policy will apply.\nDefault: \n\"*\"\n\n\n\n\n\n\naudit\n\n\nno\n\n\nDid this policy is audited by Ranger.\nDefault: \nyes\n\n\n\n\n\n\nenabled\n\n\nno\n\n\nAllow this policy to be disabled.\nDefault: \nyes\n\n\n\n\n\n\nno_remove\n\n\nFalse\n\n\nBoolean: Prevent this policy to be removed when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\npermissions\n\n\nyes\n\n\nA list of permissions defining rights granted by this policy. See below\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\npermissions\n\n\nEach item of the permission list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nusers\n\n\nyes if \ngroups\n is undefined\n\n\nA list of users this policy will apply on. May be empty if some groups are defined.\n\n\n\n\n\n\ngroups\n\n\nyes if \nusers\n is undefined\n\n\nA list of groups this policy will apply on. May be empty if some users are defined.\n\n\n\n\n\n\naccesses\n\n\nyes\n\n\nThe list of rights granted by this policy. May include \nread\n, \nwrite\n, \ncreate\n and \nadmin\n.\n\n\n\n\n\n\ndelegate_admin\n\n\nno\n\n\nWhen a policy is assigned to a user or a group of users those users become the delegated admin. The delegated admin can update and delete the policies.\nDefault: \nno\n\n\n\n\n\n\n\n\nExamples\n\n\nhbase_ranger_policies:\n- name: \nFullAccessToNamespace1\n\n  tables: [ \nnamespace1:*\n ]\n  permissions:\n  - users:\n    - ns1admin\n    accesses:\n    - read\n    - write\n    - create\n    - admin\n  - groups:\n    - someUsers\n    users:\n    - anotherUser\n    accesses:\n    - read\n- name: \nReadAccesstoTableData1InNamespace2\n\n  tables: [ \nnamespace2:data1\n ]\n  permissions:\n  - groups:\n    - users\n    accesses:\n    - read", 
            "title": "hbase_ranger_policies"
        }, 
        {
            "location": "/plugins_reference/ranger/hbase_ranger_policies/#hbase_ranger_policies", 
            "text": "", 
            "title": "hbase_ranger_policies"
        }, 
        {
            "location": "/plugins_reference/ranger/hbase_ranger_policies/#synopsis", 
            "text": "Allow definition of a list of Apache Ranger policies for setting HBase permissions", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/ranger/hbase_ranger_policies/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  The policy name. Will be decorated to mark it as managed by HADeploy, as described in  ranger_relay .    tables  yes  A list of HBase tables on which this policy will apply. Table should be expressed as  namespace : tablename . Accept wildcard characters '*' and '?'.    column_families  no  A list of column families on which the policy will apply. Default:  \"*\"    columns  no  A list of columns on which the policy will apply. Default:  \"*\"    audit  no  Did this policy is audited by Ranger. Default:  yes    enabled  no  Allow this policy to be disabled. Default:  yes    no_remove  False  Boolean: Prevent this policy to be removed when HADeploy will be used in REMOVE mode. Default:  no    permissions  yes  A list of permissions defining rights granted by this policy. See below    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/ranger/hbase_ranger_policies/#permissions", 
            "text": "Each item of the permission list has the following attributes:     Name  req?  Description      users  yes if  groups  is undefined  A list of users this policy will apply on. May be empty if some groups are defined.    groups  yes if  users  is undefined  A list of groups this policy will apply on. May be empty if some users are defined.    accesses  yes  The list of rights granted by this policy. May include  read ,  write ,  create  and  admin .    delegate_admin  no  When a policy is assigned to a user or a group of users those users become the delegated admin. The delegated admin can update and delete the policies. Default:  no", 
            "title": "permissions"
        }, 
        {
            "location": "/plugins_reference/ranger/hbase_ranger_policies/#examples", 
            "text": "hbase_ranger_policies:\n- name:  FullAccessToNamespace1 \n  tables: [  namespace1:*  ]\n  permissions:\n  - users:\n    - ns1admin\n    accesses:\n    - read\n    - write\n    - create\n    - admin\n  - groups:\n    - someUsers\n    users:\n    - anotherUser\n    accesses:\n    - read\n- name:  ReadAccesstoTableData1InNamespace2 \n  tables: [  namespace2:data1  ]\n  permissions:\n  - groups:\n    - users\n    accesses:\n    - read", 
            "title": "Examples"
        }, 
        {
            "location": "/plugins_reference/ranger/hdfs_ranger_policies/", 
            "text": "hdfs_ranger_policies\n\n\nSynopsis\n\n\nAllow definition of a list of Apache Ranger policies for setting HDFS permissions\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe policy name. Will be decorated to mark it as managed by HADeploy, as described in \nranger_relay\n.\n\n\n\n\n\n\npaths\n\n\nyes\n\n\nA list of HDFS path on which this policy will apply. Accept wildcard characters '*' and '?'\n\n\n\n\n\n\nrecursive\n\n\nno\n\n\nDid this policy apply recursively.\nDefault: \nyes\n\n\n\n\n\n\naudit\n\n\nno\n\n\nDid this policy is audited by Ranger.\nDefault: \nyes\n\n\n\n\n\n\nenabled\n\n\nno\n\n\nAllow this policy to be disabled.\nDefault: \nyes\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this policy to be removed when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\npermissions\n\n\nyes\n\n\nA list of permissions defining rights granted by this policy. See below\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\npermissions\n\n\nEach item of the permission list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nusers\n\n\nyes if \ngroups\n is undefined\n\n\nA list of users this policy will apply on. May be empty if some groups are defined.\n\n\n\n\n\n\ngroups\n\n\nyes if \nusers\n is undefined\n\n\nA list of groups this policy will apply on. May be empty if some users are defined.\n\n\n\n\n\n\naccesses\n\n\nyes\n\n\nThe list of rights granted by this policy. May include \nread\n, \nwrite\n and \nexecute\n.\n\n\n\n\n\n\ndelegate_admin\n\n\nno\n\n\nWhen a policy is assigned to a user or a group of users those users become the delegated admin. The delegated admin can update and delete the policies.\nDefault: \nno\n\n\n\n\n\n\n\n\nExamples\n\n\nhdfs_ranger_policies:\n- name: \n/apps/pixo\n\n  paths: [ \n/apps/pixo\n ]\n  permissions:\n  - { users: [ user1 ], groups: [ grp1, grp2 ], accesses: [ read, write, execute ] }", 
            "title": "hdfs_ranger_policies"
        }, 
        {
            "location": "/plugins_reference/ranger/hdfs_ranger_policies/#hdfs_ranger_policies", 
            "text": "", 
            "title": "hdfs_ranger_policies"
        }, 
        {
            "location": "/plugins_reference/ranger/hdfs_ranger_policies/#synopsis", 
            "text": "Allow definition of a list of Apache Ranger policies for setting HDFS permissions", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/ranger/hdfs_ranger_policies/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  The policy name. Will be decorated to mark it as managed by HADeploy, as described in  ranger_relay .    paths  yes  A list of HDFS path on which this policy will apply. Accept wildcard characters '*' and '?'    recursive  no  Did this policy apply recursively. Default:  yes    audit  no  Did this policy is audited by Ranger. Default:  yes    enabled  no  Allow this policy to be disabled. Default:  yes    no_remove  no  Boolean: Prevent this policy to be removed when HADeploy will be used in REMOVE mode. Default:  no    permissions  yes  A list of permissions defining rights granted by this policy. See below    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/ranger/hdfs_ranger_policies/#permissions", 
            "text": "Each item of the permission list has the following attributes:     Name  req?  Description      users  yes if  groups  is undefined  A list of users this policy will apply on. May be empty if some groups are defined.    groups  yes if  users  is undefined  A list of groups this policy will apply on. May be empty if some users are defined.    accesses  yes  The list of rights granted by this policy. May include  read ,  write  and  execute .    delegate_admin  no  When a policy is assigned to a user or a group of users those users become the delegated admin. The delegated admin can update and delete the policies. Default:  no", 
            "title": "permissions"
        }, 
        {
            "location": "/plugins_reference/ranger/hdfs_ranger_policies/#examples", 
            "text": "hdfs_ranger_policies:\n- name:  /apps/pixo \n  paths: [  /apps/pixo  ]\n  permissions:\n  - { users: [ user1 ], groups: [ grp1, grp2 ], accesses: [ read, write, execute ] }", 
            "title": "Examples"
        }, 
        {
            "location": "/plugins_reference/ranger/hive_ranger_policies/", 
            "text": "hive_ranger_policies\n\n\nSynopsis\n\n\nAllow definition of a list of Apache Ranger policies for setting Hive permissions\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe policy name. Will be decorated to mark it as managed by HADeploy, as described in \nranger_relay\n.\n\n\n\n\n\n\ndatabases\n\n\nyes\n\n\nA list of Hive databases on which this policy will apply. Accept wildcard characters '*' and '?'.\n\n\n\n\n\n\ntables\n\n\nno\n\n\nA list of HBase tables on which this policy will apply. Accept wildcard characters '*' and '?'.\nIf defined, \nudfs\nattribute must be not..\nDefault: \n\"*\"\n\n\n\n\n\n\ncolumns\n\n\nno\n\n\nA list of columns on which the policy will apply.\nDefault: \n\"*\"\n\n\n\n\n\n\nudfs\n\n\nno\n\n\nA list of UDF (User Defined Function) this policy will apply on.. Accept wildcard characters '*' and '?'.\nIf defined, \ntable\nand \ncolumns\n attribute must be not.\n\n\n\n\n\n\naudit\n\n\nno\n\n\nDid this policy is audited by Ranger.\nDefault: \nyes\n\n\n\n\n\n\nenabled\n\n\nno\n\n\nAllow this policy to be disabled.\nDefault: \nyes\n\n\n\n\n\n\nno_remove\n\n\nFalse\n\n\nBoolean: Prevent this policy to be removed when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\npermissions\n\n\nyes\n\n\nA list of permissions defining rights granted by this policy. See below\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\npermissions\n\n\nEach item of the permission list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nusers\n\n\nyes if \ngroups\n is undefined\n\n\nA list of users this policy will apply on. May be empty if some groups are defined.\n\n\n\n\n\n\ngroups\n\n\nyes if \nusers\n is undefined\n\n\nA list of groups this policy will apply on. May be empty if some users are defined.\n\n\n\n\n\n\naccesses\n\n\nyes\n\n\nThe list of rights granted by this policy. May include \nselect\n, \nupdate\n, \ncreate\n, \ndrop\n, \nalter\n, \nindex\n, \nlock\n and \nall\n.\n\n\n\n\n\n\ndelegate_admin\n\n\nno\n\n\nWhen a policy is assigned to a user or a group of users those users become the delegated admin. The delegated admin can update and delete the policies.\nDefault: \nno\n\n\n\n\n\n\n\n\nExamples\n\n\nhive_ranger_policies:\n- name: mktDbRule\n  databases: \n  - dbmarketing\n  tables: \n  - \n*\n\n  permissions:\n    - groups:\n      - users\n      accesses: \n      - select\n      - update\n    - users:\n      - admin\n      accesses:\n      - all\n      delegate_admin: yes", 
            "title": "hive_ranger_policies"
        }, 
        {
            "location": "/plugins_reference/ranger/hive_ranger_policies/#hive_ranger_policies", 
            "text": "", 
            "title": "hive_ranger_policies"
        }, 
        {
            "location": "/plugins_reference/ranger/hive_ranger_policies/#synopsis", 
            "text": "Allow definition of a list of Apache Ranger policies for setting Hive permissions", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/ranger/hive_ranger_policies/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  The policy name. Will be decorated to mark it as managed by HADeploy, as described in  ranger_relay .    databases  yes  A list of Hive databases on which this policy will apply. Accept wildcard characters '*' and '?'.    tables  no  A list of HBase tables on which this policy will apply. Accept wildcard characters '*' and '?'. If defined,  udfs attribute must be not.. Default:  \"*\"    columns  no  A list of columns on which the policy will apply. Default:  \"*\"    udfs  no  A list of UDF (User Defined Function) this policy will apply on.. Accept wildcard characters '*' and '?'. If defined,  table and  columns  attribute must be not.    audit  no  Did this policy is audited by Ranger. Default:  yes    enabled  no  Allow this policy to be disabled. Default:  yes    no_remove  False  Boolean: Prevent this policy to be removed when HADeploy will be used in REMOVE mode. Default:  no    permissions  yes  A list of permissions defining rights granted by this policy. See below    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/ranger/hive_ranger_policies/#permissions", 
            "text": "Each item of the permission list has the following attributes:     Name  req?  Description      users  yes if  groups  is undefined  A list of users this policy will apply on. May be empty if some groups are defined.    groups  yes if  users  is undefined  A list of groups this policy will apply on. May be empty if some users are defined.    accesses  yes  The list of rights granted by this policy. May include  select ,  update ,  create ,  drop ,  alter ,  index ,  lock  and  all .    delegate_admin  no  When a policy is assigned to a user or a group of users those users become the delegated admin. The delegated admin can update and delete the policies. Default:  no", 
            "title": "permissions"
        }, 
        {
            "location": "/plugins_reference/ranger/hive_ranger_policies/#examples", 
            "text": "hive_ranger_policies:\n- name: mktDbRule\n  databases: \n  - dbmarketing\n  tables: \n  -  * \n  permissions:\n    - groups:\n      - users\n      accesses: \n      - select\n      - update\n    - users:\n      - admin\n      accesses:\n      - all\n      delegate_admin: yes", 
            "title": "Examples"
        }, 
        {
            "location": "/plugins_reference/ranger/kafka_ranger_policies/", 
            "text": "kafka_ranger_policies\n\n\nSynopsis\n\n\nAllow definition of a list of Apache Ranger policies for setting Kafka permissions\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe policy name. Will be decorated to mark it as managed by HADeploy, as described in \nranger_relay\n.\n\n\n\n\n\n\ntopics\n\n\nyes\n\n\nA list of topics on which this policy will apply. Accept wildcard characters '*' and '?'.\n\n\n\n\n\n\naudit\n\n\nno\n\n\nDid this policy is audited by Ranger.\nDefault: \nyes\n\n\n\n\n\n\nenabled\n\n\nno\n\n\nAllow this policy to be disabled.\nDefault: \nyes\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this policy to be removed when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\npermissions\n\n\nyes\n\n\nA list of permissions defining rights granted by this policy. See below\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nPermissions\n\n\nEach item of the permission list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nusers\n\n\nyes if \ngroups\n is undefined\n\n\nA list of users this policy will apply on. May be empty if some groups are defined.\n\n\n\n\n\n\ngroups\n\n\nyes if \nusers\n is undefined\n\n\nA list of groups this policy will apply on. May be empty if some users are defined.\n\n\n\n\n\n\nip_addresses\n\n\nno\n\n\nA list of source IP addresses this policy will apply on.\n\n\n\n\n\n\naccesses\n\n\nyes\n\n\nThe list of rights granted by this policy. May include \npublish\n, \nconsume\n, \nconfigure\n, \ndescribe\n, \ncreate\n, \ndelete\n and \nkafka_admin\n.\n\n\n\n\n\n\ndelegate_admin\n\n\nno\n\n\nWhen a policy is assigned to a user or a group of users those users become the delegated admin. The delegated admin can update, delete the policies.\nDefault: \nno\n\n\n\n\n\n\n\n\nExamples\n\n\nkafka_ranger_policies:\n- name: \napp1Kafka\n\n  topics: \n  - \napp1_*\n\n  permissions:\n  - users:\n    - app1Admin\n    accesses:\n    - publish\n    - consume\n    - configure\n    - describe\n    - create\n    - delete\n    - kafka_admin\n  - groups:\n    - app1Consumers\n    accesses:\n    - consume", 
            "title": "kafka_ranger_policies"
        }, 
        {
            "location": "/plugins_reference/ranger/kafka_ranger_policies/#kafka_ranger_policies", 
            "text": "", 
            "title": "kafka_ranger_policies"
        }, 
        {
            "location": "/plugins_reference/ranger/kafka_ranger_policies/#synopsis", 
            "text": "Allow definition of a list of Apache Ranger policies for setting Kafka permissions", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/ranger/kafka_ranger_policies/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  The policy name. Will be decorated to mark it as managed by HADeploy, as described in  ranger_relay .    topics  yes  A list of topics on which this policy will apply. Accept wildcard characters '*' and '?'.    audit  no  Did this policy is audited by Ranger. Default:  yes    enabled  no  Allow this policy to be disabled. Default:  yes    no_remove  no  Boolean: Prevent this policy to be removed when HADeploy will be used in REMOVE mode. Default:  no    permissions  yes  A list of permissions defining rights granted by this policy. See below    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/ranger/kafka_ranger_policies/#permissions", 
            "text": "Each item of the permission list has the following attributes:     Name  req?  Description      users  yes if  groups  is undefined  A list of users this policy will apply on. May be empty if some groups are defined.    groups  yes if  users  is undefined  A list of groups this policy will apply on. May be empty if some users are defined.    ip_addresses  no  A list of source IP addresses this policy will apply on.    accesses  yes  The list of rights granted by this policy. May include  publish ,  consume ,  configure ,  describe ,  create ,  delete  and  kafka_admin .    delegate_admin  no  When a policy is assigned to a user or a group of users those users become the delegated admin. The delegated admin can update, delete the policies. Default:  no", 
            "title": "Permissions"
        }, 
        {
            "location": "/plugins_reference/ranger/kafka_ranger_policies/#examples", 
            "text": "kafka_ranger_policies:\n- name:  app1Kafka \n  topics: \n  -  app1_* \n  permissions:\n  - users:\n    - app1Admin\n    accesses:\n    - publish\n    - consume\n    - configure\n    - describe\n    - create\n    - delete\n    - kafka_admin\n  - groups:\n    - app1Consumers\n    accesses:\n    - consume", 
            "title": "Examples"
        }, 
        {
            "location": "/plugins_reference/ranger/ranger_relay/", 
            "text": "ranger_relay\n\n\nSynopsis\n\n\nAll Apache Ranger commands will be performed using Ranger HTTP/REST API. This definition will provide informations for HADeploy to use this interface.\n\n\nThere should be only one entry of this type in the HADeploy definition file.\n\n\nAttributes\n\n\nranger_relay is a map with the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nhost\n\n\nyes\n\n\nFrom which host these commands will be launched. Typically, an edge node in the cluster. But may be a host outside the cluster.\n\n\n\n\n\n\nranger_url\n\n\nyes\n\n\nThe Ranger base URL to access Ranger API. Same host:port as the Ranger Admin GUI. Typically: \nhttp://myranger.server.com:6080\nor\nhttps://myranger.server.com:6182\n\n\n\n\n\n\nranger_username\n\n\nyes\n\n\nThe user name to log on the Ranger Admin. Must have enough rights to manage policies\n\n\n\n\n\n\nranger_password\n\n\nyes\n\n\nThe password associated with the admin_username. May be encrypted. Refer to \nencrypted variables\n\n\n\n\n\n\nvalidate_certs\n\n\nno\n\n\nUseful if Ranger Admin connection is using SSL. If no, SSL certificates will not be validated. This should only be used on personally controlled sites using self-signed certificates.\nDefault: \nyes\n\n\n\n\n\n\nca_bundle_relay_file\n\n\nno\n\n\nUseful if Ranger Admin connection is using SSL. Allow to specify a CA_BUNDLE file, a file that contains root and intermediate certificates to validate the Ranger Admin certificate in .pem format.\nThis file will be looked up on the relay host system, on which this module will be executed.\n\n\n\n\n\n\nca_bundle_local_file\n\n\nno\n\n\nSame as above, except this file will be looked up locally, relative to the main file. It will be copied on the relay host at the location defined by \nca_bundle_relay_file\n\n\n\n\n\n\nhdfs_service_name\n\n\nno\n\n\nIn most cases, you should not need to set this parameter. It defines the Ranger Admin HDFS service, typically: \nyourClusterName\n_hadoop\n.\nIt must be set if there are several such services defined in your Ranger Admin configuration, to select the one you intend to use.\n\n\n\n\n\n\nhbase_service_name\n\n\nno\n\n\nIn most cases, you should not need to set this parameter. It defines the Ranger Admin HBase service, typically: \nyourClusterName\n_hbase\n.\nIt must be set if there are several such services defined in your Ranger Admin configuration, to select the one you intend to use.\n\n\n\n\n\n\nkafka_service_name\n\n\nno\n\n\nIn most cases, you should not need to set this parameter. It defines the Ranger Admin Kafka service, typically: \nyourClusterName\n_kafka\n.\nIt must be set if there are several such services defined in your Ranger Admin configuration, to select the one you intend to use.\n\n\n\n\n\n\nhive_service_name\n\n\nno\n\n\nIn most cases, you should not need to set this parameter. It defines the Ranger Admin Hive service, typically: \nyourClusterName\n_hive\n.\nIt must be set if there are several such services defined in your Ranger Admin configuration, to select the one you intend to use.\n\n\n\n\n\n\nyarn_service_name\n\n\nno\n\n\nIn most cases, you should not need to set this parameter. It defines the Ranger Admin Yarn service, typically: \nyourClusterName\n_yarn\n.\nIt must be set if there are several such services defined in your Ranger Admin configuration, to select the one you intend to use.\n\n\n\n\n\n\nstorm_service_name\n\n\nno\n\n\nIn most cases, you should not need to set this parameter. It defines the Ranger Admin Storm service, typically: \nyourClusterName\n_storm\n.\nIt must be set if there are several such services defined in your Ranger Admin configuration, to select the one you intend to use.\n\n\n\n\n\n\npolicy_name_decorator\n\n\nno\n\n\nTo distinguish Ranger policy managed by HADeploy, a naming convention is applied by default. The policy name, as it will appears in the GUI Ranger interface will be in the form \nHAD[\npolicyName\n]\n, where \npolicyName\n is the name of the policy as you provide it.\nThis is achieved by wrapping the name with this pattern, where \n{0}\n is substituted with the policy name. \nFor python aware reader, this is performed as:\n\"HAD[{0}]\".format(policyName)\n.\nIf you just want to have raw policy name, simply define the parameter with \n{0}\n.\nDefault: \nHAD[{0}]\n\n\n\n\n\n\nno_log\n\n\nno\n\n\nBoolean. Prevent some messages to be displayed, as they can contains sensitive information. This can make debugging somewhat more difficult, so temporary setting this swith to \nFalse\n may be useful to get more information (Including sensitive ones!).\nDefault \nTrue\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nExample\n\n\nA simple configuration:\n\n\nranger_relay:\n  host: en1\n  ranger_url:  http://ranger.mycluster.mycompany.com:6080\n  ranger_username: admin\n  ranger_password: admin\n\n\n\n\nA more secure configuration, with https, certificate validation and encrypted password.\n\n\nencrypted_vars:\n  ranger_password: |\n    $ANSIBLE_VAULT;1.1;AES256\n    34396662613462623565323936616330623661623065343033646136643635653430636238613962\n    3537343131346462343138343064313937646366363435340a633532366162623838376436366362\n    61393033343932303636653066336130616132383463373934396265306364363562613565613165\n    6163613739303430650a356136353865623534643237646166393230613933396166663963633538\n    3664\n\nranger_relay:\n  host: en1\n  ranger_url:  https://ranger.mycluster.mycompany.com:6182\n  ranger_username: admin\n  ranger_password: \n{{ranger_password}}\n  \n  ca_bundle_local_file: cert/ranger_mycluster_cert.pem\n  ca_bundle_relay_file: /etc/security/certs/ranger_mycluster_cert.pem\n\n\n\n\nWhen using such encryption feature, you will need to provide a password when launching HADeploy. Otherwise you will have an error like the following: \n\n\nThe offending line appears to be:\n\n  vars:\n      rangerPassword: !vault |\n                      ^ here\n\n\n\n\nMore detail on how to encrypt a value and providing a password on execution at \nencrypted variables\n\n\nCA_BUNDLE\n\n\nInternally, HADeploy use the python \nrequests\n API to access Ranger. The provided \nca_bundle_relay_file\n will be used as the \nverify\n parameter of all HTTP requests. More info  \nhere\n.\n\n\nIn some cases, a CA_BUNDLE may be simply the certificate of the Ranger server, in PEM format.\n\n\nTo grab this certificate, you may use a tiny python program like the following:\n\n\nimport ssl\n\nif __name__ == '__main__':\n    cert = ssl.get_server_certificate((\nranger.mycluster.corp.com\n, 6182), ssl_version=ssl.PROTOCOL_SSLv23)\n    print cert\n    f = open(\ncert.pem\n, \nw\n)\n    f.write(cert)\n    f.close()", 
            "title": "ranger_relay"
        }, 
        {
            "location": "/plugins_reference/ranger/ranger_relay/#ranger_relay", 
            "text": "", 
            "title": "ranger_relay"
        }, 
        {
            "location": "/plugins_reference/ranger/ranger_relay/#synopsis", 
            "text": "All Apache Ranger commands will be performed using Ranger HTTP/REST API. This definition will provide informations for HADeploy to use this interface.  There should be only one entry of this type in the HADeploy definition file.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/ranger/ranger_relay/#attributes", 
            "text": "ranger_relay is a map with the following attributes:     Name  req?  Description      host  yes  From which host these commands will be launched. Typically, an edge node in the cluster. But may be a host outside the cluster.    ranger_url  yes  The Ranger base URL to access Ranger API. Same host:port as the Ranger Admin GUI. Typically:  http://myranger.server.com:6080 or https://myranger.server.com:6182    ranger_username  yes  The user name to log on the Ranger Admin. Must have enough rights to manage policies    ranger_password  yes  The password associated with the admin_username. May be encrypted. Refer to  encrypted variables    validate_certs  no  Useful if Ranger Admin connection is using SSL. If no, SSL certificates will not be validated. This should only be used on personally controlled sites using self-signed certificates. Default:  yes    ca_bundle_relay_file  no  Useful if Ranger Admin connection is using SSL. Allow to specify a CA_BUNDLE file, a file that contains root and intermediate certificates to validate the Ranger Admin certificate in .pem format. This file will be looked up on the relay host system, on which this module will be executed.    ca_bundle_local_file  no  Same as above, except this file will be looked up locally, relative to the main file. It will be copied on the relay host at the location defined by  ca_bundle_relay_file    hdfs_service_name  no  In most cases, you should not need to set this parameter. It defines the Ranger Admin HDFS service, typically:  yourClusterName _hadoop . It must be set if there are several such services defined in your Ranger Admin configuration, to select the one you intend to use.    hbase_service_name  no  In most cases, you should not need to set this parameter. It defines the Ranger Admin HBase service, typically:  yourClusterName _hbase . It must be set if there are several such services defined in your Ranger Admin configuration, to select the one you intend to use.    kafka_service_name  no  In most cases, you should not need to set this parameter. It defines the Ranger Admin Kafka service, typically:  yourClusterName _kafka . It must be set if there are several such services defined in your Ranger Admin configuration, to select the one you intend to use.    hive_service_name  no  In most cases, you should not need to set this parameter. It defines the Ranger Admin Hive service, typically:  yourClusterName _hive . It must be set if there are several such services defined in your Ranger Admin configuration, to select the one you intend to use.    yarn_service_name  no  In most cases, you should not need to set this parameter. It defines the Ranger Admin Yarn service, typically:  yourClusterName _yarn . It must be set if there are several such services defined in your Ranger Admin configuration, to select the one you intend to use.    storm_service_name  no  In most cases, you should not need to set this parameter. It defines the Ranger Admin Storm service, typically:  yourClusterName _storm . It must be set if there are several such services defined in your Ranger Admin configuration, to select the one you intend to use.    policy_name_decorator  no  To distinguish Ranger policy managed by HADeploy, a naming convention is applied by default. The policy name, as it will appears in the GUI Ranger interface will be in the form  HAD[ policyName ] , where  policyName  is the name of the policy as you provide it. This is achieved by wrapping the name with this pattern, where  {0}  is substituted with the policy name.  For python aware reader, this is performed as: \"HAD[{0}]\".format(policyName) . If you just want to have raw policy name, simply define the parameter with  {0} . Default:  HAD[{0}]    no_log  no  Boolean. Prevent some messages to be displayed, as they can contains sensitive information. This can make debugging somewhat more difficult, so temporary setting this swith to  False  may be useful to get more information (Including sensitive ones!). Default  True    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/ranger/ranger_relay/#example", 
            "text": "A simple configuration:  ranger_relay:\n  host: en1\n  ranger_url:  http://ranger.mycluster.mycompany.com:6080\n  ranger_username: admin\n  ranger_password: admin  A more secure configuration, with https, certificate validation and encrypted password.  encrypted_vars:\n  ranger_password: |\n    $ANSIBLE_VAULT;1.1;AES256\n    34396662613462623565323936616330623661623065343033646136643635653430636238613962\n    3537343131346462343138343064313937646366363435340a633532366162623838376436366362\n    61393033343932303636653066336130616132383463373934396265306364363562613565613165\n    6163613739303430650a356136353865623534643237646166393230613933396166663963633538\n    3664\n\nranger_relay:\n  host: en1\n  ranger_url:  https://ranger.mycluster.mycompany.com:6182\n  ranger_username: admin\n  ranger_password:  {{ranger_password}}   \n  ca_bundle_local_file: cert/ranger_mycluster_cert.pem\n  ca_bundle_relay_file: /etc/security/certs/ranger_mycluster_cert.pem  When using such encryption feature, you will need to provide a password when launching HADeploy. Otherwise you will have an error like the following:   The offending line appears to be:\n\n  vars:\n      rangerPassword: !vault |\n                      ^ here  More detail on how to encrypt a value and providing a password on execution at  encrypted variables", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/ranger/ranger_relay/#ca_bundle", 
            "text": "Internally, HADeploy use the python  requests  API to access Ranger. The provided  ca_bundle_relay_file  will be used as the  verify  parameter of all HTTP requests. More info   here .  In some cases, a CA_BUNDLE may be simply the certificate of the Ranger server, in PEM format.  To grab this certificate, you may use a tiny python program like the following:  import ssl\n\nif __name__ == '__main__':\n    cert = ssl.get_server_certificate(( ranger.mycluster.corp.com , 6182), ssl_version=ssl.PROTOCOL_SSLv23)\n    print cert\n    f = open( cert.pem ,  w )\n    f.write(cert)\n    f.close()", 
            "title": "CA_BUNDLE"
        }, 
        {
            "location": "/plugins_reference/ranger/storm_ranger_policies/", 
            "text": "storm_ranger_policies\n\n\nSynopsis\n\n\nAllow definition of a list of Apache Ranger policies for setting permissions on Storm server access\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe policy name. Will be decorated to mark it as managed by HADeploy, as described in \nranger_relay\n.\n\n\n\n\n\n\ntopologies\n\n\nyes\n\n\nA list of topologies on which this policy will apply. Accept wildcard characters '*' and '?'.\n\n\n\n\n\n\naudit\n\n\nno\n\n\nDid this policy is audited by Ranger.\nDefault: \nyes\n\n\n\n\n\n\nenabled\n\n\nno\n\n\nAllow this policy to be disabled.\nDefault: \nyes\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this policy to be removed when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\npermissions\n\n\nyes\n\n\nA list of permissions defining rights granted by this policy. See below\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nPermissions\n\n\nEach item of the permission list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nusers\n\n\nyes if \ngroups\n is undefined\n\n\nA list of users this policy will apply on. May be empty if some groups are defined.\n\n\n\n\n\n\ngroups\n\n\nyes if \nusers\n is undefined\n\n\nA list of groups this policy will apply on. May be empty if some users are defined.\n\n\n\n\n\n\naccesses\n\n\nyes\n\n\nThe list of rights granted by this policy. May include \nsubmitTopology\n, \nfileUpload\n, \nfileDownload\n, \nkillTopology\n, \nrebalance\n, \nactivate\n, \ndeactivate\n, \ngetTopologyConf\n, \ngetTopology\n, \ngetUserTopology\n, \ngetTopologyInfo\n and \nuploadNewCredentials\n.\n\n\n\n\n\n\ndelegate_admin\n\n\nno\n\n\nWhen a policy is assigned to a user or a group of users those users become the delegated admin. The delegated admin can update, delete the policies.\nDefault: \nno\n\n\n\n\n\n\n\n\nExamples\n\n\nThis example will:\n\n\n\n\n\n\nGrant the right to 'stormrunner' user to launch new topology\n\n\n\n\n\n\nGrant all rights to 'stormrunner' on all topologies where name begin with 'storm'.\n\n\n\n\n\n\nstorm_ranger_policies:\n- name: \nstormrunnerAsSubmitter\n\n  topologies: \n  - \n*\n\n  - \ndummy1\n\n  permissions:\n  - users:\n    - stormrunner\n    accesses:\n    - 'submitTopology'\n    - 'fileUpload'\n\n- name: \nstormrunnerAsPartialAdmin\n\n  topologies: \n  - \nstorm*\n\n  permissions:\n  - users:\n    - stormrunner\n    accesses:\n    - 'submitTopology'\n    - 'fileUpload'\n    - 'fileDownload'\n    - 'killTopology'\n    - 'rebalance'\n    - 'activate'\n    - 'deactivate'\n    - 'getTopologyConf'\n    - 'getTopology'\n    - 'getUserTopology'\n    - 'getTopologyInfo'\n    - 'uploadNewCredentials'\n\n\n\n\nNote the trick on the first definition: Adding a 'dummy1' prevent this rule to clash with another one applying on all topologies (Ranger does to allow two policies to apply on the same set of topologies).", 
            "title": "storm_ranger_policies"
        }, 
        {
            "location": "/plugins_reference/ranger/storm_ranger_policies/#storm_ranger_policies", 
            "text": "", 
            "title": "storm_ranger_policies"
        }, 
        {
            "location": "/plugins_reference/ranger/storm_ranger_policies/#synopsis", 
            "text": "Allow definition of a list of Apache Ranger policies for setting permissions on Storm server access", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/ranger/storm_ranger_policies/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  The policy name. Will be decorated to mark it as managed by HADeploy, as described in  ranger_relay .    topologies  yes  A list of topologies on which this policy will apply. Accept wildcard characters '*' and '?'.    audit  no  Did this policy is audited by Ranger. Default:  yes    enabled  no  Allow this policy to be disabled. Default:  yes    no_remove  no  Boolean: Prevent this policy to be removed when HADeploy will be used in REMOVE mode. Default:  no    permissions  yes  A list of permissions defining rights granted by this policy. See below    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/ranger/storm_ranger_policies/#permissions", 
            "text": "Each item of the permission list has the following attributes:     Name  req?  Description      users  yes if  groups  is undefined  A list of users this policy will apply on. May be empty if some groups are defined.    groups  yes if  users  is undefined  A list of groups this policy will apply on. May be empty if some users are defined.    accesses  yes  The list of rights granted by this policy. May include  submitTopology ,  fileUpload ,  fileDownload ,  killTopology ,  rebalance ,  activate ,  deactivate ,  getTopologyConf ,  getTopology ,  getUserTopology ,  getTopologyInfo  and  uploadNewCredentials .    delegate_admin  no  When a policy is assigned to a user or a group of users those users become the delegated admin. The delegated admin can update, delete the policies. Default:  no", 
            "title": "Permissions"
        }, 
        {
            "location": "/plugins_reference/ranger/storm_ranger_policies/#examples", 
            "text": "This example will:    Grant the right to 'stormrunner' user to launch new topology    Grant all rights to 'stormrunner' on all topologies where name begin with 'storm'.    storm_ranger_policies:\n- name:  stormrunnerAsSubmitter \n  topologies: \n  -  * \n  -  dummy1 \n  permissions:\n  - users:\n    - stormrunner\n    accesses:\n    - 'submitTopology'\n    - 'fileUpload'\n\n- name:  stormrunnerAsPartialAdmin \n  topologies: \n  -  storm* \n  permissions:\n  - users:\n    - stormrunner\n    accesses:\n    - 'submitTopology'\n    - 'fileUpload'\n    - 'fileDownload'\n    - 'killTopology'\n    - 'rebalance'\n    - 'activate'\n    - 'deactivate'\n    - 'getTopologyConf'\n    - 'getTopology'\n    - 'getUserTopology'\n    - 'getTopologyInfo'\n    - 'uploadNewCredentials'  Note the trick on the first definition: Adding a 'dummy1' prevent this rule to clash with another one applying on all topologies (Ranger does to allow two policies to apply on the same set of topologies).", 
            "title": "Examples"
        }, 
        {
            "location": "/plugins_reference/ranger/yarn_ranger_policies/", 
            "text": "yarn_ranger_policies\n\n\nSynopsis\n\n\nAllow definition of a list of Apache Ranger policies for setting Yarn permissions\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe policy name. Will be decorated to mark it as managed by HADeploy, as described in \nranger_relay\n.\n\n\n\n\n\n\nqueues\n\n\nyes\n\n\nA list of Yarn queues on which this policy will apply. Accept wildcard characters '*' and '?'.\n\n\n\n\n\n\naudit\n\n\nno\n\n\nDid this policy is audited by Ranger.\nDefault: \nyes\n\n\n\n\n\n\nenabled\n\n\nno\n\n\nAllow this policy to be disabled.\nDefault: \nyes\n\n\n\n\n\n\nrecursive\n\n\nno\n\n\nDid this policy apply recursively.\nDefault: \nyes\n\n\n\n\n\n\nno_remove\n\n\nFalse\n\n\nBoolean: Prevent this policy to be removed when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\npermissions\n\n\nyes\n\n\nA list of permissions defining rights granted by this policy. See below\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\npermissions\n\n\nEach item of the permission list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nusers\n\n\nyes if \ngroups\n is undefined\n\n\nA list of users this policy will apply on. May be empty if some groups are defined.\n\n\n\n\n\n\ngroups\n\n\nyes if \nusers\n is undefined\n\n\nA list of groups this policy will apply on. May be empty if some users are defined.\n\n\n\n\n\n\naccesses\n\n\nyes\n\n\nThe list of rights granted by this policy. May include \nsubmit-app\n and \nadmin-queue\n.\n\n\n\n\n\n\ndelegate_admin\n\n\nno\n\n\nWhen a policy is assigned to a user or a group of users those users become the delegated admin. The delegated admin can update and delete the policies.\nDefault: \nno\n\n\n\n\n\n\n\n\nExamples\n\n\nyarn_ranger_policies:\n- name: puser1\n  queues: \n  - user1\n  audit: true\n  permissions:\n  - groups: \n    - users\n    accesses:\n    - submit-app\n  - users:\n    - admin \n    accesses:\n    - submit-app\n    - admin-queue\n    delegate_admin: true", 
            "title": "yarn_ranger_policies"
        }, 
        {
            "location": "/plugins_reference/ranger/yarn_ranger_policies/#yarn_ranger_policies", 
            "text": "", 
            "title": "yarn_ranger_policies"
        }, 
        {
            "location": "/plugins_reference/ranger/yarn_ranger_policies/#synopsis", 
            "text": "Allow definition of a list of Apache Ranger policies for setting Yarn permissions", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/ranger/yarn_ranger_policies/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  The policy name. Will be decorated to mark it as managed by HADeploy, as described in  ranger_relay .    queues  yes  A list of Yarn queues on which this policy will apply. Accept wildcard characters '*' and '?'.    audit  no  Did this policy is audited by Ranger. Default:  yes    enabled  no  Allow this policy to be disabled. Default:  yes    recursive  no  Did this policy apply recursively. Default:  yes    no_remove  False  Boolean: Prevent this policy to be removed when HADeploy will be used in REMOVE mode. Default:  no    permissions  yes  A list of permissions defining rights granted by this policy. See below    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/ranger/yarn_ranger_policies/#permissions", 
            "text": "Each item of the permission list has the following attributes:     Name  req?  Description      users  yes if  groups  is undefined  A list of users this policy will apply on. May be empty if some groups are defined.    groups  yes if  users  is undefined  A list of groups this policy will apply on. May be empty if some users are defined.    accesses  yes  The list of rights granted by this policy. May include  submit-app  and  admin-queue .    delegate_admin  no  When a policy is assigned to a user or a group of users those users become the delegated admin. The delegated admin can update and delete the policies. Default:  no", 
            "title": "permissions"
        }, 
        {
            "location": "/plugins_reference/ranger/yarn_ranger_policies/#examples", 
            "text": "yarn_ranger_policies:\n- name: puser1\n  queues: \n  - user1\n  audit: true\n  permissions:\n  - groups: \n    - users\n    accesses:\n    - submit-app\n  - users:\n    - admin \n    accesses:\n    - submit-app\n    - admin-queue\n    delegate_admin: true", 
            "title": "Examples"
        }, 
        {
            "location": "/plugins_reference/storm/storm_overview/", 
            "text": "Storm plugin: Overview\n\n\nAim of the Storm plugin is to handle Storm topologies lifecycle.\n\n\n\n\n\n\nStart topologies\n\n\n\n\n\n\nStop topologies\n\n\n\n\n\n\nGet current topologies status\n\n\n\n\n\n\nThis is achieved by:\n\n\n\n\n\n\nIssuing command to the Storm subsystem through the Storm UI REST API\n\n\n\n\n\n\nStart topology when required using a user provided launching script.\n\n\n\n\n\n\nAll these operations are performed on a specific node included in the cluster. This node is designated as a \nstorm_relay\n\n\nTopologies deployment.\n\n\nThe deployment of each topology by itself is NOT in the scope of this plugin. Typically this consist in:\n\n\n\n\n\n\nCreate a deployment folder.\n\n\n\n\n\n\nDeploying a jar\n\n\n\n\n\n\nDeploying one or several configuration files\n\n\n\n\n\n\nEnventually, deploying a script to launch the topology.\n\n\n\n\n\n\nThis at least on the Storm relay node and eventually on one or several other nodes, for resiliency. \n\n\nAll these tasks can be achieved using this HADeploy \nfolders\n and \nfiles\n specification. \n\n\nTemplating mechanism and support of Maven repository built in the \nfiles\n plugin will be of great help here. \n\n\nActions \nstop\n,\nstart\n and \nstatus\n\n\nThe \nstorm\n plugin introduce three new actions:\n\n\nhadeploy --src ..... --action start\n\n\n\n\nWill start all toplogies described by the \nstorm_topologies\n list. And \n\n\nhadeploy --src ..... --action stop\n\n\n\n\nWill kill the same toplogies. While \n\n\nhadeploy --src ..... --action status\n\n\n\n\nwill display current status of the topologies, in a rather primitive form.\n\n\nAlso, the Storm plugin kill all running topologies at one of the first step of the removal action (\n--action remove\n).\n\n\nOf course, all this will occur only on topologies HADeploy is aware of (Defined with \nstorm_topologies\n). Other topologies will not be impacted.\n\n\nAsynchronous mode\n\n\nA single topology launch take a signifiant amount of time. When there are several ones to launch, performing all launch simultaneously can save a lot of time. This is the default behavior of HADeploy.\n\n\nIn this default mode, all launching commands are run in a detached mode. Then HADeploy wait for all topologies to reach the \nactive\n state.\n\n\nBut this mode has a drawback. If a launch fail, there is no easy way to get the error message. \nThis is why this mode can be deactivated by setting the \nasync\n attribute of \nstorm_relay\n to false. \nIn such case, topologies wil be launched one at a time, and in case of error, processing will stop and appropriate error message will be raised.\n\n\nTopologies killing.\n\n\nWhen HADeploy is instructed to halt all topologies (\n--action stop\n), it also perform this task asynchronously.\n\n\n\n\n\n\nAll topologies are swithed to the \nKILLED\n state.\n\n\n\n\n\n\nThen HADeploy wait for all topologies to terminate, after the defined 'wait_time_secs' (Delay between spouts deactivation and topology destruction)\n\n\n\n\n\n\nSetting the \nasync\n flag of of \nstorm_relay\n to false has no effect on this behavior.\n\n\nNotifications: Topologies restart\n\n\nLet's say we now want to update the topology's jar or one of the associated configuration files.\n\n\nWe can modify it and trigger a new deployment. HADeploy will notice the modification and push the new version on the target hosts. But, the running topologies will be unafected.\n\n\nWe can restart it manually. But, HADeploy provide a mechanisme to automate this. By adding a \nnotify\n attribute to the \nfiles\n definition. See the example below.\n\n\nNote also if the deployment trigger the restart of several topologies, both kill and restart will be performed asynchronously, to save time.\n\n\nExample\n\n\nHere is a snippet describing the deployment of two very simple topologies:\n\n\nvars:\n  storm_launcher_host: en1\n  basedir1: \n/opt/storm1\n\n  basedir2: \n/opt/storm2\n\n  user: stormrunner\n  group: stormrunner\n  storm1_version: \n0.1.0-SNAPSHOT\n\n  storm2_version: \n0.3.0-SNAPSHOT\n\n\nstorm_relay:\n  host: ${storm_launcher_host}\n  storm_ui_url: \nhttp://stui.mycluster.mydomain.com:8744/\n\n\nmaven_repositories:\n- name: myrepo\n  snapshots_url: http://myrepo.mydomain.com/nexus/repository/maven-snapshots/\n  releases_url: http://myrepo.mydomain.com/nexus/repository/maven-releases/\n\nfolders:\n- { path: \n${basedir1}\n, scope: \n${storm_launcher_host}\n, owner: \n${user}\n, group: \n${group}\n, mode: \n755\n }\n- { path: \n${basedir2}\n, scope: \n${storm_launcher_host}\n, owner: \n${user}\n, group: \n${group}\n, mode: \n755\n }\n\nfiles:\n- { scope: \n${storm_launcher_host}\n, src: \nmvn://myrepo/com.mydomain/storm1/${storm1_version}/uber\n, \n    notify: ['storm://storm1'], dest_folder: \n${basedir1}\n, owner: \n${user}\n, group: \n${group}\n, mode: \n0644\n }\n- { scope: \n${storm_launcher_host}\n, src: \nmvn://myrepo/com.mydomain/storm2/${storm2_version}/uber\n, \n    notify: ['storm://storm2'], dest_folder: \n${basedir2}\n, owner: \n${user}\n, group: \n${group}\n, mode: \n0644\n }\n\n- { scope: \n${storm_launcher_host}\n, src: \ntmpl://launch2.sh\n, dest_folder: \n${basedir2}\n, \n    notify: ['storm://storm2'], owner: \n${user}\n, group: \n${group}\n, mode: \n0744\n }\n\n\nstorm_topologies:\n\n- name: storm1\n  launching_dir: ${basedir1}\n  launching_cmd: \nstorm jar ./storm1-${storm1_version}-uber.jar com.mydomain.storm1.ClusterTopology\n\n  wait_time_secs: 10\n\n- name: storm2\n  launching_dir: ${basedir2}\n  launching_cmd: \n./launch2.sh\n\n  wait_time_secs: 15\n\n\n\n\n\nThis is of course not complete, as it lack at least the target cluster definition.\n\n\nPlease refer to \nstorm_relay\n and \nstorm_topologies\n for a complete description. And to \nfiles\n for the \nnotify\n syntax.\n\n\nOf course, before being able to launch the topologies (\n--action start\n), a deployment must be performed before (\n--action deploy\n)", 
            "title": "storm_overview"
        }, 
        {
            "location": "/plugins_reference/storm/storm_overview/#storm-plugin-overview", 
            "text": "Aim of the Storm plugin is to handle Storm topologies lifecycle.    Start topologies    Stop topologies    Get current topologies status    This is achieved by:    Issuing command to the Storm subsystem through the Storm UI REST API    Start topology when required using a user provided launching script.    All these operations are performed on a specific node included in the cluster. This node is designated as a  storm_relay", 
            "title": "Storm plugin: Overview"
        }, 
        {
            "location": "/plugins_reference/storm/storm_overview/#topologies-deployment", 
            "text": "The deployment of each topology by itself is NOT in the scope of this plugin. Typically this consist in:    Create a deployment folder.    Deploying a jar    Deploying one or several configuration files    Enventually, deploying a script to launch the topology.    This at least on the Storm relay node and eventually on one or several other nodes, for resiliency.   All these tasks can be achieved using this HADeploy  folders  and  files  specification.   Templating mechanism and support of Maven repository built in the  files  plugin will be of great help here.", 
            "title": "Topologies deployment."
        }, 
        {
            "location": "/plugins_reference/storm/storm_overview/#actions-stopstart-and-status", 
            "text": "The  storm  plugin introduce three new actions:  hadeploy --src ..... --action start  Will start all toplogies described by the  storm_topologies  list. And   hadeploy --src ..... --action stop  Will kill the same toplogies. While   hadeploy --src ..... --action status  will display current status of the topologies, in a rather primitive form.  Also, the Storm plugin kill all running topologies at one of the first step of the removal action ( --action remove ).  Of course, all this will occur only on topologies HADeploy is aware of (Defined with  storm_topologies ). Other topologies will not be impacted.", 
            "title": "Actions stop,start and status"
        }, 
        {
            "location": "/plugins_reference/storm/storm_overview/#asynchronous-mode", 
            "text": "A single topology launch take a signifiant amount of time. When there are several ones to launch, performing all launch simultaneously can save a lot of time. This is the default behavior of HADeploy.  In this default mode, all launching commands are run in a detached mode. Then HADeploy wait for all topologies to reach the  active  state.  But this mode has a drawback. If a launch fail, there is no easy way to get the error message. \nThis is why this mode can be deactivated by setting the  async  attribute of  storm_relay  to false. \nIn such case, topologies wil be launched one at a time, and in case of error, processing will stop and appropriate error message will be raised.", 
            "title": "Asynchronous mode"
        }, 
        {
            "location": "/plugins_reference/storm/storm_overview/#topologies-killing", 
            "text": "When HADeploy is instructed to halt all topologies ( --action stop ), it also perform this task asynchronously.    All topologies are swithed to the  KILLED  state.    Then HADeploy wait for all topologies to terminate, after the defined 'wait_time_secs' (Delay between spouts deactivation and topology destruction)    Setting the  async  flag of of  storm_relay  to false has no effect on this behavior.", 
            "title": "Topologies killing."
        }, 
        {
            "location": "/plugins_reference/storm/storm_overview/#notifications-topologies-restart", 
            "text": "Let's say we now want to update the topology's jar or one of the associated configuration files.  We can modify it and trigger a new deployment. HADeploy will notice the modification and push the new version on the target hosts. But, the running topologies will be unafected.  We can restart it manually. But, HADeploy provide a mechanisme to automate this. By adding a  notify  attribute to the  files  definition. See the example below.  Note also if the deployment trigger the restart of several topologies, both kill and restart will be performed asynchronously, to save time.", 
            "title": "Notifications: Topologies restart"
        }, 
        {
            "location": "/plugins_reference/storm/storm_overview/#example", 
            "text": "Here is a snippet describing the deployment of two very simple topologies:  vars:\n  storm_launcher_host: en1\n  basedir1:  /opt/storm1 \n  basedir2:  /opt/storm2 \n  user: stormrunner\n  group: stormrunner\n  storm1_version:  0.1.0-SNAPSHOT \n  storm2_version:  0.3.0-SNAPSHOT \n\nstorm_relay:\n  host: ${storm_launcher_host}\n  storm_ui_url:  http://stui.mycluster.mydomain.com:8744/ \n\nmaven_repositories:\n- name: myrepo\n  snapshots_url: http://myrepo.mydomain.com/nexus/repository/maven-snapshots/\n  releases_url: http://myrepo.mydomain.com/nexus/repository/maven-releases/\n\nfolders:\n- { path:  ${basedir1} , scope:  ${storm_launcher_host} , owner:  ${user} , group:  ${group} , mode:  755  }\n- { path:  ${basedir2} , scope:  ${storm_launcher_host} , owner:  ${user} , group:  ${group} , mode:  755  }\n\nfiles:\n- { scope:  ${storm_launcher_host} , src:  mvn://myrepo/com.mydomain/storm1/${storm1_version}/uber , \n    notify: ['storm://storm1'], dest_folder:  ${basedir1} , owner:  ${user} , group:  ${group} , mode:  0644  }\n- { scope:  ${storm_launcher_host} , src:  mvn://myrepo/com.mydomain/storm2/${storm2_version}/uber , \n    notify: ['storm://storm2'], dest_folder:  ${basedir2} , owner:  ${user} , group:  ${group} , mode:  0644  }\n\n- { scope:  ${storm_launcher_host} , src:  tmpl://launch2.sh , dest_folder:  ${basedir2} , \n    notify: ['storm://storm2'], owner:  ${user} , group:  ${group} , mode:  0744  }\n\n\nstorm_topologies:\n\n- name: storm1\n  launching_dir: ${basedir1}\n  launching_cmd:  storm jar ./storm1-${storm1_version}-uber.jar com.mydomain.storm1.ClusterTopology \n  wait_time_secs: 10\n\n- name: storm2\n  launching_dir: ${basedir2}\n  launching_cmd:  ./launch2.sh \n  wait_time_secs: 15  This is of course not complete, as it lack at least the target cluster definition.  Please refer to  storm_relay  and  storm_topologies  for a complete description. And to  files  for the  notify  syntax.  Of course, before being able to launch the topologies ( --action start ), a deployment must be performed before ( --action deploy )", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/storm/storm_relay/", 
            "text": "storm_relay\n\n\nSynopsis\n\n\nMost of Storm commands will be performed using Storm UI HTTP/REST API. This definition will provide informations for HADeploy to use this interface.\n\n\nThis is a reference part. Refer to the associated \noverview\n for a more synthetical view.\n\n\nAttributes\n\n\nstorm_relay\n is a map with the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nhost\n\n\nyes\n\n\nThe host which will be used for both launching topologies and accessing the Storm UI REST interface\n\n\n\n\n\n\nstorm_ui_url\n\n\nyes\n\n\nThe base URL to access Storm UI REST API. Same host:port as the Storm GUI.\nTypically: \nhttp://mystormui.mycluster.com:8744\n\n\n\n\n\n\nasync\n\n\nno\n\n\nBoolean Specify if the all the topologies can be launched simultaneously. Default: \nyes\n. More info \nhere\n.\n\n\n\n\n\n\ndefault_timeout_secs\n\n\nno\n\n\nDefault value for \ntimeout_secs\n value on \nstorm_topologies\n entry. Default to 90 seconds\n\n\n\n\n\n\nprincipal\n\n\nno\n\n\nA Kerberos principal allowing all Storm related operation to be performed. See \nbelow\n\n\n\n\n\n\nlocal_keytab_path\n\n\nno\n\n\nA local path to the associated keytab file. This path is relative to the embeding file. See \nbelow\n\n\n\n\n\n\nrelay_keytab_path\n\n\nno\n\n\nA path to the associated keytab file on the relay host. See \nbelow\n\n\n\n\n\n\ntools_folder\n\n\nno\n\n\nFolder used by HADeploy to store keytab if needed.\nDefault: \n/tmp/hadeploy_\nuser\n/\n where \nuser\n is the \nssh_user\n defined for this relay host.\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nKerberos authentication\n\n\nWhen \nprincipal\n and \n..._keytab_path\n variables are defined, Kerberos authentication will be activated for all Storm operations. This means a \nkinit\n will be issued with provided values before any Storm access, and a \nkdestroy\n issued after. This has the following consequences:\n\n\n\n\n\n\nAll Storm operations will be performed on behalf of the user defined by the provided principal. \n\n\n\n\n\n\nThe \nkinit\n will be issued on the relay host with the \nssh_user\n account. This means any previous ticket own by this user on this node will be lost. \n\n\n\n\n\n\nRegarding the keytab file, two cases:\n\n\n\n\n\n\nThis keytab file already exists on the relay host. In such case, the \nrelay_keytab_path\n must be set to the location of this file. And the relay host's \nssh_user\n must have read access on it.\n\n\n\n\n\n\nThis keytab file is not present on the relay host. In such case the \nlocal_keytab_path\n must be set to its local location. HADeploy will take care of copying it on the remote relay host, \nin a location under \ntools_folder\n. Note you can also modify this target location by setting also the \nrelay_keytab_path\n parameter. In this case, \nit must be the full path, including the keytab file name. And the containing folder must exists.\n\n\n\n\n\n\nExample\n\n\nThe simplest case:\n\n\nstorm_relay:\n  host: en1\n  storm_ui_url: \nhttp://stui.mycluster.mydomain.com:8744/\n\n\n\n\n\nAnd a more complete case, in a secured environement.\n\n\nstorm_relay:\n  host: en1\n  storm_ui_url: \nhttp://stui.mysecuredcluster.mydomain.com:8744/\n\n  async: no\n  principal: sa\n  local_keytab_path: ./sa-gate17.keytab\n  default_timeout_secs: 240", 
            "title": "storm_relay"
        }, 
        {
            "location": "/plugins_reference/storm/storm_relay/#storm_relay", 
            "text": "", 
            "title": "storm_relay"
        }, 
        {
            "location": "/plugins_reference/storm/storm_relay/#synopsis", 
            "text": "Most of Storm commands will be performed using Storm UI HTTP/REST API. This definition will provide informations for HADeploy to use this interface.  This is a reference part. Refer to the associated  overview  for a more synthetical view.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/storm/storm_relay/#attributes", 
            "text": "storm_relay  is a map with the following attributes:     Name  req?  Description      host  yes  The host which will be used for both launching topologies and accessing the Storm UI REST interface    storm_ui_url  yes  The base URL to access Storm UI REST API. Same host:port as the Storm GUI. Typically:  http://mystormui.mycluster.com:8744    async  no  Boolean Specify if the all the topologies can be launched simultaneously. Default:  yes . More info  here .    default_timeout_secs  no  Default value for  timeout_secs  value on  storm_topologies  entry. Default to 90 seconds    principal  no  A Kerberos principal allowing all Storm related operation to be performed. See  below    local_keytab_path  no  A local path to the associated keytab file. This path is relative to the embeding file. See  below    relay_keytab_path  no  A path to the associated keytab file on the relay host. See  below    tools_folder  no  Folder used by HADeploy to store keytab if needed. Default:  /tmp/hadeploy_ user /  where  user  is the  ssh_user  defined for this relay host.    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/storm/storm_relay/#kerberos-authentication", 
            "text": "When  principal  and  ..._keytab_path  variables are defined, Kerberos authentication will be activated for all Storm operations. This means a  kinit  will be issued with provided values before any Storm access, and a  kdestroy  issued after. This has the following consequences:    All Storm operations will be performed on behalf of the user defined by the provided principal.     The  kinit  will be issued on the relay host with the  ssh_user  account. This means any previous ticket own by this user on this node will be lost.     Regarding the keytab file, two cases:    This keytab file already exists on the relay host. In such case, the  relay_keytab_path  must be set to the location of this file. And the relay host's  ssh_user  must have read access on it.    This keytab file is not present on the relay host. In such case the  local_keytab_path  must be set to its local location. HADeploy will take care of copying it on the remote relay host, \nin a location under  tools_folder . Note you can also modify this target location by setting also the  relay_keytab_path  parameter. In this case, \nit must be the full path, including the keytab file name. And the containing folder must exists.", 
            "title": "Kerberos authentication"
        }, 
        {
            "location": "/plugins_reference/storm/storm_relay/#example", 
            "text": "The simplest case:  storm_relay:\n  host: en1\n  storm_ui_url:  http://stui.mycluster.mydomain.com:8744/   And a more complete case, in a secured environement.  storm_relay:\n  host: en1\n  storm_ui_url:  http://stui.mysecuredcluster.mydomain.com:8744/ \n  async: no\n  principal: sa\n  local_keytab_path: ./sa-gate17.keytab\n  default_timeout_secs: 240", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/storm/storm_topologies/", 
            "text": "storm_topologies\n\n\nSynopsis\n\n\nProvide a list of Storm topologies to manage\n\n\nThis is a reference part. Refer to the associated \noverview\n for a more synthetical view.\n\n\nAttributes\n\n\nEach item of the list has the following attribute: \n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nName of the topology.\n\n\n\n\n\n\nlaunching_cmd\n\n\nyes\n\n\nThe command to launch this topology. May be 'storm jar ...' for simple case. A launching script for more complex one.\n\n\n\n\n\n\nlaunching_dir\n\n\nno\n\n\nWill \ncd\n in this folder before launching 'launching_cmd' Must be an absolute path.\nDefault: \n~\n\n\n\n\n\n\nwait_time_secs\n\n\nno\n\n\nThe wait_time in seconds provided to the `kill' command (Delay between spouts deactivation and topology destruction)\nDefault: 30.\n\n\n\n\n\n\ntimeout_secs\n\n\nno\n\n\nTimeout before raising an error value when waiting a target state.\nDefault: Value set by \nstorm_relay\n:\ndefault_timeout_secs\n\n\n\n\n\n\nranger_policy\n\n\nno\n\n\nDefinition of Apache Ranger policy bound to this topology. Parameters are same as \nstorm_ranger_policies\n except than \ntopologies\n should not be defined.\nThe policy name can be explicitly defined. Otherwise, a name will be generated as \"\n_\ntopology\n_\n\".\nSee example below for more information\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBolean. Used only for eventual associated ranger policy.  Setting to \nyes\nprevent this policy to be remove when  HADeploy will be used in REMOVE mode.\nDefault: \nno\n.\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nAsynchronous launch\n\n\nTo preserve launching time, HADeploy launch all topologies in a simultaneous way, by using some asynchronous procesing. \n\n\nThis can lead to difficulty to debug, as failing launch des not raise any error, except a timeout after quite long time. More on that \nhere\n\n\nExample\n\n\nstorm_topologies:\n- name: storm1\n  launching_dir: ${basedir1}\n  launching_cmd: \nstorm jar ./storm1-${storm1_version}-uber.jar com.mydomain.storm1.ClusterTopology\n\n  wait_time_secs: 10\n- name: storm2\n  launching_dir: ${basedir2}\n  launching_cmd: \n./launch2.sh\n\n  wait_time_secs: 15\n\n\n\n\nstorm_topologies:\n- name: storm2\n  launching_dir: /opt/storm2\n  launching_cmd: \n./launch2.sh\n\n  wait_time_secs: 10\n  ranger_policy:\n    audit: yes\n    enabled: yes\n    permissions:\n    - users:\n      - stormrunner\n      accesses:\n      - 'fileDownload'\n      - 'killTopology'\n      - 'rebalance'\n      - 'activate'\n      - 'deactivate'\n      - 'getTopologyConf'\n      - 'getTopology'\n      - 'getUserTopology'\n      - 'getTopologyInfo'\n\n\n\n\nThe launching cmd or script\n\n\nHere is some requirement about the launching command or script.\n\n\n\n\n\n\nIt must ensure setting topology name same as the one provided in this description.\n\n\n\n\n\n\nIt must be fully synchronous, i.e. not running in the background. But it must exit once topology is started.\n\n\n\n\n\n\nFor kerberos: client_jaas.conf\n\n\nIn a kerboros secured context, the storm command use a configuration file to define how it will authenticate against the Storm server. \n\n\nFor HADeploy usage, this file must be configured to use the current ticket in the local cache. Unfortunately, this is not always the case. Sometime, it may be configured to use another principal and keytab.\n\n\nIf this is the case, we suggest to explicitly provide such a configuration file: For example, in an Hortonworks context, one can write:\n\n\nstorm -c java.security.auth.login.config=/usr/hdp/current/storm-client/conf/client_jaas.conf jar ./myjar.jar com.mydomain.mytopology.ClusterTopology  $1\n\n\n\n\nHere is the content of a \nclient_jaas.conf\n we can use:\n\n\nStormClient {\n   com.sun.security.auth.module.Krb5LoginModule required\n   useTicketCache=true\n   renewTicket=true\n   serviceName=\nnimbus\n;\n};", 
            "title": "storm_topologies"
        }, 
        {
            "location": "/plugins_reference/storm/storm_topologies/#storm_topologies", 
            "text": "", 
            "title": "storm_topologies"
        }, 
        {
            "location": "/plugins_reference/storm/storm_topologies/#synopsis", 
            "text": "Provide a list of Storm topologies to manage  This is a reference part. Refer to the associated  overview  for a more synthetical view.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/storm/storm_topologies/#attributes", 
            "text": "Each item of the list has the following attribute:      Name  req?  Description      name  yes  Name of the topology.    launching_cmd  yes  The command to launch this topology. May be 'storm jar ...' for simple case. A launching script for more complex one.    launching_dir  no  Will  cd  in this folder before launching 'launching_cmd' Must be an absolute path. Default:  ~    wait_time_secs  no  The wait_time in seconds provided to the `kill' command (Delay between spouts deactivation and topology destruction) Default: 30.    timeout_secs  no  Timeout before raising an error value when waiting a target state. Default: Value set by  storm_relay : default_timeout_secs    ranger_policy  no  Definition of Apache Ranger policy bound to this topology. Parameters are same as  storm_ranger_policies  except than  topologies  should not be defined. The policy name can be explicitly defined. Otherwise, a name will be generated as \" _ topology _ \". See example below for more information    no_remove  no  Bolean. Used only for eventual associated ranger policy.  Setting to  yes prevent this policy to be remove when  HADeploy will be used in REMOVE mode. Default:  no .    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/storm/storm_topologies/#asynchronous-launch", 
            "text": "To preserve launching time, HADeploy launch all topologies in a simultaneous way, by using some asynchronous procesing.   This can lead to difficulty to debug, as failing launch des not raise any error, except a timeout after quite long time. More on that  here", 
            "title": "Asynchronous launch"
        }, 
        {
            "location": "/plugins_reference/storm/storm_topologies/#example", 
            "text": "storm_topologies:\n- name: storm1\n  launching_dir: ${basedir1}\n  launching_cmd:  storm jar ./storm1-${storm1_version}-uber.jar com.mydomain.storm1.ClusterTopology \n  wait_time_secs: 10\n- name: storm2\n  launching_dir: ${basedir2}\n  launching_cmd:  ./launch2.sh \n  wait_time_secs: 15  storm_topologies:\n- name: storm2\n  launching_dir: /opt/storm2\n  launching_cmd:  ./launch2.sh \n  wait_time_secs: 10\n  ranger_policy:\n    audit: yes\n    enabled: yes\n    permissions:\n    - users:\n      - stormrunner\n      accesses:\n      - 'fileDownload'\n      - 'killTopology'\n      - 'rebalance'\n      - 'activate'\n      - 'deactivate'\n      - 'getTopologyConf'\n      - 'getTopology'\n      - 'getUserTopology'\n      - 'getTopologyInfo'", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/storm/storm_topologies/#the-launching-cmd-or-script", 
            "text": "Here is some requirement about the launching command or script.    It must ensure setting topology name same as the one provided in this description.    It must be fully synchronous, i.e. not running in the background. But it must exit once topology is started.", 
            "title": "The launching cmd or script"
        }, 
        {
            "location": "/plugins_reference/storm/storm_topologies/#for-kerberos-client_jaasconf", 
            "text": "In a kerboros secured context, the storm command use a configuration file to define how it will authenticate against the Storm server.   For HADeploy usage, this file must be configured to use the current ticket in the local cache. Unfortunately, this is not always the case. Sometime, it may be configured to use another principal and keytab.  If this is the case, we suggest to explicitly provide such a configuration file: For example, in an Hortonworks context, one can write:  storm -c java.security.auth.login.config=/usr/hdp/current/storm-client/conf/client_jaas.conf jar ./myjar.jar com.mydomain.mytopology.ClusterTopology  $1  Here is the content of a  client_jaas.conf  we can use:  StormClient {\n   com.sun.security.auth.module.Krb5LoginModule required\n   useTicketCache=true\n   renewTicket=true\n   serviceName= nimbus ;\n};", 
            "title": "For kerberos: client_jaas.conf"
        }, 
        {
            "location": "/plugins_reference/systemd/systemd_units/", 
            "text": "systemd_units\n\n\nSynopsis\n\n\nProvide a list of \nsystemd\n service units, related to the deployed application.\n\n\nThis will allow to declare node local application processes as system services. All \nsystemctl\n commands (start, stop, status, ...) will then apply to this newly created service.\n\n\nThis module require \nroot\n access on the target host. If you need to deploy local services under the control of a non-privileged user, you can use the \n'supervisor'\n module.\n\n\nThis module will also allow HADeploy to handle start and stop of the defined services.\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nName of the service.\n\n\n\n\n\n\nscope\n\n\nyes\n\n\nOn which target does this file be deployed? May be:\nA single \nhost\n name\nA single \nhost_group\n name\nSeveral \nhosts\n or \nhost_groups\n, separated by the character ':'\n\n\n\n\n\n\nunit_file\n\n\nyes\n\n\nThe unit file, describing how and when this service will be launched. See below\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this service to be removed when HADeploy will be used with \n--action remove\n.\nDefault: \nno\n\n\n\n\n\n\nenabled\n\n\nno\n\n\nIs this service start on system boot ? Default: \nyes\n.\n\n\n\n\n\n\nstate\n\n\nno\n\n\nstarted\n: The service will be started on deployment\nstopped\n: The service will be stopped on deployment.\ncurrent\n: The service state will be left unchanged during deployment. (stopped on initial creation).\nDefault value: \ncurrent\n.\n\n\n\n\n\n\naction_on_notify\n\n\nno\n\n\nAction to be performed when this service is notified of the modification of some file it depend on.\nCan be \nrestart\n, \nreload\n or \nnone\n. Default is \nrestart\n.\nNot all services support \nreload\n.\nSee the \nnotify\n attribute the \nfiles\n module.\n\n\n\n\n\n\n\n\nUnit files\n\n\nA systemd unit file encode information about several system object type, such as services, device, mount point, etc... In our case, only services unit are of interest.\n\n\nA systemd.service unit file will provide all information to handle operation of a service life cycle, such as start, stop, restart on failure, reload, etc.. Full documentation can be found \nhere\n\n\nHADeploy will place the provided unit file in the \n/usr/lib/systemd/system\n folder and trigger appropriate operation to make systemd aware of this newly created (or modified) file.\n\n\nUnit file example\n\n\nThe following is a unit file for a simple service providing a REST interface.\n\n\n[Unit]\nDescription=HARelay (Helper REST server for Hadoop)\n\n[Service]\nType=simple\nUser={{user}}\nGroup={{group}}\nSyslogIdentifier=harelay\nTimeoutSec=15\nRestart=always\nExecStart=/bin/bash /opt/harelay/bin/harelay -c /opt/harelay/etc/config.conf\n\n[Install]\nWantedBy=multi-user.target\n\n\n\n\n\nNote the following for the above example:\n\n\n\n\n\n\nThe \nType=simple\n means the process lauching by \nExecStart\n is the main process and will NOT exit. In other word, it does not 'daemonize' itself.\n\n\n\n\n\n\nThe process will be restarted by systemd in case of unattended exit, in all case (i.e on failure).\n\n\n\n\n\n\nThe process will be executed as a specific user/group, defined by the \n{{user}}\n and \n{{group}}\n variable. It is expected these variables are set in a \nvars\n block in the HADeploy file. And than the unit file is a 'template'.\n\n\n\n\n\n\nExample\n\n\nvars:\n  user: harelay\n  group: harelay\n\nfiles:\n- { scope: edge_nodes, notify: [ \nsystemd://harelay\n ], src: \ntmpl://harelay\n, dest_folder: \n/opt/harelay/bin\n, \n    owner: \n${user}\n, group: \n${group}\n, mode: \n0755\n }\n- { scope: edge_nodes, notify: [ \nsystemd://harelay\n ], src: \ntmpl://config.conf\n, dest_folder: \n/opt/harelay/etc\n, \n    owner: \n${user}\n, group: \n${group}\n, mode: \n0644\n }\n\nsystemd_units:\n- name: harelay\n  scope: edge_nodes\n  unit_file: \ntmpl://harelay.service.j2\n\n\n\n\n\n\nNote the \nnotify\n attribute in the \nfiles\n entries, to trigger a service restart in case of modification of these files.\n\n\nActions \nstop\n, \nstart\n and \nstatus\n\n\nThe \nservices\n plugin introduce three new actions:\n\n\nhadeploy --src ..... --action stop\n\n\n\n\nWill stop all services described by the \nsystemd_units\n list. And \n\n\nhadeploy --src ..... --action start\n\n\n\n\nWill start the same services. While \n\n\nhadeploy --src ..... --action status\n\n\n\n\nwill display current status of the managed services, in a rather primitive form.", 
            "title": "systemd_units"
        }, 
        {
            "location": "/plugins_reference/systemd/systemd_units/#systemd_units", 
            "text": "", 
            "title": "systemd_units"
        }, 
        {
            "location": "/plugins_reference/systemd/systemd_units/#synopsis", 
            "text": "Provide a list of  systemd  service units, related to the deployed application.  This will allow to declare node local application processes as system services. All  systemctl  commands (start, stop, status, ...) will then apply to this newly created service.  This module require  root  access on the target host. If you need to deploy local services under the control of a non-privileged user, you can use the  'supervisor'  module.  This module will also allow HADeploy to handle start and stop of the defined services.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/systemd/systemd_units/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  Name of the service.    scope  yes  On which target does this file be deployed? May be: A single  host  name A single  host_group  name Several  hosts  or  host_groups , separated by the character ':'    unit_file  yes  The unit file, describing how and when this service will be launched. See below    no_remove  no  Boolean: Prevent this service to be removed when HADeploy will be used with  --action remove . Default:  no    enabled  no  Is this service start on system boot ? Default:  yes .    state  no  started : The service will be started on deployment stopped : The service will be stopped on deployment. current : The service state will be left unchanged during deployment. (stopped on initial creation). Default value:  current .    action_on_notify  no  Action to be performed when this service is notified of the modification of some file it depend on. Can be  restart ,  reload  or  none . Default is  restart . Not all services support  reload . See the  notify  attribute the  files  module.", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/systemd/systemd_units/#unit-files", 
            "text": "A systemd unit file encode information about several system object type, such as services, device, mount point, etc... In our case, only services unit are of interest.  A systemd.service unit file will provide all information to handle operation of a service life cycle, such as start, stop, restart on failure, reload, etc.. Full documentation can be found  here  HADeploy will place the provided unit file in the  /usr/lib/systemd/system  folder and trigger appropriate operation to make systemd aware of this newly created (or modified) file.", 
            "title": "Unit files"
        }, 
        {
            "location": "/plugins_reference/systemd/systemd_units/#unit-file-example", 
            "text": "The following is a unit file for a simple service providing a REST interface.  [Unit]\nDescription=HARelay (Helper REST server for Hadoop)\n\n[Service]\nType=simple\nUser={{user}}\nGroup={{group}}\nSyslogIdentifier=harelay\nTimeoutSec=15\nRestart=always\nExecStart=/bin/bash /opt/harelay/bin/harelay -c /opt/harelay/etc/config.conf\n\n[Install]\nWantedBy=multi-user.target  Note the following for the above example:    The  Type=simple  means the process lauching by  ExecStart  is the main process and will NOT exit. In other word, it does not 'daemonize' itself.    The process will be restarted by systemd in case of unattended exit, in all case (i.e on failure).    The process will be executed as a specific user/group, defined by the  {{user}}  and  {{group}}  variable. It is expected these variables are set in a  vars  block in the HADeploy file. And than the unit file is a 'template'.", 
            "title": "Unit file example"
        }, 
        {
            "location": "/plugins_reference/systemd/systemd_units/#example", 
            "text": "vars:\n  user: harelay\n  group: harelay\n\nfiles:\n- { scope: edge_nodes, notify: [  systemd://harelay  ], src:  tmpl://harelay , dest_folder:  /opt/harelay/bin , \n    owner:  ${user} , group:  ${group} , mode:  0755  }\n- { scope: edge_nodes, notify: [  systemd://harelay  ], src:  tmpl://config.conf , dest_folder:  /opt/harelay/etc , \n    owner:  ${user} , group:  ${group} , mode:  0644  }\n\nsystemd_units:\n- name: harelay\n  scope: edge_nodes\n  unit_file:  tmpl://harelay.service.j2   Note the  notify  attribute in the  files  entries, to trigger a service restart in case of modification of these files.", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/systemd/systemd_units/#actions-stop-start-and-status", 
            "text": "The  services  plugin introduce three new actions:  hadeploy --src ..... --action stop  Will stop all services described by the  systemd_units  list. And   hadeploy --src ..... --action start  Will start the same services. While   hadeploy --src ..... --action status  will display current status of the managed services, in a rather primitive form.", 
            "title": "Actions stop, start and status"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_overview/", 
            "text": "Supervisor plugin: Overview\n\n\nSupervisor is a client/server system that allows its users to monitor and control a number of processes on UNIX-like operating systems.\n\n\nIt shares some of the same goals of programs like systemd, but unlike this one, it is not meant to be run as a substitute for init as \u201cprocess id 1\u201d. \nInstead it is meant to be used to control processes related to a project or a customer, and is meant to start like any other program at boot time.\n\n\nA supervisor instance is aimed to control several application programs. In practice, in most case, only one instance per application will be necessary.\n\n\nThis plugin allow both to create a \nsupervisor\n instance and to define all programs this instance will have to manage, by defined  \nsupervisor_programs\n\n\nCreating a \nsupervisor\n instance using this module require \nroot\n access on the target host. \nBut this instance is then delegated to a specific (typically non-priviledged) user (or technical account) which then will be able to perform any admin operation (Add/remove programs, start/stop, ...).\n\n\nThis module will also allow HADeploy to handle start and stop of the associated programs.\n\n\nA simple case\n\n\nLet's begin with a simple example. In this example, we want to deploy a dameon on the two edge nodes of a cluster. \n\n\nThis deamon will run under the \ntsup\n technical account. We also want the supervisor to be manageable throught this account.\n\n\nHere is a main application file, saved as \napp.yml\n\n\nhosts:\n- { name: en1, ssh_host: en1.mycluster.com, ssh_user: root, ssh_private_key_file: keys/root_id }\n- { name: en2, ssh_host: en2.mycluster.com, ssh_user: root, ssh_private_key_file: keys/root_id }\n\nlocal_templates_folders:\n- ./templates\n\nusers:\n- login: tsup\n\nfolders:\n- { scope: all, path: /opt/tsup, owner: tsup, group: tsup,  mode: \n0755\n }\n\nsupervisors:\n- name: tsupv \n  scope: all\n  user: tsup\n  group: tsup\n\nfiles:\n- { scope: all, src: \ntmpl://program1.sh\n, dest_folder: /opt/tsup, owner: tsup, group: tsup, mode: \n0755\n }\n\nsupervisor_programs:\n- name: program1\n  supervisor: tsupv\n  command: \n/bin/bash /opt/tsup/program1.sh\n\n\n\n\n\nIn this file we:\n\n\n\n\n\n\nDescribe the target cluster and how to connect to.\n\n\n\n\n\n\nDefine a local location as our template repository.\n\n\n\n\n\n\nCreate the \ntsup\n technical account.\n\n\n\n\n\n\nCreate a folder \n/opt/tsup\n as an home folder for the application.\n\n\n\n\n\n\nCreate a supervisor instance name \ntsupv\n.\n\n\n\n\n\n\nCopy the \nprogram1.sh\n in the \n/opt/tsup\n folder.\n\n\n\n\n\n\nAnd declare this program to be managed by the supervisor instance we just defined. \n\n\n\n\n\n\nOf course, we will need a test program to manage. Let's write a simple script:\n\n\n#!/bin/bash\nuser=$(id -un)\nprg=program1\nwhile true\ndo \n    echo \n$(date -Iseconds) - - - Hello from ${user}:${prg} to stdout.....\n \n    sleep 2\ndone\n\n\n\n\nAnd save it as \ntemplates/program1.sh\n.\n\n\nOur application is now ready to be deployed. On the deployement node (Called \ndeployer\n here):\n\n\n[hadeployer]$ hadeploy --src app.yml --action deploy\n\n\n\n\nThen, we can log on one edge node and ensure both the daemon and the supervisor run under the \ntsup\n account.\n\n\n[en1]$ pgrep -au tsup\n4392 /usr/bin/python /usr/bin/supervisord -c /etc/supervisord_tsupv.conf\n4621 /bin/bash /opt/tsup/program1.sh\n7983 sleep 2\n\n\n\n\nAnd we can now play with the supervisor command line interface created for our instance (\nsupervisorctl_tsupv\n), under the \ntsup\n account:\n\n\n[en1]$ sudo -u tsup supervisorctl_tsupv\nprogram1                         RUNNING   pid 4621, uptime 0:07:03\nsupervisor\n help\n\ndefault commands (type help \ntopic\n):\n=====================================\nadd    clear  fg        open  quit    remove  restart   start   stop  update\navail  exit   maintail  pid   reload  reread  shutdown  status  tail  version\n\nsupervisor\n status program1\nprogram1                         RUNNING   pid 4621, uptime 0:07:20\nsupervisor\n tail program1\nllo from tsup:program1 to stdout.....\n2017-10-15T08:53:28+0000 - - - Hello from tsup:program1 to stdout.....\n...\n...\n2017-10-15T08:54:09+0000 - - - Hello from tsup:program1 to stdout.....\n2017-10-15T08:54:11+0000 - - - Hello from tsup:program1 to stdout.....\n\nsupervisor\n stop program1\nprogram1: stopped\nsupervisor\n status\nprogram1                         STOPPED   Oct 15 08:54 AM\nsupervisor\n start program1\nprogram1: started\nsupervisor\n status\nprogram1                         RUNNING   pid 8270, uptime 0:00:05\nsupervisor\n\n\n\n\n\nFrom the HADeploy node, we can stop both the program and the supervisor in one command:\n\n\n[hadeployer]$ hadeploy --src app.yml --action stop\n\n\n\n\nEnsure this is no process running under \ntsup\n account on the edge node:\n\n\n[en1]$ pgrep -au tsup\n\n\n\n\nBack to the HADeploy node, restart everything:\n\n\n[hadeployer]$ hadeploy --src app.yml --action start\n\n\n\n\nAn check on the edge node:\n\n\n[en1]$ pgrep -au tsup\n8887 /usr/bin/python /usr/bin/supervisord -c /etc/supervisord_tsupv.conf\n8974 /bin/bash /opt/tsup/program1.sh\n9046 sleep 2\n\n\n\n\nActions \nstop\n, \nstart\n and \nstatus\n\n\nAs just mentioned, the \nSupervisor\n plugin introduce three new actions:\n\n\nhadeploy --src ..... --action stop\n\n\n\n\nWill stop all \nsupervisor_programs\n and then all supervisord daemon described by the \nsupervisors\n list which have the \nmanaged\n flag to \ntrue\n. And: \n\n\nhadeploy --src ..... --action start\n\n\n\n\nWill start the same supervisord daemon, then all \nsupervisor_programs\n. And:\n\n\nhadeploy --src ..... --action status\n\n\n\n\nWill display current status in a rather primitive form.\n\n\nWeb interface\n\n\nEach supervisor instance can expose a web interface for management. We can easily configure it by adding a line to our supervisor definition:\n\n\nsupervisors:\n- name: tsupv \n  scope: all\n  user: tsup\n  group: tsup\n  http_server: { endpoint: \n0.0.0.0:9001\n, username: admin, password: admin }\n\n\n\n\nThen, we just neeed to trigger a new deployment:\n\n\n[hadeployer]$ hadeploy --src app.yml --action deploy\n\n\n\n\nIf we look at the deployment messages, we can see only two tasks where performed:\n\n\nTASK [Setup configuration file /etc/supervisord_tsupv.conf] ****************************************************************************************************\nchanged: [en1]\n\nTASK [Setup supervisord_tsupv.service] *************************************************************************************************************************\nok: [en1]\n\nTASK [Setup /usr/bin/supervisorctl_tsupv] **********************************************************************************************************************\nok: [en1]\n\nRUNNING HANDLER [Restart supervisord_tsupv service] ************************************************************************************************************\nchanged: [en1]\n.....\n.....\nPLAY RECAP *****************************************************************************************************************************************************\nen1                        : ok=19   changed=2    unreachable=0    failed=0\n\n\n\n\n\n\n\n\nThe supervisor configuration file \n/etc/supervisord_tsupv.conf\n has be regenerated, with the http_server configuration.\n\n\n\n\n\n\nThe supervisor \nsupervisord_tsupv\n has been restarted\n\n\n\n\n\n\n\n\nOf course, if we trigger again a deployment no change will be performed anymore.\n \n\n\n\n\nWe can now point a browser to \nhttp://en1.mycluster.com:9001/\n, to see:\n\n\n\n\nOf course, in real life, we will not use 'admin' as a password. And we will not provide it as clear text, but provide its SHA-1 form instead. See \nSupervisors Web interface\n\n\nNotifications: daemon restart\n\n\nLet's say we now want to update our \nprogram1.sh\n.\n\n\nWe can modify it and trigger a new deployment. HADeploy will notice the modification and push the new version on the target hosts. But, the running daemon will be unafected.\n\n\nWe can restart it manually. But, HADeploy provide a mechanisme to automate this. By adding a \nnotify\n attribute to the \nfiles\n definition:\n\n\nfiles:\n- { scope: all, notify: [\nsupervisor://tsupv/program1\n], src: \ntmpl://program1.sh\n, dest_folder: /opt/tsup, owner: tsup, group: tsup, mode: \n0755\n }\n\n\n\n\nThis will instruct HADeploy to restart the program \nprogram1\n of the supervisor instance \ntsupv\n in case of modification of program1.sh\n\n\nTASK [Copy template file 'program1.sh' to '/opt/tsup/program1.sh'] *********************************************************************************************\nchanged: [en1]\n\nTASK [Get current state for supervisor_tsupv_program1 programs] ************************************************************************************************\nok: [en1]\n\nRUNNING HANDLER [Restart supervisor_tsupv program program1] ****************************************************************************************************\nchanged: [en1]\n...\n...\nPLAY RECAP *****************************************************************************************************************************************************\nen1                        : ok=20   changed=2    unreachable=0    failed=0\n\n\n\n\nNon-root deployment\n\n\nCreating a supervisor instance needs root access on the target nodes. \n\n\nRequiring root access for all deployment can be a problem, as it may restrain the number of people able to deploy application. And, an error in the deployment file can corrupt or even destroy the whole target cluster.\n\n\nA solution to this problem would be to split the deployment in two phases:\n\n\n\n\n\n\nAn initial deployment, under root access, will create dedicated folders, namespace, databases, ... thus defining a limited space inside which the application will be deployed. Such space can be called 'box', 'container' or 'corridor'.\n\n\n\n\n\n\nApplication deployment, performed under non-priviledged account (Typically the application technical account) and which will be limited to operate in the space defined previously.\n\n\n\n\n\n\nIn our use case, the supervisor instance creation will be performed during the initial stage, while program1.sh will be deployed under application account.\n\n\nThis will lead to split our application file in two parts:\n\n\nA first file, named \ninit.yml\n:\n\n\nhosts:\n- { name: en1, ssh_host: en1.mycluster.com, ssh_user: root, ssh_private_key_file: keys/root_id }\n- { name: en2, ssh_host: en2.mycluster.com, ssh_user: root, ssh_private_key_file: keys/root_id }\n\nusers:\n- login: tsup\n  authorized_keys:\n  - keys/tsup_id.pub\n\nfolders:\n- { scope: all, path: /opt/tsup, owner: tsup, group: tsup,  mode: \n0755\n }\n\nsupervisors:\n- name: tsupv \n  scope: all\n  user: tsup\n  group: tsup\n  http_server: { endpoint: \n0.0.0.0:9001\n, username: admin, password: admin }\n\n\n\n\nLet's comment it:\n\n\n\n\n\n\nFirst, as before, we access the target node using the \nroot\n credential.\n\n\n\n\n\n\nAs before, we create our application technical account. But we had an \nauthorized_key\n to be able to directly log on under this account. Of course, we have previously generated a key pair.\n\n\n\n\n\n\nThen, we create the application main folder. Note we need to be root to create a folder in \n/opt\n.\n\n\n\n\n\n\nAnd we create the supervisor instance as previously.\n\n\n\n\n\n\nNow, we will create the application deployment part, as \napp.yml\n:\n\n\nhosts:\n- { name: en1, ssh_host: en1.mycluster.com, ssh_user: tsup, ssh_private_key_file: keys/tsup_id }\n- { name: en2, ssh_host: en2.mycluster.com, ssh_user: tsup, ssh_private_key_file: keys/tsup_id }\n\nlocal_templates_folders:\n- ./templates\n\nfiles:\n- { scope: all, notify: \ntsupv:program1\n, src: \ntmpl://program1.sh\n, dest_folder: /opt/tsup, owner: tsup, group: tsup, mode: \n0755\n }\n\nsupervisor_programs:\n- name: program1\n  supervisor: tsupv\n  command: \n/bin/bash /opt/tsup/program1.sh\n\n\nsupervisors:\n- name: tsupv \n  scope: all\n  user: tsup\n  group: tsup\n  http_server: { endpoint: \n0.0.0.0:9001\n, username: admin, password: admin }\n  managed: no\n\n\n\n\nLet's comment it:\n\n\n\n\n\n\nWe now access the target host with the \ntsup\n credential.\n\n\n\n\n\n\nAs before, we define a local location as our template repository.\n\n\n\n\n\n\nAs before, we copy the \nprogram1.sh\n in the \n/opt/tsup\n folder. \n\n\n\n\n\n\nAnd, as before, we declare this program to be managed by the supervisor instance we defined previously.\n\n\n\n\n\n\nBut, there is a problem. To perform this last operation, HADeploy need to know most of the parameters of the supervisor (Mostly its file layout). This is why we need to insert the supervisor definition in the file, here at the end.\n\n\nBut, we just want to describe it. We don't want to manage it (This is the role of the \ninit.yml\n script). This is why we add the \nmanaged: no\n attribute.\n\n\n\n\nDon't be confused. Managing supervisor's program is not managing the supervisor by itself.\n\n\n\n\nYou can now deploy the application container:\n\n\nhadeploy --src init.yml --action deploy\n\n\n\n\nAnd then the application by itself:\n\n\nhadeploy --src app.yml --action deploy\n\n\n\n\nThis pattern provide other benefits:\n\n\n\n\n\n\nApplication (re)deployment is faster, as the number of task is reduced.\n\n\n\n\n\n\nWe have now finer control on application stop and start. \nhadeploy --src app.yml --action stop\n will only stop the application programs, not the supervisor itself. \n\n\n\n\n\n\nRoot key protection\n\n\nBut, how can we prevent a regular (non-root) user to launch the \ninit.yml\n script ?\n\n\nA simple answer is to arrange for the root private key (\nkeys/root_id\n in our case) to be accessible only by the root user (or by a priviledged users group) on the HADeploy node. \nThis is an easy way to bind priviledged access on target cluster to privileged access on the deployment node.\n\n\nNon-root deployment improved\n\n\nIf you look at the previous \ninit.yml\n and \napp.yml\n, you will find some redundancies:\n\n\n\n\n\n\nThe host inventory\n\n\n\n\n\n\nThe supervisor definition.\n\n\n\n\n\n\nSo, we need some refactoring to have something more maintainable:\n\n\nFirst, we isolate our target cluster inventory definition in a separate file \nmycluster.yml\n:\n\n\nhosts:\n- { name: en1, ssh_host: en1.mycluster.com } \n- { name: en2, ssh_host: en2.mycluster.com } \n\n\n\n\nNote than we have remove all ssh connection information.\n\n\nThe, we isolate the supervisor(s) definitions in another file \nsupervisors.yml\n:\n\n\nsupervisors:\n- name: tsupv \n  scope: all\n  user: tsup\n  group: tsup\n  http_server: { endpoint: \n0.0.0.0:9001\n, username: admin, password: admin }\n  managed: ${managing_supervisors}\n\n\n\n\nNote than we can set the \nmanaged\n attribute by a variable.\n\n\nAnd here is the new version of the \ninit.yml\nfile:\n\n\nhost_overrides:\n- name: all\n  ssh_user: root\n  ssh_private_key_file: keys/root_id \n\nusers:\n- login: tsup\n  authorized_keys:\n  - keys/tsup_id.pub\n\nfolders:\n- { scope: all, path: /opt/tsup, owner: tsup, group: tsup,  mode: \n0755\n }\n\nvars:\n  managing_supervisors: yes\n\ninclude: supervisors.yml\n\n\n\n\nAnd the new version of the \napp.yml\n file:\n\n\nhost_overrides:\n- name: all\n  ssh_user: tsup\n  ssh_private_key_file: keys/tsup_id \n\nvars:\n  managing_supervisors: no\n\ninclude: supervisors.yml\n\nlocal_templates_folders:\n- ./templates\n\nfiles:\n- { scope: all, notify: \ntsupv:program1\n, src: \ntmpl://program1.sh\n, dest_folder: /opt/tsup, owner: tsup, group: tsup, mode: \n0755\n }\n\nsupervisor_programs:\n- name: program1\n  supervisor: tsupv\n  command: \n/bin/bash /opt/tsup/program1.sh\n\n\n\n\n\nWe can now proceed with the application container deployment:\n\n\nhadeploy --src mycluster.yml --src init.yml --action deploy\n\n\n\n\nAnd then the application deployment:\n\n\nhadeploy --src mycluster.yml --src app.yml --action deploy\n\n\n\n\nNote than we provide the \nmycluster.yml\n on the command line. An alternative would be to include it in the \ninit.yml\n and the \napp.yml\n. But, if you have several target clusters, this solution is far more flexible.", 
            "title": "Overview"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_overview/#supervisor-plugin-overview", 
            "text": "Supervisor is a client/server system that allows its users to monitor and control a number of processes on UNIX-like operating systems.  It shares some of the same goals of programs like systemd, but unlike this one, it is not meant to be run as a substitute for init as \u201cprocess id 1\u201d. \nInstead it is meant to be used to control processes related to a project or a customer, and is meant to start like any other program at boot time.  A supervisor instance is aimed to control several application programs. In practice, in most case, only one instance per application will be necessary.  This plugin allow both to create a  supervisor  instance and to define all programs this instance will have to manage, by defined   supervisor_programs  Creating a  supervisor  instance using this module require  root  access on the target host. \nBut this instance is then delegated to a specific (typically non-priviledged) user (or technical account) which then will be able to perform any admin operation (Add/remove programs, start/stop, ...).  This module will also allow HADeploy to handle start and stop of the associated programs.", 
            "title": "Supervisor plugin: Overview"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_overview/#a-simple-case", 
            "text": "Let's begin with a simple example. In this example, we want to deploy a dameon on the two edge nodes of a cluster.   This deamon will run under the  tsup  technical account. We also want the supervisor to be manageable throught this account.  Here is a main application file, saved as  app.yml  hosts:\n- { name: en1, ssh_host: en1.mycluster.com, ssh_user: root, ssh_private_key_file: keys/root_id }\n- { name: en2, ssh_host: en2.mycluster.com, ssh_user: root, ssh_private_key_file: keys/root_id }\n\nlocal_templates_folders:\n- ./templates\n\nusers:\n- login: tsup\n\nfolders:\n- { scope: all, path: /opt/tsup, owner: tsup, group: tsup,  mode:  0755  }\n\nsupervisors:\n- name: tsupv \n  scope: all\n  user: tsup\n  group: tsup\n\nfiles:\n- { scope: all, src:  tmpl://program1.sh , dest_folder: /opt/tsup, owner: tsup, group: tsup, mode:  0755  }\n\nsupervisor_programs:\n- name: program1\n  supervisor: tsupv\n  command:  /bin/bash /opt/tsup/program1.sh   In this file we:    Describe the target cluster and how to connect to.    Define a local location as our template repository.    Create the  tsup  technical account.    Create a folder  /opt/tsup  as an home folder for the application.    Create a supervisor instance name  tsupv .    Copy the  program1.sh  in the  /opt/tsup  folder.    And declare this program to be managed by the supervisor instance we just defined.     Of course, we will need a test program to manage. Let's write a simple script:  #!/bin/bash\nuser=$(id -un)\nprg=program1\nwhile true\ndo \n    echo  $(date -Iseconds) - - - Hello from ${user}:${prg} to stdout.....  \n    sleep 2\ndone  And save it as  templates/program1.sh .  Our application is now ready to be deployed. On the deployement node (Called  deployer  here):  [hadeployer]$ hadeploy --src app.yml --action deploy  Then, we can log on one edge node and ensure both the daemon and the supervisor run under the  tsup  account.  [en1]$ pgrep -au tsup\n4392 /usr/bin/python /usr/bin/supervisord -c /etc/supervisord_tsupv.conf\n4621 /bin/bash /opt/tsup/program1.sh\n7983 sleep 2  And we can now play with the supervisor command line interface created for our instance ( supervisorctl_tsupv ), under the  tsup  account:  [en1]$ sudo -u tsup supervisorctl_tsupv\nprogram1                         RUNNING   pid 4621, uptime 0:07:03\nsupervisor  help\n\ndefault commands (type help  topic ):\n=====================================\nadd    clear  fg        open  quit    remove  restart   start   stop  update\navail  exit   maintail  pid   reload  reread  shutdown  status  tail  version\n\nsupervisor  status program1\nprogram1                         RUNNING   pid 4621, uptime 0:07:20\nsupervisor  tail program1\nllo from tsup:program1 to stdout.....\n2017-10-15T08:53:28+0000 - - - Hello from tsup:program1 to stdout.....\n...\n...\n2017-10-15T08:54:09+0000 - - - Hello from tsup:program1 to stdout.....\n2017-10-15T08:54:11+0000 - - - Hello from tsup:program1 to stdout.....\n\nsupervisor  stop program1\nprogram1: stopped\nsupervisor  status\nprogram1                         STOPPED   Oct 15 08:54 AM\nsupervisor  start program1\nprogram1: started\nsupervisor  status\nprogram1                         RUNNING   pid 8270, uptime 0:00:05\nsupervisor   From the HADeploy node, we can stop both the program and the supervisor in one command:  [hadeployer]$ hadeploy --src app.yml --action stop  Ensure this is no process running under  tsup  account on the edge node:  [en1]$ pgrep -au tsup  Back to the HADeploy node, restart everything:  [hadeployer]$ hadeploy --src app.yml --action start  An check on the edge node:  [en1]$ pgrep -au tsup\n8887 /usr/bin/python /usr/bin/supervisord -c /etc/supervisord_tsupv.conf\n8974 /bin/bash /opt/tsup/program1.sh\n9046 sleep 2", 
            "title": "A simple case"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_overview/#actions-stop-start-and-status", 
            "text": "As just mentioned, the  Supervisor  plugin introduce three new actions:  hadeploy --src ..... --action stop  Will stop all  supervisor_programs  and then all supervisord daemon described by the  supervisors  list which have the  managed  flag to  true . And:   hadeploy --src ..... --action start  Will start the same supervisord daemon, then all  supervisor_programs . And:  hadeploy --src ..... --action status  Will display current status in a rather primitive form.", 
            "title": "Actions stop, start and status"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_overview/#web-interface", 
            "text": "Each supervisor instance can expose a web interface for management. We can easily configure it by adding a line to our supervisor definition:  supervisors:\n- name: tsupv \n  scope: all\n  user: tsup\n  group: tsup\n  http_server: { endpoint:  0.0.0.0:9001 , username: admin, password: admin }  Then, we just neeed to trigger a new deployment:  [hadeployer]$ hadeploy --src app.yml --action deploy  If we look at the deployment messages, we can see only two tasks where performed:  TASK [Setup configuration file /etc/supervisord_tsupv.conf] ****************************************************************************************************\nchanged: [en1]\n\nTASK [Setup supervisord_tsupv.service] *************************************************************************************************************************\nok: [en1]\n\nTASK [Setup /usr/bin/supervisorctl_tsupv] **********************************************************************************************************************\nok: [en1]\n\nRUNNING HANDLER [Restart supervisord_tsupv service] ************************************************************************************************************\nchanged: [en1]\n.....\n.....\nPLAY RECAP *****************************************************************************************************************************************************\nen1                        : ok=19   changed=2    unreachable=0    failed=0    The supervisor configuration file  /etc/supervisord_tsupv.conf  has be regenerated, with the http_server configuration.    The supervisor  supervisord_tsupv  has been restarted     Of course, if we trigger again a deployment no change will be performed anymore.     We can now point a browser to  http://en1.mycluster.com:9001/ , to see:   Of course, in real life, we will not use 'admin' as a password. And we will not provide it as clear text, but provide its SHA-1 form instead. See  Supervisors Web interface", 
            "title": "Web interface"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_overview/#notifications-daemon-restart", 
            "text": "Let's say we now want to update our  program1.sh .  We can modify it and trigger a new deployment. HADeploy will notice the modification and push the new version on the target hosts. But, the running daemon will be unafected.  We can restart it manually. But, HADeploy provide a mechanisme to automate this. By adding a  notify  attribute to the  files  definition:  files:\n- { scope: all, notify: [ supervisor://tsupv/program1 ], src:  tmpl://program1.sh , dest_folder: /opt/tsup, owner: tsup, group: tsup, mode:  0755  }  This will instruct HADeploy to restart the program  program1  of the supervisor instance  tsupv  in case of modification of program1.sh  TASK [Copy template file 'program1.sh' to '/opt/tsup/program1.sh'] *********************************************************************************************\nchanged: [en1]\n\nTASK [Get current state for supervisor_tsupv_program1 programs] ************************************************************************************************\nok: [en1]\n\nRUNNING HANDLER [Restart supervisor_tsupv program program1] ****************************************************************************************************\nchanged: [en1]\n...\n...\nPLAY RECAP *****************************************************************************************************************************************************\nen1                        : ok=20   changed=2    unreachable=0    failed=0", 
            "title": "Notifications: daemon restart"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_overview/#non-root-deployment", 
            "text": "Creating a supervisor instance needs root access on the target nodes.   Requiring root access for all deployment can be a problem, as it may restrain the number of people able to deploy application. And, an error in the deployment file can corrupt or even destroy the whole target cluster.  A solution to this problem would be to split the deployment in two phases:    An initial deployment, under root access, will create dedicated folders, namespace, databases, ... thus defining a limited space inside which the application will be deployed. Such space can be called 'box', 'container' or 'corridor'.    Application deployment, performed under non-priviledged account (Typically the application technical account) and which will be limited to operate in the space defined previously.    In our use case, the supervisor instance creation will be performed during the initial stage, while program1.sh will be deployed under application account.  This will lead to split our application file in two parts:  A first file, named  init.yml :  hosts:\n- { name: en1, ssh_host: en1.mycluster.com, ssh_user: root, ssh_private_key_file: keys/root_id }\n- { name: en2, ssh_host: en2.mycluster.com, ssh_user: root, ssh_private_key_file: keys/root_id }\n\nusers:\n- login: tsup\n  authorized_keys:\n  - keys/tsup_id.pub\n\nfolders:\n- { scope: all, path: /opt/tsup, owner: tsup, group: tsup,  mode:  0755  }\n\nsupervisors:\n- name: tsupv \n  scope: all\n  user: tsup\n  group: tsup\n  http_server: { endpoint:  0.0.0.0:9001 , username: admin, password: admin }  Let's comment it:    First, as before, we access the target node using the  root  credential.    As before, we create our application technical account. But we had an  authorized_key  to be able to directly log on under this account. Of course, we have previously generated a key pair.    Then, we create the application main folder. Note we need to be root to create a folder in  /opt .    And we create the supervisor instance as previously.    Now, we will create the application deployment part, as  app.yml :  hosts:\n- { name: en1, ssh_host: en1.mycluster.com, ssh_user: tsup, ssh_private_key_file: keys/tsup_id }\n- { name: en2, ssh_host: en2.mycluster.com, ssh_user: tsup, ssh_private_key_file: keys/tsup_id }\n\nlocal_templates_folders:\n- ./templates\n\nfiles:\n- { scope: all, notify:  tsupv:program1 , src:  tmpl://program1.sh , dest_folder: /opt/tsup, owner: tsup, group: tsup, mode:  0755  }\n\nsupervisor_programs:\n- name: program1\n  supervisor: tsupv\n  command:  /bin/bash /opt/tsup/program1.sh \n\nsupervisors:\n- name: tsupv \n  scope: all\n  user: tsup\n  group: tsup\n  http_server: { endpoint:  0.0.0.0:9001 , username: admin, password: admin }\n  managed: no  Let's comment it:    We now access the target host with the  tsup  credential.    As before, we define a local location as our template repository.    As before, we copy the  program1.sh  in the  /opt/tsup  folder.     And, as before, we declare this program to be managed by the supervisor instance we defined previously.    But, there is a problem. To perform this last operation, HADeploy need to know most of the parameters of the supervisor (Mostly its file layout). This is why we need to insert the supervisor definition in the file, here at the end.  But, we just want to describe it. We don't want to manage it (This is the role of the  init.yml  script). This is why we add the  managed: no  attribute.   Don't be confused. Managing supervisor's program is not managing the supervisor by itself.   You can now deploy the application container:  hadeploy --src init.yml --action deploy  And then the application by itself:  hadeploy --src app.yml --action deploy  This pattern provide other benefits:    Application (re)deployment is faster, as the number of task is reduced.    We have now finer control on application stop and start.  hadeploy --src app.yml --action stop  will only stop the application programs, not the supervisor itself.", 
            "title": "Non-root deployment"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_overview/#root-key-protection", 
            "text": "But, how can we prevent a regular (non-root) user to launch the  init.yml  script ?  A simple answer is to arrange for the root private key ( keys/root_id  in our case) to be accessible only by the root user (or by a priviledged users group) on the HADeploy node. \nThis is an easy way to bind priviledged access on target cluster to privileged access on the deployment node.", 
            "title": "Root key protection"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_overview/#non-root-deployment-improved", 
            "text": "If you look at the previous  init.yml  and  app.yml , you will find some redundancies:    The host inventory    The supervisor definition.    So, we need some refactoring to have something more maintainable:  First, we isolate our target cluster inventory definition in a separate file  mycluster.yml :  hosts:\n- { name: en1, ssh_host: en1.mycluster.com } \n- { name: en2, ssh_host: en2.mycluster.com }   Note than we have remove all ssh connection information.  The, we isolate the supervisor(s) definitions in another file  supervisors.yml :  supervisors:\n- name: tsupv \n  scope: all\n  user: tsup\n  group: tsup\n  http_server: { endpoint:  0.0.0.0:9001 , username: admin, password: admin }\n  managed: ${managing_supervisors}  Note than we can set the  managed  attribute by a variable.  And here is the new version of the  init.yml file:  host_overrides:\n- name: all\n  ssh_user: root\n  ssh_private_key_file: keys/root_id \n\nusers:\n- login: tsup\n  authorized_keys:\n  - keys/tsup_id.pub\n\nfolders:\n- { scope: all, path: /opt/tsup, owner: tsup, group: tsup,  mode:  0755  }\n\nvars:\n  managing_supervisors: yes\n\ninclude: supervisors.yml  And the new version of the  app.yml  file:  host_overrides:\n- name: all\n  ssh_user: tsup\n  ssh_private_key_file: keys/tsup_id \n\nvars:\n  managing_supervisors: no\n\ninclude: supervisors.yml\n\nlocal_templates_folders:\n- ./templates\n\nfiles:\n- { scope: all, notify:  tsupv:program1 , src:  tmpl://program1.sh , dest_folder: /opt/tsup, owner: tsup, group: tsup, mode:  0755  }\n\nsupervisor_programs:\n- name: program1\n  supervisor: tsupv\n  command:  /bin/bash /opt/tsup/program1.sh   We can now proceed with the application container deployment:  hadeploy --src mycluster.yml --src init.yml --action deploy  And then the application deployment:  hadeploy --src mycluster.yml --src app.yml --action deploy  Note than we provide the  mycluster.yml  on the command line. An alternative would be to include it in the  init.yml  and the  app.yml . But, if you have several target clusters, this solution is far more flexible.", 
            "title": "Non-root deployment improved"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisors/", 
            "text": "supervisors\n\n\nSynopsis\n\n\nProvide a list of \nsupervisor\n instances\n\n\nThis is a reference part. Refer to the associated \noverview\n for a more synthetical view.\n\n\nAttributes\n\n\nEach item of the list has the following attribute: \n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nName of the supervisor.\n\n\n\n\n\n\nscope\n\n\nyes\n\n\nOn which target does this supervisor be deployed? May be:\nA single \nhost\n name\nA single \nhost_group\n name\nSeveral \nhosts\n or \nhost_groups\n, separated by the character ':'\n\n\n\n\n\n\nuser\n\n\nyes\n\n\nThe owner of this supervisor. This account will be allowed to fully control all associated \nsupervisor_programs\n.\n\n\n\n\n\n\ngroup\n\n\nyes\n\n\nThe group of the owner.\n\n\n\n\n\n\nmanaged\n\n\nno\n\n\nBoolean. Did this supervisor under the control of HADeploy ? If \nno\n, it will only act as a description to be referenced by \nsupervisor_programs\nYou will find more information on what this attribute is intended for in \nSupervisor overview / Non-root deployment\nDefault: \nyes\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this supervisor to be removed when HADeploy will be used with \n--action remove\n.\nDefault: \nno\n\n\n\n\n\n\nenabled\n\n\nno\n\n\nIs this supervisord instance daemon start on system boot ? Default: \nyes\n.\n\n\n\n\n\n\nstate\n\n\nno\n\n\nstarted\n: The supervisors instance daemon will be started on deployment\nstopped\n: The supervisord instance daemon will be stopped on deployment.\ncurrent\n: The supervisord instance daemon state will be left unchanged during deployment. (stopped on initial creation).\nDefault value: \nstarted\n.\n\n\n\n\n\n\nhttp_server\n\n\nno\n\n\nAllow a management HTTP server to be exposed. See \nWeb Interface\n below to see how to activate and configure it.\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nBased on these parameters, HADeploy will instanciate a specific \nsupervisord\n daemon, with the following characteristics:\n\n\n\n\n\n\nThe deamon is a systemd service of name \nsupervisord_\nname\n.service\n.\n\n\n\n\n\n\nIts associated supervisorctl client is named \nsupervisorctl_\nname\n\n\n\n\n\n\nAs a general rules, all associated files and folders layout are the same than the original supervisor, except postfixed by \n_\nname\n. \n(See the default value of the supplementary attributes below).\n\n\n\n\n\n\nIf you need to modify this layout, you can use the following supplementary attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nconf_file_dst\n\n\nno\n\n\nDefine the location of the supervisor instance configuration file. Default is \n/etc/supervisord_\nname\n.conf\n\n\n\n\n\n\nlogs_dir\n\n\nno\n\n\nDefine the location of the supervisor instance logs folder. Default is \n/var/log/supervisor_\nname\n/\n\n\n\n\n\n\npid_dir\n\n\nno\n\n\nDefine the location of the supervisor instance pid folder. Default is \n/var/run/supervisor_\nname\n/\n\n\n\n\n\n\nsocks_dir\n\n\nno\n\n\nCommunication between supervisord daemon and the supervisorctl client use a local UNIX domain socket file located in this folder. Default is \n/var/run/supervisor_\nname\n\n\n\n\n\n\nsupervisorctl\n\n\nno\n\n\nFor each supervisor instance, HADeploy will generate a shortcut for the command line management tool, build from the supervisor name.  Default is \n/usr/bin/supervisorctl_\nname\n\n\n\n\n\n\nconf_file_src\n\n\nno\n\n\nAll supervisor instance parameters are stored in a configuration file. Such file is generated by HADeploy from a template, populated with attributes described in this page. If you need deeper control, you can provide our own template. See \nProvided configuration file\n below.\n\n\n\n\n\n\n\n\nWeb interface\n\n\nEach supervisor instance can expose a small admin Web interface like this:\n\n\n\nTo activate this interface, you need to define the attribute \nhttp_server\n as a map with the following sub-attribute:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nendpoint\n\n\nno\n\n\nThe bind address and port of this web server. Default is \n127.0.0.0:9001\n (Can be only acceded locally, on port 9001). If you want this web server to be accesible remotly, you must modify this value. For example, \n0.0.0.0:9001\n will bind on all interfaces, on port 9001.\n\n\n\n\n\n\nusername\n\n\nno\n\n\nUsed to force a login to protect this interface. Default is open interface if no password is provided, or the supervisor's user if a password is provided.\n\n\n\n\n\n\npassword\n\n\nno\n\n\nUsed to force a login to protect this interface. Can be a clear text password (to be avoided!) or an SHA-1 value. See below.\n\n\n\n\n\n\n\n\n\n\nOne limit of this interface is it manage only a single server. If you have a cluster of nodes with installed supervisors, there is several third party products which will provide you with centralized view of your application. \nCheck this \nlink\n\n\n\n\nPassword\n\n\nAn obvious good practice is to not put clear text password in configuration file. Fortunatly, supervisor allow to provide the password as its SHA-1 hash value, prefixed by \n{SHA}\n.\n\n\nFor example, \n{SHA}82ab876d1387bfafe46cc1c8a2ef074eae50cb1d\n is the SHA-stored version of the password \nthepassword\n.\n\n\nHere is a trick to generate such password. Just enter:\n\n\npython -c 'import hashlib; print \n{SHA}\n + hashlib.sha1(\nthepassword\n).hexdigest()'\n\n\n\n\nExample\n\n\nHere is a typical supervisor definition, with an HTTP server exposed on port 9003.\n\n\nsupervisors:\n- name: tech1 \n  scope: all\n  user: tsup\n  group: tsup\n  http_server: \n    endpoint: 0.0.0.0:9003\n    username: admin\n    password: \n{SHA}d033e22ae348aeb5660fc2140aec35850c4da997\n\n\n\n\n\nOnce logged on the target node \nunder the \ntsup\n account\n, you can launch the client command line interface:\n\n\n[tsup@dg17 ~]$ supervisorctl_tech1\nprogram1                         STOPPED   Not started\nprogram2                         RUNNING   pid 10623, uptime 0:33:30\nprogram3                         RUNNING   pid 23687, uptime 21:09:50\nsupervisor\n help\n\ndefault commands (type help \ntopic\n):\n=====================================\nadd    clear  fg        open  quit    remove  restart   start   stop  update\navail  exit   maintail  pid   reload  reread  shutdown  status  tail  version\n\n\n\n\nOf course, here there is already some programs launched. See \nsupervisor_programs\n\n\nHere is another example where one want to relocate all related file in a specific folder:\n\n\nfolders:\n- { scope: all, path: /opt/tech2, owner: tsup, group: tsup, mode: \n0755\n } \n\nsupervisors:\n- name: tech2\n  scope: all\n  user: tsup\n  group: tsup\n  conf_file_dst: /opt/tech2/supervisord.conf\n  logs_dir: /opt/tech2/logs\n  pid_dir: /opt/tech2/run\n  socks_dir: /opt/tech2/pid\n  include_dir: /opt/tech2/supervisord.d\n  supervisorctl: /opt/tech2/supervisorctl\n\n\n\n\nThen, the command line interface will be launched using:\n\n\n[tsup@dg17 ~]$ /opt/tech2/supervisorctl\nprogram4                         RUNNING   pid 23701, uptime 21:17:01\n\n\n\n\nSystem instance\n\n\nWhen installing its own instance of the supervisor program, HADeploy will first install the \nsupervisor\n system package. This will create a first, system default, instance.\n\n\nBy default, this instance is not activated:\n\n\n[root@dg17:~]# systemctl status supervisord\n\u25cf supervisord.service - Process Monitoring and Control Daemon\n   Loaded: loaded (/usr/lib/systemd/system/supervisord.service; disabled; vendor preset: disabled)\n   Active: inactive (dead)\n\n\n\n\nIt is up to the user to activate it if needed. But keep in mind this instance is under \nroot\n control, and as such can't be managed by non-priviledged users. \n\n\nProvided configuration file.\n\n\n\n\nThis is Advanded configuration.\n\n\n\n\nAt the hearth of each instance of supervisor is a main configuration file, used by both the \nsupervisord_\nname\n daemon and the \nsupervisorctl_\nname\n command line interface.\n\n\nSuch file is generated by HADeploy by populating a template from the values provided as attributes.\n\n\nIn most case this will be sufficient. But if, for some specifics cases, you need to modify other parameters of this configuration file, you can provide your own template.\n\n\nOf course, you should be familiar with the \noriginal supervisor configuration syntax\n.\n\n\nThen, you can provide your own file using the \nconf_file_sr\n attribute, by providing it as a file, or as a template, such as:\n\n\nsupervisors:\n- name: tech2\n  ...\n  conf_file_src: tmpl://supervisord_tech2.conf.jj2\n  ...\n\n\n\n\nThis will be handled as usual files or template ressource (\nSee here\n)\n\n\n\n\nNB: Only \nfile://\n or \ntmpl://\n schema are allowed here.\n\n\n\n\nAs a starting point, here is the provided template:\n\n\n;  supervisor_{{{name}}}.conf (HADeploy generated) \n\n[unix_http_server]\nfile={{{socks_dir}}}/supervisor.sock   ; (the path to the socket file)\nchmod=0700                 ; sockef file mode (default 0700)\nchown={{{user}}}           ; socket file uid:gid owner\nchgrp={{{group}}}\n;username=user              ; (default is no username (open server))\n;password=123               ; (default is no password (open server))\n\n{{% if http_server is defined %}}\n[inet_http_server]         ; inet (TCP) server disabled by default\nport={{{ http_server.endpoint }}}        ; (ip_address:port specifier, *:port for all iface)\n{{% if http_server.password is defined %}}\nusername={{{ http_server.username }}}              ; (default is no username (open server))\npassword={{{ http_server.password }}}               ; (default is no password (open server))\n{{% endif %}}\n{{% endif %}}\n\n[supervisord]\nlogfile={{{logs_dir}}}/supervisord.log  ; (main log file;default $CWD/supervisord.log)\nlogfile_maxbytes=50MB       ; (max main logfile bytes b4 rotation;default 50MB)\nlogfile_backups=10          ; (num of main logfile rotation backups;default 10)\nloglevel=debug               ; (log level;default info; others: debug,warn,trace)\npidfile={{{pid_dir}}}/supervisord.pid ; (supervisord pidfile;default supervisord.pid)\nnodaemon=false              ; (start in foreground if true;default false)\nminfds=1024                 ; (min. avail startup file descriptors;default 1024)\nminprocs=200                ; (min. avail process descriptors;default 200)\numask=022                  ; (process file creation umask;default 022)\nuser={{{user}}}                 ; (default is current user, required if root)\nidentifier=supervisor_{{{name}}}       ; (supervisord identifier, default is 'supervisor')\n;directory=/tmp              ; (default is not to cd during start)\n;nocleanup=true              ; (don't clean up tempfiles at start;default false)\n;childlogdir=/tmp            ; ('AUTO' child log dir, default $TEMP)\n;environment=KEY=value       ; (key value pairs to add to environment)\n;strip_ansi=false            ; (strip ansi escape codes in logs; def. false)\n\n; the below section must remain in the config file for RPC\n; (supervisorctl/web interface) to work, additional interfaces may be\n; added by defining them in separate rpcinterface: sections\n[rpcinterface:supervisor]\nsupervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface\n\n[supervisorctl]\nserverurl=unix://{{{socks_dir}}}/supervisor.sock ; use a unix:// URL  for a unix socket\n;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket\n;username=chris              ; should be same as http_username if set\n;password=123                ; should be same as http_password if set\n;prompt=mysupervisor         ; cmd line prompt (default \nsupervisor\n)\n;history_file=~/.sc_history  ; use readline history if available\n\n\n[include]\nfiles = {{{include_dir}}}/*.ini\n\n\n\n\nYou can see than variable hosting configuration attribute are of the form \n{{{...}}}\n. If you want to use your own variable (Set in the \nvars:\nblock), you must use the \n{{...}}\nnotation.\nMore information on variable syntax in \nUnder the  hood\n\n\nYou should note also there is no part about the managed programs themself. (No [program:x] section). Such part will be managed by \n'supervisor_programs`\n, by adding files in the include folder defined at the end.\n\n\nAlso, another better starting point if you plan to provide you own template would be to use the one corresponding to you version of HADeploy. \nYou can easely grab it from \nGithub\n, by setting the tag corresponding to your version.", 
            "title": "supervisors"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisors/#supervisors", 
            "text": "", 
            "title": "supervisors"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisors/#synopsis", 
            "text": "Provide a list of  supervisor  instances  This is a reference part. Refer to the associated  overview  for a more synthetical view.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisors/#attributes", 
            "text": "Each item of the list has the following attribute:      Name  req?  Description      name  yes  Name of the supervisor.    scope  yes  On which target does this supervisor be deployed? May be: A single  host  name A single  host_group  name Several  hosts  or  host_groups , separated by the character ':'    user  yes  The owner of this supervisor. This account will be allowed to fully control all associated  supervisor_programs .    group  yes  The group of the owner.    managed  no  Boolean. Did this supervisor under the control of HADeploy ? If  no , it will only act as a description to be referenced by  supervisor_programs You will find more information on what this attribute is intended for in  Supervisor overview / Non-root deployment Default:  yes    no_remove  no  Boolean: Prevent this supervisor to be removed when HADeploy will be used with  --action remove . Default:  no    enabled  no  Is this supervisord instance daemon start on system boot ? Default:  yes .    state  no  started : The supervisors instance daemon will be started on deployment stopped : The supervisord instance daemon will be stopped on deployment. current : The supervisord instance daemon state will be left unchanged during deployment. (stopped on initial creation). Default value:  started .    http_server  no  Allow a management HTTP server to be exposed. See  Web Interface  below to see how to activate and configure it.    when  no  Boolean. Allow  conditional deployment  of this item. Default  True     Based on these parameters, HADeploy will instanciate a specific  supervisord  daemon, with the following characteristics:    The deamon is a systemd service of name  supervisord_ name .service .    Its associated supervisorctl client is named  supervisorctl_ name    As a general rules, all associated files and folders layout are the same than the original supervisor, except postfixed by  _ name . \n(See the default value of the supplementary attributes below).    If you need to modify this layout, you can use the following supplementary attributes:     Name  req?  Description      conf_file_dst  no  Define the location of the supervisor instance configuration file. Default is  /etc/supervisord_ name .conf    logs_dir  no  Define the location of the supervisor instance logs folder. Default is  /var/log/supervisor_ name /    pid_dir  no  Define the location of the supervisor instance pid folder. Default is  /var/run/supervisor_ name /    socks_dir  no  Communication between supervisord daemon and the supervisorctl client use a local UNIX domain socket file located in this folder. Default is  /var/run/supervisor_ name    supervisorctl  no  For each supervisor instance, HADeploy will generate a shortcut for the command line management tool, build from the supervisor name.  Default is  /usr/bin/supervisorctl_ name    conf_file_src  no  All supervisor instance parameters are stored in a configuration file. Such file is generated by HADeploy from a template, populated with attributes described in this page. If you need deeper control, you can provide our own template. See  Provided configuration file  below.", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisors/#web-interface", 
            "text": "Each supervisor instance can expose a small admin Web interface like this:  To activate this interface, you need to define the attribute  http_server  as a map with the following sub-attribute:     Name  req?  Description      endpoint  no  The bind address and port of this web server. Default is  127.0.0.0:9001  (Can be only acceded locally, on port 9001). If you want this web server to be accesible remotly, you must modify this value. For example,  0.0.0.0:9001  will bind on all interfaces, on port 9001.    username  no  Used to force a login to protect this interface. Default is open interface if no password is provided, or the supervisor's user if a password is provided.    password  no  Used to force a login to protect this interface. Can be a clear text password (to be avoided!) or an SHA-1 value. See below.      One limit of this interface is it manage only a single server. If you have a cluster of nodes with installed supervisors, there is several third party products which will provide you with centralized view of your application. \nCheck this  link", 
            "title": "Web interface"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisors/#password", 
            "text": "An obvious good practice is to not put clear text password in configuration file. Fortunatly, supervisor allow to provide the password as its SHA-1 hash value, prefixed by  {SHA} .  For example,  {SHA}82ab876d1387bfafe46cc1c8a2ef074eae50cb1d  is the SHA-stored version of the password  thepassword .  Here is a trick to generate such password. Just enter:  python -c 'import hashlib; print  {SHA}  + hashlib.sha1( thepassword ).hexdigest()'", 
            "title": "Password"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisors/#example", 
            "text": "Here is a typical supervisor definition, with an HTTP server exposed on port 9003.  supervisors:\n- name: tech1 \n  scope: all\n  user: tsup\n  group: tsup\n  http_server: \n    endpoint: 0.0.0.0:9003\n    username: admin\n    password:  {SHA}d033e22ae348aeb5660fc2140aec35850c4da997   Once logged on the target node  under the  tsup  account , you can launch the client command line interface:  [tsup@dg17 ~]$ supervisorctl_tech1\nprogram1                         STOPPED   Not started\nprogram2                         RUNNING   pid 10623, uptime 0:33:30\nprogram3                         RUNNING   pid 23687, uptime 21:09:50\nsupervisor  help\n\ndefault commands (type help  topic ):\n=====================================\nadd    clear  fg        open  quit    remove  restart   start   stop  update\navail  exit   maintail  pid   reload  reread  shutdown  status  tail  version  Of course, here there is already some programs launched. See  supervisor_programs  Here is another example where one want to relocate all related file in a specific folder:  folders:\n- { scope: all, path: /opt/tech2, owner: tsup, group: tsup, mode:  0755  } \n\nsupervisors:\n- name: tech2\n  scope: all\n  user: tsup\n  group: tsup\n  conf_file_dst: /opt/tech2/supervisord.conf\n  logs_dir: /opt/tech2/logs\n  pid_dir: /opt/tech2/run\n  socks_dir: /opt/tech2/pid\n  include_dir: /opt/tech2/supervisord.d\n  supervisorctl: /opt/tech2/supervisorctl  Then, the command line interface will be launched using:  [tsup@dg17 ~]$ /opt/tech2/supervisorctl\nprogram4                         RUNNING   pid 23701, uptime 21:17:01", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisors/#system-instance", 
            "text": "When installing its own instance of the supervisor program, HADeploy will first install the  supervisor  system package. This will create a first, system default, instance.  By default, this instance is not activated:  [root@dg17:~]# systemctl status supervisord\n\u25cf supervisord.service - Process Monitoring and Control Daemon\n   Loaded: loaded (/usr/lib/systemd/system/supervisord.service; disabled; vendor preset: disabled)\n   Active: inactive (dead)  It is up to the user to activate it if needed. But keep in mind this instance is under  root  control, and as such can't be managed by non-priviledged users.", 
            "title": "System instance"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisors/#provided-configuration-file", 
            "text": "This is Advanded configuration.   At the hearth of each instance of supervisor is a main configuration file, used by both the  supervisord_ name  daemon and the  supervisorctl_ name  command line interface.  Such file is generated by HADeploy by populating a template from the values provided as attributes.  In most case this will be sufficient. But if, for some specifics cases, you need to modify other parameters of this configuration file, you can provide your own template.  Of course, you should be familiar with the  original supervisor configuration syntax .  Then, you can provide your own file using the  conf_file_sr  attribute, by providing it as a file, or as a template, such as:  supervisors:\n- name: tech2\n  ...\n  conf_file_src: tmpl://supervisord_tech2.conf.jj2\n  ...  This will be handled as usual files or template ressource ( See here )   NB: Only  file://  or  tmpl://  schema are allowed here.   As a starting point, here is the provided template:  ;  supervisor_{{{name}}}.conf (HADeploy generated) \n\n[unix_http_server]\nfile={{{socks_dir}}}/supervisor.sock   ; (the path to the socket file)\nchmod=0700                 ; sockef file mode (default 0700)\nchown={{{user}}}           ; socket file uid:gid owner\nchgrp={{{group}}}\n;username=user              ; (default is no username (open server))\n;password=123               ; (default is no password (open server))\n\n{{% if http_server is defined %}}\n[inet_http_server]         ; inet (TCP) server disabled by default\nport={{{ http_server.endpoint }}}        ; (ip_address:port specifier, *:port for all iface)\n{{% if http_server.password is defined %}}\nusername={{{ http_server.username }}}              ; (default is no username (open server))\npassword={{{ http_server.password }}}               ; (default is no password (open server))\n{{% endif %}}\n{{% endif %}}\n\n[supervisord]\nlogfile={{{logs_dir}}}/supervisord.log  ; (main log file;default $CWD/supervisord.log)\nlogfile_maxbytes=50MB       ; (max main logfile bytes b4 rotation;default 50MB)\nlogfile_backups=10          ; (num of main logfile rotation backups;default 10)\nloglevel=debug               ; (log level;default info; others: debug,warn,trace)\npidfile={{{pid_dir}}}/supervisord.pid ; (supervisord pidfile;default supervisord.pid)\nnodaemon=false              ; (start in foreground if true;default false)\nminfds=1024                 ; (min. avail startup file descriptors;default 1024)\nminprocs=200                ; (min. avail process descriptors;default 200)\numask=022                  ; (process file creation umask;default 022)\nuser={{{user}}}                 ; (default is current user, required if root)\nidentifier=supervisor_{{{name}}}       ; (supervisord identifier, default is 'supervisor')\n;directory=/tmp              ; (default is not to cd during start)\n;nocleanup=true              ; (don't clean up tempfiles at start;default false)\n;childlogdir=/tmp            ; ('AUTO' child log dir, default $TEMP)\n;environment=KEY=value       ; (key value pairs to add to environment)\n;strip_ansi=false            ; (strip ansi escape codes in logs; def. false)\n\n; the below section must remain in the config file for RPC\n; (supervisorctl/web interface) to work, additional interfaces may be\n; added by defining them in separate rpcinterface: sections\n[rpcinterface:supervisor]\nsupervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface\n\n[supervisorctl]\nserverurl=unix://{{{socks_dir}}}/supervisor.sock ; use a unix:// URL  for a unix socket\n;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket\n;username=chris              ; should be same as http_username if set\n;password=123                ; should be same as http_password if set\n;prompt=mysupervisor         ; cmd line prompt (default  supervisor )\n;history_file=~/.sc_history  ; use readline history if available\n\n\n[include]\nfiles = {{{include_dir}}}/*.ini  You can see than variable hosting configuration attribute are of the form  {{{...}}} . If you want to use your own variable (Set in the  vars: block), you must use the  {{...}} notation.\nMore information on variable syntax in  Under the  hood  You should note also there is no part about the managed programs themself. (No [program:x] section). Such part will be managed by  'supervisor_programs` , by adding files in the include folder defined at the end.  Also, another better starting point if you plan to provide you own template would be to use the one corresponding to you version of HADeploy. \nYou can easely grab it from  Github , by setting the tag corresponding to your version.", 
            "title": "Provided configuration file."
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_groups/", 
            "text": "supervisor_groups\n\n\nSynopsis\n\n\nIt is often useful to group programs together into a process group so they can be controlled as a unit from Supervisor\u2019s various controller interfaces, such as supervisorctl.\n\n\nRefer to \nsupervisor documentation\n for more information\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsupervisor\n\n\nyes\n\n\nThe supervisor managing this group\n\n\n\n\n\n\nname\n\n\nyes\n\n\nName of the group.\n\n\n\n\n\n\nprograms\n\n\nyes\n\n\nThe list of program belonging tho this group.\n\n\n\n\n\n\npriority\n\n\nno\n\n\nThe priority assigned to this group. Refer to \nsupervisor documentation\n.\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this program to be removed when HADeploy will be used with \n--action remove\n.\nDefault: \nno\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nExample\n\n\nsupervisor_groups:\n- supervisor: tech1\n  name: grp1\n  programs:\n  - program1\n  - program2", 
            "title": "supervisor_groups"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_groups/#supervisor_groups", 
            "text": "", 
            "title": "supervisor_groups"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_groups/#synopsis", 
            "text": "It is often useful to group programs together into a process group so they can be controlled as a unit from Supervisor\u2019s various controller interfaces, such as supervisorctl.  Refer to  supervisor documentation  for more information", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_groups/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      supervisor  yes  The supervisor managing this group    name  yes  Name of the group.    programs  yes  The list of program belonging tho this group.    priority  no  The priority assigned to this group. Refer to  supervisor documentation .    no_remove  no  Boolean: Prevent this program to be removed when HADeploy will be used with  --action remove . Default:  no    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_groups/#example", 
            "text": "supervisor_groups:\n- supervisor: tech1\n  name: grp1\n  programs:\n  - program1\n  - program2", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_programs/", 
            "text": "supervisor_programs\n\n\nSynopsis\n\n\nProvide a list of program managed by one (or several) \nsupervisors\n instances.\n\n\nThis is a reference part. Refer to the associated \noverview\n for a more synthetical view.\n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsupervisor\n\n\nyes\n\n\nThe supervisor managing this program\n\n\n\n\n\n\nname\n\n\nyes\n\n\nName of the program.\n\n\n\n\n\n\nscope\n\n\nno\n\n\nOn which target does this supervisor's program be deployed? May be:\nA single \nhost\n name\nA single \nhost_group\n name\nSeveral \nhosts\n or \nhost_groups\n, separated by the character ':'\nDefault to the supervisor's scope\n\n\n\n\n\n\ncommand\n\n\nyes if \nconf_file_src\n is not provided\n\n\nThe command to launch. Refer to the \nsupervisor documentation\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this program to be removed when HADeploy will be used with \n--action remove\n.\nDefault: \nno\n\n\n\n\n\n\nuser\n\n\nno\n\n\nFrom the \nsupervisor documentation\n: Instruct supervisord to use this UNIX user account as the account which runs the program. The user can only be switched if supervisord is run as the root user. If supervisord can\u2019t switch to the specified user, the program will not be started.\nDefault is the supervisord's user.\n\n\n\n\n\n\nstate\n\n\nno\n\n\nstarted\n: The program will be started on deployment\nstopped\n: The program will be stopped on deployment.\ncurrent\n: The program state will be left unchanged during deployment. (stopped on initial creation).\nDefault value: \nstarted\n.\n\n\n\n\n\n\nautostart\n\n\nno\n\n\nBoolean. If true, this program will start automatically when supervisord is started. Refer to the \nsupervisor documentation\n\n\n\n\n\n\nprocess_name\n\n\nno\n\n\nRefer to the \nsupervisor documentation\n\n\n\n\n\n\nnumprocs\n\n\nno\n\n\nInteger. Refer to the \nsupervisor documentation\n\n\n\n\n\n\nnumprocs_start\n\n\nno\n\n\nInteger. Refer to the \nsupervisor documentation\n\n\n\n\n\n\npriority\n\n\nno\n\n\nInteger. Refer to the \nsupervisor documentation\n\n\n\n\n\n\nstartsecs\n\n\nno\n\n\nInteger. Refer to the \nsupervisor documentation\n\n\n\n\n\n\nstartretries\n\n\nno\n\n\nInteger. Refer to the \nsupervisor documentation\n\n\n\n\n\n\nautorestart\n\n\nno\n\n\nRefer to the \nsupervisor documentation\n\n\n\n\n\n\nexitcodes\n\n\nno\n\n\nList of Integer. Refer to the \nsupervisor documentation\n\n\n\n\n\n\nstopsignal\n\n\nno\n\n\nRefer to the \nsupervisor documentation\n\n\n\n\n\n\nstopwaitsecs\n\n\nno\n\n\nInteger. Refer to the \nsupervisor documentation\n\n\n\n\n\n\nstopasgroup\n\n\nno\n\n\nBoolean. Refer to the \nsupervisor documentation\n. May be required if your program fork another one.\n\n\n\n\n\n\nkillasgroup\n\n\nno\n\n\nBoolean. Refer to the \nsupervisor documentation\n\n\n\n\n\n\nredirect_stderr\n\n\nno\n\n\nBoolan. Refer to the \nsupervisor documentation\n\n\n\n\n\n\nstdout_logfile\n\n\nno\n\n\nRefer to the \nsupervisor documentation\n\n\n\n\n\n\nstdout_logfile_maxbytes\n\n\nno\n\n\nRefer to the \nsupervisor documentation\n\n\n\n\n\n\nstdout_logfile_backups\n\n\nno\n\n\nInteger. Refer to the \nsupervisor documentation\n\n\n\n\n\n\nstderr_logfile\n\n\nno\n\n\nRefer to the \nsupervisor documentation\n\n\n\n\n\n\nstderr_logfile_maxbytes\n\n\nno\n\n\nRefer to the \nsupervisor documentation\n\n\n\n\n\n\nstderr_logfile_backups\n\n\nno\n\n\nInteger. Refer to the \nsupervisor documentation\n\n\n\n\n\n\nenvironment\n\n\nno\n\n\nList of String. Refer to the \nsupervisor documentation\n\n\n\n\n\n\ndirectory\n\n\nno\n\n\nRefer to the \nsupervisor documentation\n\n\n\n\n\n\numask\n\n\nno\n\n\nRefer to the \nsupervisor documentation\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nExample\n\n\nsupervisor_programs:\n- name: program1\n  supervisor: tech1\n  command: \n/bin/bash /opt/tsupervisor/program1.sh\n\n\n\n\n\nThis snippet create a program 'program1' running as a daemon. This program being a simple shell script (\nprogram1.sh\n) and will be managed by the \nsupervisor named tech1\n.\n\n\nThis program can then be managed (stopped/restarted/...) by:\n\n\n\n\n\n\nThe Web interface provided by the supervisor, if any.\n\n\n\n\n\n\nThe command line interface \nsupervisorctl_tech1\n\n\n\n\n\n\nHADeploy using \n--action start\n and \n--action stop\n.\n\n\n\n\n\n\nAnother example, with more parameters:\n\n\nsupervisor_programs:\n- name: program2\n  supervisor: tech1\n  command: \n/bin/bash /opt/tsupervisor/program2.sh\n\n  stopasgroup: yes\n  environment:\n  - MY_HOME=\n/opt/program2\n\n  - JAVA_HOME=\n/usr/java/jdk1.8.0_74\n\n  directory: /opt/program2/work\n  stdout_logfile: /opt/program2/logs/program2.out\n  stderr_logfile: /opt/program2/logs/program2.err\n\n\n\n\nAbout managed programs\n\n\n\n\n\n\nPrograms meant to be run under supervisor should not daemonize themselves\n. Instead, they should run in the foreground. They should not detach from the terminal from which they are started.\nMore info \nhere\n\n\n\n\n\n\nIf the launched programs fork one or several child processes, there may be a problem on stop. Only the initial process will be killed by supervisor, and all children will become orphan. \nThere will be two solutions for this:\n\n\n\n\n\n\nCatch the \nstopsignal\n and then kill all child processes.\n\n\n\n\n\n\nSet the \nstopasgroup\n flag\n\n\n\n\n\n\nThis last solution is of course the simplest. \n\n\nA typical case encountering this issue is a java programs launched by a wrapper script.", 
            "title": "supervisor_programs"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_programs/#supervisor_programs", 
            "text": "", 
            "title": "supervisor_programs"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_programs/#synopsis", 
            "text": "Provide a list of program managed by one (or several)  supervisors  instances.  This is a reference part. Refer to the associated  overview  for a more synthetical view.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_programs/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      supervisor  yes  The supervisor managing this program    name  yes  Name of the program.    scope  no  On which target does this supervisor's program be deployed? May be: A single  host  name A single  host_group  name Several  hosts  or  host_groups , separated by the character ':' Default to the supervisor's scope    command  yes if  conf_file_src  is not provided  The command to launch. Refer to the  supervisor documentation    no_remove  no  Boolean: Prevent this program to be removed when HADeploy will be used with  --action remove . Default:  no    user  no  From the  supervisor documentation : Instruct supervisord to use this UNIX user account as the account which runs the program. The user can only be switched if supervisord is run as the root user. If supervisord can\u2019t switch to the specified user, the program will not be started. Default is the supervisord's user.    state  no  started : The program will be started on deployment stopped : The program will be stopped on deployment. current : The program state will be left unchanged during deployment. (stopped on initial creation). Default value:  started .    autostart  no  Boolean. If true, this program will start automatically when supervisord is started. Refer to the  supervisor documentation    process_name  no  Refer to the  supervisor documentation    numprocs  no  Integer. Refer to the  supervisor documentation    numprocs_start  no  Integer. Refer to the  supervisor documentation    priority  no  Integer. Refer to the  supervisor documentation    startsecs  no  Integer. Refer to the  supervisor documentation    startretries  no  Integer. Refer to the  supervisor documentation    autorestart  no  Refer to the  supervisor documentation    exitcodes  no  List of Integer. Refer to the  supervisor documentation    stopsignal  no  Refer to the  supervisor documentation    stopwaitsecs  no  Integer. Refer to the  supervisor documentation    stopasgroup  no  Boolean. Refer to the  supervisor documentation . May be required if your program fork another one.    killasgroup  no  Boolean. Refer to the  supervisor documentation    redirect_stderr  no  Boolan. Refer to the  supervisor documentation    stdout_logfile  no  Refer to the  supervisor documentation    stdout_logfile_maxbytes  no  Refer to the  supervisor documentation    stdout_logfile_backups  no  Integer. Refer to the  supervisor documentation    stderr_logfile  no  Refer to the  supervisor documentation    stderr_logfile_maxbytes  no  Refer to the  supervisor documentation    stderr_logfile_backups  no  Integer. Refer to the  supervisor documentation    environment  no  List of String. Refer to the  supervisor documentation    directory  no  Refer to the  supervisor documentation    umask  no  Refer to the  supervisor documentation    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_programs/#example", 
            "text": "supervisor_programs:\n- name: program1\n  supervisor: tech1\n  command:  /bin/bash /opt/tsupervisor/program1.sh   This snippet create a program 'program1' running as a daemon. This program being a simple shell script ( program1.sh ) and will be managed by the  supervisor named tech1 .  This program can then be managed (stopped/restarted/...) by:    The Web interface provided by the supervisor, if any.    The command line interface  supervisorctl_tech1    HADeploy using  --action start  and  --action stop .    Another example, with more parameters:  supervisor_programs:\n- name: program2\n  supervisor: tech1\n  command:  /bin/bash /opt/tsupervisor/program2.sh \n  stopasgroup: yes\n  environment:\n  - MY_HOME= /opt/program2 \n  - JAVA_HOME= /usr/java/jdk1.8.0_74 \n  directory: /opt/program2/work\n  stdout_logfile: /opt/program2/logs/program2.out\n  stderr_logfile: /opt/program2/logs/program2.err", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/supervisor/supervisor_programs/#about-managed-programs", 
            "text": "Programs meant to be run under supervisor should not daemonize themselves . Instead, they should run in the foreground. They should not detach from the terminal from which they are started.\nMore info  here    If the launched programs fork one or several child processes, there may be a problem on stop. Only the initial process will be killed by supervisor, and all children will become orphan. \nThere will be two solutions for this:    Catch the  stopsignal  and then kill all child processes.    Set the  stopasgroup  flag    This last solution is of course the simplest.   A typical case encountering this issue is a java programs launched by a wrapper script.", 
            "title": "About managed programs"
        }, 
        {
            "location": "/plugins_reference/users/groups/", 
            "text": "groups\n\n\nSynopsis\n\n\nProvide a list of Linux local groups required by the application \n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nThe group name\n\n\n\n\n\n\nsystem\n\n\nno\n\n\nIs this a System group.\nDefault: \nno\n\n\n\n\n\n\nmanaged\n\n\nno\n\n\nBoolean: Does HADeploy manage this group? If not, no action will be performed on it, but a failure will the triggered if it does not exists.\nDefault: \nyes\n\n\n\n\n\n\nscope\n\n\nno\n\n\nOn which host does this group will be created? May be:\nA single host name\nA single host_group name\nSeveral hosts or host_groups, separated by the character ':'\nDefault: \nall\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this group to be removed when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n## Example\n\n\n\n\n\n\n\n\n\n\n\n\ngroups:\n\n# A simple local group, created on all hosts.\n- name: broadgroup\n\n# A system group, created on host 'node1' and on all hosts belonging to the host_group 'hgrp1'.\n# Will never be removed by HADeploy\n- name: broadsystem\n  system: yes\n  managed: yes\n  scope: node1:hgrp1\n  no_remove: yes", 
            "title": "groups"
        }, 
        {
            "location": "/plugins_reference/users/groups/#groups", 
            "text": "", 
            "title": "groups"
        }, 
        {
            "location": "/plugins_reference/users/groups/#synopsis", 
            "text": "Provide a list of Linux local groups required by the application", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/users/groups/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      name  yes  The group name    system  no  Is this a System group. Default:  no    managed  no  Boolean: Does HADeploy manage this group? If not, no action will be performed on it, but a failure will the triggered if it does not exists. Default:  yes    scope  no  On which host does this group will be created? May be: A single host name A single host_group name Several hosts or host_groups, separated by the character ':' Default:  all    no_remove  no  Boolean: Prevent this group to be removed when HADeploy will be used in REMOVE mode. Default:  no    when  no  Boolean. Allow  conditional deployment  of this item. Default  True    ## Example       groups:\n\n# A simple local group, created on all hosts.\n- name: broadgroup\n\n# A system group, created on host 'node1' and on all hosts belonging to the host_group 'hgrp1'.\n# Will never be removed by HADeploy\n- name: broadsystem\n  system: yes\n  managed: yes\n  scope: node1:hgrp1\n  no_remove: yes", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/users/users/", 
            "text": "users\n\n\nSynopsis\n\n\nProvide a list of Linux local users required by the application \n\n\nAttributes\n\n\nEach item of the list has the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nlogin\n\n\nyes\n\n\nThe user's name\n\n\n\n\n\n\ncomment\n\n\nno\n\n\nDescription of the user's account\n\n\n\n\n\n\ngroup\n\n\nno\n\n\nSet the user primary group.\n\n\n\n\n\n\ngroups\n\n\nno\n\n\nPuts the user in this comma-delimited list of groups..\n\n\n\n\n\n\nsystem\n\n\nno\n\n\nIs this a System user.  (Note this can't be changed after user creation).\nDefault: \nno\n\n\n\n\n\n\nmanaged\n\n\nno\n\n\nBoolean: Does HADeploy manage this user?\nIf \nfalse\n, no action will be performed on it (except \nauthorized_keys\n and \ncan_sudo_to\n handling), but a failure will the triggered if it does not exists.\nDefault: \nyes\n\n\n\n\n\n\nscope\n\n\nno\n\n\nOn which host does this user will be created? May be:\nA single host name\nA single host_group name\nSeveral hosts or host_groups, separated by the character ':'\nDefault: \nall\n\n\n\n\n\n\ncan_sudo_to\n\n\nno\n\n\nA comma separated list of account this user will be able to 'sudo' into. May also be 'ALL'. Note this will be applied even if \nmanaged == no\n.\n\n\n\n\n\n\ncreate_home\n\n\nno\n\n\nUnless set to no, a home directory will be made for the user when the account is created or if the home directory does not exist.\nDefault: \nyes\n\n\n\n\n\n\npassword\n\n\nno\n\n\nSet the user's password to this crypted value.\n\n\n\n\n\n\nno_remove\n\n\nno\n\n\nBoolean: Prevent this group to be removed when HADeploy will be used in REMOVE mode.\nDefault: \nno\n\n\n\n\n\n\nauthorized_keys\n\n\nno\n\n\nProvide a list of public key files, which will be added to this user account in its authorized_keys file. Note this will be applied even if \nmanaged==no\n.\nIf these public key file locations are not absolute, they will be relative to the HADeploy embedding file location.\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nExample\n\n\nusers:\n- login: broadapp\n  comment: \nBroadapp technical account\n\n  groups: \nbroadgroup\n\n  authorized_keys:\n  - keys/id_rsa.pub\n\n- login: broaduser\n  scope: control_nodes\n  comment: \nBroadapp user account\n\n  groups: \nbroadgroup\n,\n  can_sudo_to: \nhdfs, yarn\n\n  password: \n$6$MySalt32$IrUOiWSpX.....8vkQfZgOslTQz.skFMhIWha.NLijJla/\n\n\n\n\n\nTrick\n\n\nA method to generate the crypted form of a password:\n\n\npython -c 'import crypt; print crypt.crypt(\nxxxxxx\n, \n$6$MySalt32$\n)'", 
            "title": "users"
        }, 
        {
            "location": "/plugins_reference/users/users/#users", 
            "text": "", 
            "title": "users"
        }, 
        {
            "location": "/plugins_reference/users/users/#synopsis", 
            "text": "Provide a list of Linux local users required by the application", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/users/users/#attributes", 
            "text": "Each item of the list has the following attributes:     Name  req?  Description      login  yes  The user's name    comment  no  Description of the user's account    group  no  Set the user primary group.    groups  no  Puts the user in this comma-delimited list of groups..    system  no  Is this a System user.  (Note this can't be changed after user creation). Default:  no    managed  no  Boolean: Does HADeploy manage this user? If  false , no action will be performed on it (except  authorized_keys  and  can_sudo_to  handling), but a failure will the triggered if it does not exists. Default:  yes    scope  no  On which host does this user will be created? May be: A single host name A single host_group name Several hosts or host_groups, separated by the character ':' Default:  all    can_sudo_to  no  A comma separated list of account this user will be able to 'sudo' into. May also be 'ALL'. Note this will be applied even if  managed == no .    create_home  no  Unless set to no, a home directory will be made for the user when the account is created or if the home directory does not exist. Default:  yes    password  no  Set the user's password to this crypted value.    no_remove  no  Boolean: Prevent this group to be removed when HADeploy will be used in REMOVE mode. Default:  no    authorized_keys  no  Provide a list of public key files, which will be added to this user account in its authorized_keys file. Note this will be applied even if  managed==no . If these public key file locations are not absolute, they will be relative to the HADeploy embedding file location.    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/users/users/#example", 
            "text": "users:\n- login: broadapp\n  comment:  Broadapp technical account \n  groups:  broadgroup \n  authorized_keys:\n  - keys/id_rsa.pub\n\n- login: broaduser\n  scope: control_nodes\n  comment:  Broadapp user account \n  groups:  broadgroup ,\n  can_sudo_to:  hdfs, yarn \n  password:  $6$MySalt32$IrUOiWSpX.....8vkQfZgOslTQz.skFMhIWha.NLijJla/", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/users/users/#trick", 
            "text": "A method to generate the crypted form of a password:  python -c 'import crypt; print crypt.crypt( xxxxxx ,  $6$MySalt32$ )'", 
            "title": "Trick"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_overview/", 
            "text": "Yarn plugin: Overview\n\n\nGoal\n\n\nAim of the Yarn plugin is to handle Yarn services Life cycle\n\n\n\n\n\n\nStart Yarn services\n\n\n\n\n\n\nStop Yarn services\n\n\n\n\n\n\nGet current Yarn service status\n\n\n\n\n\n\nBy 'Yarn services', we mean Yarn jobs running indefinitely, such as Spark Streaming jobs. \n\n\nThis all also named as 'Yarn Long Running Services', or 'Yarn Long Running Jobs'.\n\n\nThis plugin is NOT intended to manage Batch jobs.\n\n\nHow it works\n\n\nThis is achieved by:\n\n\n\n\n\n\nIssuing command to the Yarn Resource Manager through its REST API\n\n\n\n\n\n\nStart Yarn service when required using a user provided launching script.\n\n\n\n\n\n\nAll these operations are performed on a specific node included in the cluster. This node is designated as a \nyarn_relay\n.\n\n\nRequirement\n\n\nThere is some requirement for the launching script.\n\n\n\n\n\n\nHADeploy identify Yarn services by its name. So care must be taken to ensure the service name match the one provided in the definition. \nThis can easily be achieved using the \n--name\n option on the yarn/spark \nsubmit\n command. See example below.\n\n\n\n\n\n\nThe launching script must exit after launching the job. For Spark, the \n--conf \"spark.yarn.submit.waitAppCompletion=false\"\n option can be used. See example below.\n\n\n\n\n\n\nYarn services deployment.\n\n\nThe deployment of each services by itself is NOT in the scope of this plugin. Typically this consist in:\n\n\n\n\n\n\nCreate a deployment folder.\n\n\n\n\n\n\nDeploying a jar\n\n\n\n\n\n\nDeploying one or several configuration files\n\n\n\n\n\n\nEventually, deploying a script to launch the service.\n\n\n\n\n\n\nThis at least on the Yarn relay node and eventually on one or several other nodes, for resiliency. \n\n\nAll these tasks can be achieved using this HADeploy \nfolders\n and \nfiles\n specification. \n\n\nTemplating mechanism and support of Maven repository built in the \nfiles\n plugin will be of great help here. \n\n\nActions \nstop\n,\nstart\n and \nstatus\n\n\nThe \nyarn\n plugin introduce three new actions:\n\n\nhadeploy --src ..... --action start\n\n\n\n\nWill start all services described by the \nyarn_services\n list. And \n\n\nhadeploy --src ..... --action stop\n\n\n\n\nWill kill the same services. While \n\n\nhadeploy --src ..... --action status\n\n\n\n\nwill display current status of the services, in a rather primitive form.\n\n\nAlso, the Yarn plugin kill all running services at one of the first step of the removal action (\n--action remove\n).\n\n\nOf course, all this will occur only on services HADeploy is aware of (Defined with \nyarn_services\n). Other services will not be impacted.\n\n\nServices shutdown.\n\n\nWhen HADeploy is instructed to halt all services (\n--action stop\n), by default, it will use the RM REST API, setting application in the 'KILLED' state. This is equivalent to a \nyarn application --kill\n command.\n\n\nAn alternate way to shutdown a yarn job is to provide a script issuing the kill command, and to define such script using the \nkilling_cmd\n attribute. This can be used in the following case: \n\n\n\n\n\n\nIf your application provide a more graceful shutdown method\n\n\n\n\n\n\nIf your cluster is secured by Kerberos and if the RM REST API is not secured by SPNEGO. In such case, using such API will not allow setting of the appropriate user, and a killing script is a far more easy solution. \n\n\n\n\n\n\nNotifications: Services restart\n\n\nLet's say we now want to update the service's jar or one of the associated configuration files.\n\n\nWe can modify it and trigger a new deployment. HADeploy will notice the modification and push the new version on the target hosts. But, the running services will be unaffected.\n\n\nWe can restart it manually. But, HADeploy provide a mechanism to automate this. By adding a \nnotify\n attribute to the \nfiles\n definition. See the example below.\n\n\nRanger support.\n\n\nRanger handling on Yarn jobs is based on Yarn Queue management. HADeploy allow you to define such permission using \nyarn_ranger_policies\n.\n\n\nExample\n\n\nHere is a snippet describing the deployment of a simple Yarn services 'datastep':\n\n\n\nvars:\n  yarn_launcher_host: en1\n  basedir: \n/opt/datastep\n\n  user: dsrunner\n  group: dsrunner\n  datastep_version: \n0.1.0-SNAPSHOT\n\n\nyarn_relay:\n  host: ${yarn_launcher_host}\n\nmaven_repositories:\n- name: myrepo\n  snapshots_url: http://myrepo.mydomain.com/nexus/repository/maven-snapshots/\n  releases_url: http://myrepo.mydomain.com/nexus/repository/maven-releases/\n\nfolders:\n- { path: \n${basedir}\n, scope: \n${yarn_launcher_host}\n, owner: \n${user}\n, group: \n${group}\n, mode: \n755\n }\n\nfiles:\n- { scope: \n${yarn_launcher_host}\n, src: \nmvn://myrepo/com.mydomain/datastep/${datastep_version}/uber\n, \n    notify: ['yarn://datastep'], dest_folder: \n${basedir}\n, owner: \n${user}\n, group: \n${group}\n, mode: \n0644\n }\n\n- { scope: \n${yarn_launcher_host}\n, src: \ntmpl://submit.sh\n, dest_folder: \n${basedir}\n, \n    notify: ['yarn://datastep'], owner: \n${user}\n, group: \n${group}\n, mode: \n0744\n }\n\n- { scope: \n${yarn_launcher_host}\n, src: \ntmpl://kill.sh\n, dest_folder: \n${basedir}\n, \n    notify: ['yarn://datastep'], owner: \n${user}\n, group: \n${group}\n, mode: \n0744\n }\n\nyarn_services:\n- name: datastep\n  launching_cmd: ./submit.sh\n  launching_cmd: ./kill.sh\n  launching_dir: ${basedir}\n\n\n\n\n\nAnd here is what could be a simplistic submit script template:\n\n\n#/bin/bash\n\n{% if kerberos is defined and kerberos %}\nkinit -kt /etc/security/keytabs/{{user}}.keytab {{user}}\n{% endif %}\n\nspark-submit --name datastep --master yarn --deploy-mode cluster --class com.mydomain.datastep.Main \\\n    --conf \nspark.yarn.submit.waitAppCompletion=false\n \\\n    --jars {{basedir}}/datastep-{{datastep_version}}-uber.jar \n\n{% if kerberos is defined and kerberos %}\nkdestroy\n{% endif %}\n\n\n\n\n\nAnd a killing script:\n\n\n#/bin/bash\n\n{% if kerberos is defined and kerberos %}\nkinit -kt /etc/security/keytabs/{{user}}.keytab {{user}}\n{% endif %}\n\nAPPLICATION_ID=$(yarn application --appStates RUNNING --list 2\n/dev/null | awk \n{ if (\\$2==\\\ndatastep\\\n) print \\$1 }\n)\n\nif [ \n$APPLICATION_ID\n = \n ]\nthen\n    echo \n?? Not running\n\nelse\n    yarn application --kill ${APPLICATION_ID} 2\n/dev/null\n    echo \n$APPLICATION_ID Killed!\n\nfi\n\n{% if kerberos is defined and kerberos %}\nkdestroy\n{% endif %}\n\n\n\n\n\nThis is of course not complete, as it lack at least the target cluster definition.\n\n\nPlease refer to \nyarn_relay\n and \nyarn_services\n for a complete description. And to \nfiles\n for the \nnotify\n syntax.\n\n\nOf course, before being able to launch the services (\n--action start\n), a deployment must be performed before (\n--action deploy\n)", 
            "title": "yarn_overview"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_overview/#yarn-plugin-overview", 
            "text": "", 
            "title": "Yarn plugin: Overview"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_overview/#goal", 
            "text": "Aim of the Yarn plugin is to handle Yarn services Life cycle    Start Yarn services    Stop Yarn services    Get current Yarn service status    By 'Yarn services', we mean Yarn jobs running indefinitely, such as Spark Streaming jobs.   This all also named as 'Yarn Long Running Services', or 'Yarn Long Running Jobs'.  This plugin is NOT intended to manage Batch jobs.", 
            "title": "Goal"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_overview/#how-it-works", 
            "text": "This is achieved by:    Issuing command to the Yarn Resource Manager through its REST API    Start Yarn service when required using a user provided launching script.    All these operations are performed on a specific node included in the cluster. This node is designated as a  yarn_relay .", 
            "title": "How it works"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_overview/#requirement", 
            "text": "There is some requirement for the launching script.    HADeploy identify Yarn services by its name. So care must be taken to ensure the service name match the one provided in the definition. \nThis can easily be achieved using the  --name  option on the yarn/spark  submit  command. See example below.    The launching script must exit after launching the job. For Spark, the  --conf \"spark.yarn.submit.waitAppCompletion=false\"  option can be used. See example below.", 
            "title": "Requirement"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_overview/#yarn-services-deployment", 
            "text": "The deployment of each services by itself is NOT in the scope of this plugin. Typically this consist in:    Create a deployment folder.    Deploying a jar    Deploying one or several configuration files    Eventually, deploying a script to launch the service.    This at least on the Yarn relay node and eventually on one or several other nodes, for resiliency.   All these tasks can be achieved using this HADeploy  folders  and  files  specification.   Templating mechanism and support of Maven repository built in the  files  plugin will be of great help here.", 
            "title": "Yarn services deployment."
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_overview/#actions-stopstart-and-status", 
            "text": "The  yarn  plugin introduce three new actions:  hadeploy --src ..... --action start  Will start all services described by the  yarn_services  list. And   hadeploy --src ..... --action stop  Will kill the same services. While   hadeploy --src ..... --action status  will display current status of the services, in a rather primitive form.  Also, the Yarn plugin kill all running services at one of the first step of the removal action ( --action remove ).  Of course, all this will occur only on services HADeploy is aware of (Defined with  yarn_services ). Other services will not be impacted.", 
            "title": "Actions stop,start and status"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_overview/#services-shutdown", 
            "text": "When HADeploy is instructed to halt all services ( --action stop ), by default, it will use the RM REST API, setting application in the 'KILLED' state. This is equivalent to a  yarn application --kill  command.  An alternate way to shutdown a yarn job is to provide a script issuing the kill command, and to define such script using the  killing_cmd  attribute. This can be used in the following case:     If your application provide a more graceful shutdown method    If your cluster is secured by Kerberos and if the RM REST API is not secured by SPNEGO. In such case, using such API will not allow setting of the appropriate user, and a killing script is a far more easy solution.", 
            "title": "Services shutdown."
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_overview/#notifications-services-restart", 
            "text": "Let's say we now want to update the service's jar or one of the associated configuration files.  We can modify it and trigger a new deployment. HADeploy will notice the modification and push the new version on the target hosts. But, the running services will be unaffected.  We can restart it manually. But, HADeploy provide a mechanism to automate this. By adding a  notify  attribute to the  files  definition. See the example below.", 
            "title": "Notifications: Services restart"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_overview/#ranger-support", 
            "text": "Ranger handling on Yarn jobs is based on Yarn Queue management. HADeploy allow you to define such permission using  yarn_ranger_policies .", 
            "title": "Ranger support."
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_overview/#example", 
            "text": "Here is a snippet describing the deployment of a simple Yarn services 'datastep':  \nvars:\n  yarn_launcher_host: en1\n  basedir:  /opt/datastep \n  user: dsrunner\n  group: dsrunner\n  datastep_version:  0.1.0-SNAPSHOT \n\nyarn_relay:\n  host: ${yarn_launcher_host}\n\nmaven_repositories:\n- name: myrepo\n  snapshots_url: http://myrepo.mydomain.com/nexus/repository/maven-snapshots/\n  releases_url: http://myrepo.mydomain.com/nexus/repository/maven-releases/\n\nfolders:\n- { path:  ${basedir} , scope:  ${yarn_launcher_host} , owner:  ${user} , group:  ${group} , mode:  755  }\n\nfiles:\n- { scope:  ${yarn_launcher_host} , src:  mvn://myrepo/com.mydomain/datastep/${datastep_version}/uber , \n    notify: ['yarn://datastep'], dest_folder:  ${basedir} , owner:  ${user} , group:  ${group} , mode:  0644  }\n\n- { scope:  ${yarn_launcher_host} , src:  tmpl://submit.sh , dest_folder:  ${basedir} , \n    notify: ['yarn://datastep'], owner:  ${user} , group:  ${group} , mode:  0744  }\n\n- { scope:  ${yarn_launcher_host} , src:  tmpl://kill.sh , dest_folder:  ${basedir} , \n    notify: ['yarn://datastep'], owner:  ${user} , group:  ${group} , mode:  0744  }\n\nyarn_services:\n- name: datastep\n  launching_cmd: ./submit.sh\n  launching_cmd: ./kill.sh\n  launching_dir: ${basedir}  And here is what could be a simplistic submit script template:  #/bin/bash\n\n{% if kerberos is defined and kerberos %}\nkinit -kt /etc/security/keytabs/{{user}}.keytab {{user}}\n{% endif %}\n\nspark-submit --name datastep --master yarn --deploy-mode cluster --class com.mydomain.datastep.Main \\\n    --conf  spark.yarn.submit.waitAppCompletion=false  \\\n    --jars {{basedir}}/datastep-{{datastep_version}}-uber.jar \n\n{% if kerberos is defined and kerberos %}\nkdestroy\n{% endif %}  And a killing script:  #/bin/bash\n\n{% if kerberos is defined and kerberos %}\nkinit -kt /etc/security/keytabs/{{user}}.keytab {{user}}\n{% endif %}\n\nAPPLICATION_ID=$(yarn application --appStates RUNNING --list 2 /dev/null | awk  { if (\\$2==\\ datastep\\ ) print \\$1 } )\n\nif [  $APPLICATION_ID  =   ]\nthen\n    echo  ?? Not running \nelse\n    yarn application --kill ${APPLICATION_ID} 2 /dev/null\n    echo  $APPLICATION_ID Killed! \nfi\n\n{% if kerberos is defined and kerberos %}\nkdestroy\n{% endif %}  This is of course not complete, as it lack at least the target cluster definition.  Please refer to  yarn_relay  and  yarn_services  for a complete description. And to  files  for the  notify  syntax.  Of course, before being able to launch the services ( --action start ), a deployment must be performed before ( --action deploy )", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_relay/", 
            "text": "yarn_relay\n\n\nSynopsis\n\n\nMost of Yarn commands will be performed using RessouceManager REST API's. This definition will provide informations for HADeploy to use this interface.\n\n\nThis is a reference part. Refer to the associated \noverview\n for a more synthetical view.\n\n\nAttributes\n\n\nyarn_relay\n is a map with the following attributes:\n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nhost\n\n\nyes\n\n\nThe host which will be used for both launching services (using provided script), and accessing the RessourceManager UI REST interface\n\n\n\n\n\n\ndefault_timeout_secs\n\n\nno\n\n\nDefault value for \ntimeout_secs\n value on \nyarn_services\n entries. Default to 90 seconds\n\n\n\n\n\n\nprincipal\n\n\nno\n\n\nA Kerberos principal allowing all Yarn related operation to be performed. See \nbelow\n\n\n\n\n\n\nlocal_keytab_path\n\n\nno\n\n\nA local path to the associated keytab file. This path is relative to the embedding file. See \nbelow\n\n\n\n\n\n\nrelay_keytab_path\n\n\nno\n\n\nA path to the associated keytab file on the relay host. See \nbelow\n\n\n\n\n\n\ntools_folder\n\n\nno\n\n\nFolder used by HADeploy to store keytab if needed.\nDefault: \n/tmp/hadeploy_\nuser\n/\n where \nuser\n is the \nssh_user\n defined for this relay host.\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\nrm_endpoint\n\n\nno\n\n\nProvide Yarn REST API entry point. Typically \nnamenode.mycluster.com:8088\n. It could also be a comma separated list of entry point, which will be checked up to a valid one. This will allow resource manager H.A. handling. If not defined, will be looked up in local yarn-site.xml\n\n\n\n\n\n\nhadoop_conf_dir\n\n\nno\n\n\nSpecify Hadoop configurations file location, where HADeploy will lookup the \nyarn-site.xml\n file. Default to \n/etc/hadoop/conf\n.\n\n\n\n\n\n\n\n\nResource Manager configuration lookup\n\n\nIf this \nyarn_relay\n host is properly configured as an Hadoop client, there should be no need to provide value to \nhadoop_conf_dir\n and/or \nrm_endpoint\n, \nas HADeploy will be able to lookup the Resource Manager Web URL by using default values. \n\n\nKerberos authentication\n\n\nWhen \nprincipal\n and \n..._keytab_path\n variables are defined, Kerberos authentication will be activated for all Yarn operations. This means a \nkinit\n will be issued with provided values before any Yarn access, and a \nkdestroy\n issued after. This has the following consequences:\n\n\n\n\n\n\nAll Yarn operations will be performed on behalf of the user defined by the provided principal. \n\n\n\n\n\n\nThe \nkinit\n will be issued on the relay host with the \nssh_user\n account. This means any previous ticket own by this user on this node will be lost. \n\n\n\n\n\n\nRegarding the keytab file, two cases:\n\n\n\n\n\n\nThis keytab file already exists on the relay host. In such case, the \nrelay_keytab_path\n must be set to the location of this file. And the relay host's \nssh_user\n must have read access on it.\n\n\n\n\n\n\nThis keytab file is not present on the relay host. In such case the \nlocal_keytab_path\n must be set to its local location. HADeploy will take care of copying it on the remote relay host, \nin a location under \ntools_folder\n. Note you can also modify this target location by setting also the \nrelay_keytab_path\n parameter. In this case, \nit must be the full path, including the keytab file name. And the containing folder must exists.\n\n\n\n\n\n\nAlso, note this will lead SPNEGO to be used to authenticate on the RM REST API. If SPNEGO is not activated on such API (This could be the case even on a Kerberos enabled cluster), this will generate an error.\n\n\nThe solution in such case, is to NOT define principal/keytab on the \nyarn_relay\n, but to explicitly add a \nkinit ...\n and \nkdestroy\n commands in the \nlaunching_cmd\n and \nkilling_cmd\n associated scripts of each \nyarn_service\n (Defining a \nkilling_cmd\n script is required in this case).  \n\n\nExample\n\n\nThe simplest case:\n\n\nyarn_relay:\n  host: en1\n\n\n\n\nAnd a more complete case, in a secured environment.\n\n\nyarn_relay:\n  host: en1\n  principal: sa\n  local_keytab_path: ./sa-gate17.keytab\n  default_timeout_secs: 240", 
            "title": "yarn_relay"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_relay/#yarn_relay", 
            "text": "", 
            "title": "yarn_relay"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_relay/#synopsis", 
            "text": "Most of Yarn commands will be performed using RessouceManager REST API's. This definition will provide informations for HADeploy to use this interface.  This is a reference part. Refer to the associated  overview  for a more synthetical view.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_relay/#attributes", 
            "text": "yarn_relay  is a map with the following attributes:     Name  req?  Description      host  yes  The host which will be used for both launching services (using provided script), and accessing the RessourceManager UI REST interface    default_timeout_secs  no  Default value for  timeout_secs  value on  yarn_services  entries. Default to 90 seconds    principal  no  A Kerberos principal allowing all Yarn related operation to be performed. See  below    local_keytab_path  no  A local path to the associated keytab file. This path is relative to the embedding file. See  below    relay_keytab_path  no  A path to the associated keytab file on the relay host. See  below    tools_folder  no  Folder used by HADeploy to store keytab if needed. Default:  /tmp/hadeploy_ user /  where  user  is the  ssh_user  defined for this relay host.    when  no  Boolean. Allow  conditional deployment  of this item. Default  True    rm_endpoint  no  Provide Yarn REST API entry point. Typically  namenode.mycluster.com:8088 . It could also be a comma separated list of entry point, which will be checked up to a valid one. This will allow resource manager H.A. handling. If not defined, will be looked up in local yarn-site.xml    hadoop_conf_dir  no  Specify Hadoop configurations file location, where HADeploy will lookup the  yarn-site.xml  file. Default to  /etc/hadoop/conf .", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_relay/#resource-manager-configuration-lookup", 
            "text": "If this  yarn_relay  host is properly configured as an Hadoop client, there should be no need to provide value to  hadoop_conf_dir  and/or  rm_endpoint , \nas HADeploy will be able to lookup the Resource Manager Web URL by using default values.", 
            "title": "Resource Manager configuration lookup"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_relay/#kerberos-authentication", 
            "text": "When  principal  and  ..._keytab_path  variables are defined, Kerberos authentication will be activated for all Yarn operations. This means a  kinit  will be issued with provided values before any Yarn access, and a  kdestroy  issued after. This has the following consequences:    All Yarn operations will be performed on behalf of the user defined by the provided principal.     The  kinit  will be issued on the relay host with the  ssh_user  account. This means any previous ticket own by this user on this node will be lost.     Regarding the keytab file, two cases:    This keytab file already exists on the relay host. In such case, the  relay_keytab_path  must be set to the location of this file. And the relay host's  ssh_user  must have read access on it.    This keytab file is not present on the relay host. In such case the  local_keytab_path  must be set to its local location. HADeploy will take care of copying it on the remote relay host, \nin a location under  tools_folder . Note you can also modify this target location by setting also the  relay_keytab_path  parameter. In this case, \nit must be the full path, including the keytab file name. And the containing folder must exists.    Also, note this will lead SPNEGO to be used to authenticate on the RM REST API. If SPNEGO is not activated on such API (This could be the case even on a Kerberos enabled cluster), this will generate an error.  The solution in such case, is to NOT define principal/keytab on the  yarn_relay , but to explicitly add a  kinit ...  and  kdestroy  commands in the  launching_cmd  and  killing_cmd  associated scripts of each  yarn_service  (Defining a  killing_cmd  script is required in this case).", 
            "title": "Kerberos authentication"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_relay/#example", 
            "text": "The simplest case:  yarn_relay:\n  host: en1  And a more complete case, in a secured environment.  yarn_relay:\n  host: en1\n  principal: sa\n  local_keytab_path: ./sa-gate17.keytab\n  default_timeout_secs: 240", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_services/", 
            "text": "yarn_services\n\n\nSynopsis\n\n\nProvide a list of Yarn services to manage\n\n\nThis is a reference part. Refer to the associated \noverview\n for a more synthetical view.\n\n\nAttributes\n\n\nEach item of the list has the following attribute: \n\n\n\n\n\n\n\n\nName\n\n\nreq?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nyes\n\n\nName of the service. This should match the name of the batch job.\n\n\n\n\n\n\nlaunching_cmd\n\n\nyes\n\n\nThe command to launch this service. Typically call a script hosting a \nspark-submit\n or a \nyarn jar\n command.\n\n\n\n\n\n\nlaunching_dir\n\n\nno\n\n\nWill \ncd\n in this folder before launching 'launching_cmd' Must be an absolute path.\nDefault: \n~\n\n\n\n\n\n\nkilling_cmd\n\n\nno\n\n\nThe command to kill this service. If not provided, the service will be killed using the Resource Manager REST API. See \nOverview/Service shutdown\n\n\n\n\n\n\ntimeout_secs\n\n\nno\n\n\nTimeout before raising an error value when waiting a target state.\nDefault: Value set by \nyarn_relay\n:\ndefault_timeout_secs\n\n\n\n\n\n\nwhen\n\n\nno\n\n\nBoolean. Allow \nconditional deployment\n of this item.\nDefault \nTrue\n\n\n\n\n\n\n\n\nExample\n\n\nyarn_services:\n- name: datastep\n  launching_cmd: ./launcher.sh\n  launching_dir: ${basedir}\n\n\n\n\nThe launching script\n\n\nHere is some requirement about the launching script.\n\n\n\n\n\n\nHADeploy identify Yarn services by its name. So care must be taken to ensure the service name match the one provided in the definition. This can easily be achieved using the \n--name\n option on the yarn/spark \nsubmit\n command. See \nexample here\n\n\n\n\n\n\nThe launching script be synchronous (not running in background) but must exit after launching the job. For Spark, the \n--conf \"spark.yarn.submit.waitAppCompletion=false\"\n option can be used. See example below.", 
            "title": "yarn_service"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_services/#yarn_services", 
            "text": "", 
            "title": "yarn_services"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_services/#synopsis", 
            "text": "Provide a list of Yarn services to manage  This is a reference part. Refer to the associated  overview  for a more synthetical view.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_services/#attributes", 
            "text": "Each item of the list has the following attribute:      Name  req?  Description      name  yes  Name of the service. This should match the name of the batch job.    launching_cmd  yes  The command to launch this service. Typically call a script hosting a  spark-submit  or a  yarn jar  command.    launching_dir  no  Will  cd  in this folder before launching 'launching_cmd' Must be an absolute path. Default:  ~    killing_cmd  no  The command to kill this service. If not provided, the service will be killed using the Resource Manager REST API. See  Overview/Service shutdown    timeout_secs  no  Timeout before raising an error value when waiting a target state. Default: Value set by  yarn_relay : default_timeout_secs    when  no  Boolean. Allow  conditional deployment  of this item. Default  True", 
            "title": "Attributes"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_services/#example", 
            "text": "yarn_services:\n- name: datastep\n  launching_cmd: ./launcher.sh\n  launching_dir: ${basedir}", 
            "title": "Example"
        }, 
        {
            "location": "/plugins_reference/yarn/yarn_services/#the-launching-script", 
            "text": "Here is some requirement about the launching script.    HADeploy identify Yarn services by its name. So care must be taken to ensure the service name match the one provided in the definition. This can easily be achieved using the  --name  option on the yarn/spark  submit  command. See  example here    The launching script be synchronous (not running in background) but must exit after launching the job. For Spark, the  --conf \"spark.yarn.submit.waitAppCompletion=false\"  option can be used. See example below.", 
            "title": "The launching script"
        }, 
        {
            "location": "/more/conditional_deployment/", 
            "text": "Conditional deployment\n\n\nSometime, you want to skip a particular object creation, based on specifics condition. \n\n\nIt is quite easy to do with HADeploy, as most of the object declaration accept a \nwhen:\n attribute. If this boolean resolve to 'False', then the object creation will be skipped.\n\n\nFor example:\n\n\nvars:\n  isKerberos: true\n...\nfiles:\n- scope: all\n  dest_folder: \n/etc/security/keytabs\n\n  src: \n${appUser}.keytab\n \n  owner: \n${appUser}\n\n  group: broadgroup\n  mode: \n0400\n\n  when: ${isKerberos}\n\n\n\n\nNote than, as all variables, what is inside the ${...} delimiter is not limited to a single variable name, but is in fact a Jinja2 expression. So, it also could be, for example:\n\n\n...\nfiles:\n- scope: all\n  ...\n  when: ${isKerberos is defined and isKerberos}\n\n\n\n\nAlso, note the usefulness of the \nalternate variable notation\n for the 'flow style' notation. \n\n\nfiles:\n- {  when: \nisKerberos\n, scope: all, dest_folder: \n/etc/security/keytabs\n, src= \n${appUser}.keytab\n owner: \n${appUser}\n, group: broadgroup, mode: \n0400\n }", 
            "title": "Conditional deployment"
        }, 
        {
            "location": "/more/conditional_deployment/#conditional-deployment", 
            "text": "Sometime, you want to skip a particular object creation, based on specifics condition.   It is quite easy to do with HADeploy, as most of the object declaration accept a  when:  attribute. If this boolean resolve to 'False', then the object creation will be skipped.  For example:  vars:\n  isKerberos: true\n...\nfiles:\n- scope: all\n  dest_folder:  /etc/security/keytabs \n  src:  ${appUser}.keytab  \n  owner:  ${appUser} \n  group: broadgroup\n  mode:  0400 \n  when: ${isKerberos}  Note than, as all variables, what is inside the ${...} delimiter is not limited to a single variable name, but is in fact a Jinja2 expression. So, it also could be, for example:  ...\nfiles:\n- scope: all\n  ...\n  when: ${isKerberos is defined and isKerberos}  Also, note the usefulness of the  alternate variable notation  for the 'flow style' notation.   files:\n- {  when:  isKerberos , scope: all, dest_folder:  /etc/security/keytabs , src=  ${appUser}.keytab  owner:  ${appUser} , group: broadgroup, mode:  0400  }", 
            "title": "Conditional deployment"
        }, 
        {
            "location": "/more/under_the_hood/", 
            "text": "Under the hood\n\n\nOverview\n\n\nAs stated in the installation instruction, HADeploy use Ansible under the hood to perform operation on the remote hosts. As such Ansible is defined as a dependency for HADeploy installation.\n\n\nThe main steps of an HADeploy run are the following:\n\n\n\n\n\n\nLoad the application description by appending all \n--src\n files provided on the command line to build a single deployment file.\n\n\n\n\n\n\nCheck syntax of this deployment file, based on a YAML schema.\n\n\n\n\n\n\nBuild a data model in memory representing this file content.\n\n\n\n\n\n\nFor all objects, Check \nwhen:\n clause and remove it if \nFalse\n.\n\n\n\n\n\n\nCheck this data model for consistency, enrich it, or transform some data to ease the next stages.\n\n\n\n\n\n\nGenerate a Jinja2 template by concatenating all template snippets provided by the plugins.\n\n\n\n\n\n\nRender this template with the model to generate an Ansible playbook\n\n\n\n\n\n\nLaunch Ansible on this playbook.\n\n\n\n\n\n\nVariables\n\n\nWhen using HADeploy in an advanced way (i.e using the \nansible\n module, or developing your own plugin), you may be disturbed by different variable notation. \nThere is in fact 3 kinds of variables involved in HADeploy\n\n\n${my_variable}\n\n\nOr \nmy_variable\n\n\nThis is the only variable notation you should be aware of for standard usage of HADeploy.\n\n\nSuch variables are resolved during step 1 (Building of the deployment file).\n\n\nRefer to \nalternate notation\n for the motivation of using \nmy_variable\n.\n\n\n{{{my_variable}}}\n\n\nThis is the variable notation used during the rendering of step 6. This will allow all snippets provided by the plugin to access variables of the model.\n\n\n{{my_variable}}\n\n\nThis is the standard variable notation used by Ansible. HADeploy will not resolve such variables, passing them as is to Ansible playbook. So they will be resolved by Ansible in step 7.\n\n\nThis form need generaly to be quoted (\n\"{{my_variable}}\"\n). It must also be used for \nencrypted values\n\n\nVariable relationship\n\n\nAlthough this variables act at different level, there is some mechanism to propagate user's value (The one with ${..} notation) to lower level.\n\n\n\n\n\n\nThey are copied in the data model built in step 3, under the token \nsrc.vars\n. So, the variable \n${my_variable}\ncan be accessed by \n{{{src.vars.my_variable}}}\nby a plugin playbook snippet.\n\n\n\n\n\n\nIn the Ansible context, a \ngroup_vars/all\n file is generated, containing a line as \nmy_variable: my_value\n for each user's variable. So, all the user's variable will be directly accessible by Ansible, in step 7.\n\n\n\n\n\n\nThe working folder\n\n\nIn case of problem, it could be useful to have a look on the generated Ansible playbook.\nThis playbook is generated in a temporary folder, called the \nworking folder\n. It is named as the action provided as parameter.\n\n\nThis folder is automatically created in \n/tmp\n, under a random name. \nTo ease debugging, one can force this working folder to a specific location, using the \n--workingFolder\n command line option.\n\n\n\n\nWarning: In such case, the full content of the working folder will be deleted on each run...\n\n\n\n\nBut, this working folder not only contains the playbook. Here is a brief description of the files you may found in it:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nansible.cfg\ninventory\ngroup_vars folder\n\n\nHAdeploy create a complete Ansible context to run Ansible inside.\n\n\n\n\n\n\naction\n.yml.jj2\n\n\nThis is the Jinja2 source template, result of step 5 above, which will be merged with the data model to produce the file below.\n\n\n\n\n\n\naction\n.yml\n\n\nThe playbook for targeted action, generated on step 6. i.e \ndeploy.yml\n or \nremove.yml\n.\n\n\n\n\n\n\nmodel_src.json\n\n\nThis is the part of the data model built from the sources file.\n\n\n\n\n\n\nmodel_data.json\n\n\nThis is a the part of the data model where HADeploy store some intermediate structure.\n\n\n\n\n\n\nmodel_helper.json\n\n\nThis is a part of the data model hosting some configuration information.\n\n\n\n\n\n\nschema.yml\n\n\nThis is the YAML schema used to validate source input files. Will use \npykwalify\n tool for validation.\n\n\n\n\n\n\ndesc_xxxx.yml.j2\n\n\nSome helper files, specific to some plugins.\n\n\n\n\n\n\n\n\nNote: If you launch HADeploy with \n--action none\n then it will generate ansible playbooks for all action it is aware off. But, it will not launch ansible. \nThis is intended to validate description for all action.\n\n\nPlugins\n\n\nA plugin is a component which may be involved in all phases described at the befinning of this page. It is typically made of:\n\n\n\n\n\n\nA partial schema. All plugin's YAML schema parts will be merged to provide the overall schema against which the deployment file will be validated.\n\n\n\n\n\n\nSome Python code called to check and enrich the model. This code must include a subclass of the \nPlugin\n class, to handle plugin properties and lifecycle.\n\n\n\n\n\n\nSeveral playbook snippet. Typically, one per supported action. For a given action, all plugin plabook's snippets will be concatenated to build the target playbook. These snippets are Jinja2 templates which will be merged with the data model.\n\n\n\n\n\n\nAll theses are optional. A plugin can also host:\n\n\n\n\n\n\nAnsible roles or modules.\n\n\n\n\n\n\nOne or more helper. An helper is a specific program designed to manage services which offer only a Java API.\n\n\n\n\n\n\nCurrently, HADeploy is provided with the following internal plugin:\n\n\n\n\ninventory\n for target hosts management.\n\n\nansible_inventories\n for direct integration of an Ansible inventories.\n\n\nusers\n for management of local users and groups. \n\n\nfiles\n for base file management.\n\n\nhdfs\n which extends the previous one for HDFS accesses.\n\n\nhbase\n\n\nhive\n\n\nkafka\n\n\nansible\n\n\nranger\n\n\n\n\nEmbedded Ansible roles\n\n\n\n\n\n\nFor HDFS access, HADeploy embeds the \nhdfs_modules\n Ansible modules in the HDFS plugin\n\n\n\n\n\n\nFor Apache Ranger policy handling, HADeploy embeds the \nranger_modules\n Ansible modules in the Ranger plugin\n\n\n\n\n\n\nFor Storm topologies lifecycle handling, HADeploy embeds the \nstorm_modules\n Ansible modules in the Elasticsearch plugin\n\n\n\n\n\n\nFor Elasticsearch indices and templates managment, HADeploy embeds the \nelastic_modules\n Ansible modules in the Elasticsearch plugin\n\n\n\n\n\n\nEmbedded Helpers\n\n\nHive\n\n\n\n\nFor Hive based deployment, HADeploy embeds the \njdchive\n tool.\n\n\n\n\nHBase\n\n\n\n\n\n\nFor HBase based deployment, HADeploy embeds the \njdchtable\n tool.\n\n\n\n\n\n\nFor HBase dataset loading, HADeploy embeds the \nhbload\n tool.\n\n\n\n\n\n\nKafka\n\n\n\n\nFor Kafka based deployment, HADeploy embeds the \njdctopic\n tool.", 
            "title": "Under the hood"
        }, 
        {
            "location": "/more/under_the_hood/#under-the-hood", 
            "text": "", 
            "title": "Under the hood"
        }, 
        {
            "location": "/more/under_the_hood/#overview", 
            "text": "As stated in the installation instruction, HADeploy use Ansible under the hood to perform operation on the remote hosts. As such Ansible is defined as a dependency for HADeploy installation.  The main steps of an HADeploy run are the following:    Load the application description by appending all  --src  files provided on the command line to build a single deployment file.    Check syntax of this deployment file, based on a YAML schema.    Build a data model in memory representing this file content.    For all objects, Check  when:  clause and remove it if  False .    Check this data model for consistency, enrich it, or transform some data to ease the next stages.    Generate a Jinja2 template by concatenating all template snippets provided by the plugins.    Render this template with the model to generate an Ansible playbook    Launch Ansible on this playbook.", 
            "title": "Overview"
        }, 
        {
            "location": "/more/under_the_hood/#variables", 
            "text": "When using HADeploy in an advanced way (i.e using the  ansible  module, or developing your own plugin), you may be disturbed by different variable notation. \nThere is in fact 3 kinds of variables involved in HADeploy", 
            "title": "Variables"
        }, 
        {
            "location": "/more/under_the_hood/#my_variable", 
            "text": "Or  my_variable  This is the only variable notation you should be aware of for standard usage of HADeploy.  Such variables are resolved during step 1 (Building of the deployment file).  Refer to  alternate notation  for the motivation of using  my_variable .", 
            "title": "${my_variable}"
        }, 
        {
            "location": "/more/under_the_hood/#my_variable_1", 
            "text": "This is the variable notation used during the rendering of step 6. This will allow all snippets provided by the plugin to access variables of the model.", 
            "title": "{{{my_variable}}}"
        }, 
        {
            "location": "/more/under_the_hood/#my_variable_2", 
            "text": "This is the standard variable notation used by Ansible. HADeploy will not resolve such variables, passing them as is to Ansible playbook. So they will be resolved by Ansible in step 7.  This form need generaly to be quoted ( \"{{my_variable}}\" ). It must also be used for  encrypted values", 
            "title": "{{my_variable}}"
        }, 
        {
            "location": "/more/under_the_hood/#variable-relationship", 
            "text": "Although this variables act at different level, there is some mechanism to propagate user's value (The one with ${..} notation) to lower level.    They are copied in the data model built in step 3, under the token  src.vars . So, the variable  ${my_variable} can be accessed by  {{{src.vars.my_variable}}} by a plugin playbook snippet.    In the Ansible context, a  group_vars/all  file is generated, containing a line as  my_variable: my_value  for each user's variable. So, all the user's variable will be directly accessible by Ansible, in step 7.", 
            "title": "Variable relationship"
        }, 
        {
            "location": "/more/under_the_hood/#the-working-folder", 
            "text": "In case of problem, it could be useful to have a look on the generated Ansible playbook.\nThis playbook is generated in a temporary folder, called the  working folder . It is named as the action provided as parameter.  This folder is automatically created in  /tmp , under a random name. \nTo ease debugging, one can force this working folder to a specific location, using the  --workingFolder  command line option.   Warning: In such case, the full content of the working folder will be deleted on each run...   But, this working folder not only contains the playbook. Here is a brief description of the files you may found in it:     Name  Description      ansible.cfg inventory group_vars folder  HAdeploy create a complete Ansible context to run Ansible inside.    action .yml.jj2  This is the Jinja2 source template, result of step 5 above, which will be merged with the data model to produce the file below.    action .yml  The playbook for targeted action, generated on step 6. i.e  deploy.yml  or  remove.yml .    model_src.json  This is the part of the data model built from the sources file.    model_data.json  This is a the part of the data model where HADeploy store some intermediate structure.    model_helper.json  This is a part of the data model hosting some configuration information.    schema.yml  This is the YAML schema used to validate source input files. Will use  pykwalify  tool for validation.    desc_xxxx.yml.j2  Some helper files, specific to some plugins.     Note: If you launch HADeploy with  --action none  then it will generate ansible playbooks for all action it is aware off. But, it will not launch ansible. \nThis is intended to validate description for all action.", 
            "title": "The working folder"
        }, 
        {
            "location": "/more/under_the_hood/#plugins", 
            "text": "A plugin is a component which may be involved in all phases described at the befinning of this page. It is typically made of:    A partial schema. All plugin's YAML schema parts will be merged to provide the overall schema against which the deployment file will be validated.    Some Python code called to check and enrich the model. This code must include a subclass of the  Plugin  class, to handle plugin properties and lifecycle.    Several playbook snippet. Typically, one per supported action. For a given action, all plugin plabook's snippets will be concatenated to build the target playbook. These snippets are Jinja2 templates which will be merged with the data model.    All theses are optional. A plugin can also host:    Ansible roles or modules.    One or more helper. An helper is a specific program designed to manage services which offer only a Java API.    Currently, HADeploy is provided with the following internal plugin:   inventory  for target hosts management.  ansible_inventories  for direct integration of an Ansible inventories.  users  for management of local users and groups.   files  for base file management.  hdfs  which extends the previous one for HDFS accesses.  hbase  hive  kafka  ansible  ranger", 
            "title": "Plugins"
        }, 
        {
            "location": "/more/under_the_hood/#embedded-ansible-roles", 
            "text": "For HDFS access, HADeploy embeds the  hdfs_modules  Ansible modules in the HDFS plugin    For Apache Ranger policy handling, HADeploy embeds the  ranger_modules  Ansible modules in the Ranger plugin    For Storm topologies lifecycle handling, HADeploy embeds the  storm_modules  Ansible modules in the Elasticsearch plugin    For Elasticsearch indices and templates managment, HADeploy embeds the  elastic_modules  Ansible modules in the Elasticsearch plugin", 
            "title": "Embedded Ansible roles"
        }, 
        {
            "location": "/more/under_the_hood/#embedded-helpers", 
            "text": "", 
            "title": "Embedded Helpers"
        }, 
        {
            "location": "/more/under_the_hood/#hive", 
            "text": "For Hive based deployment, HADeploy embeds the  jdchive  tool.", 
            "title": "Hive"
        }, 
        {
            "location": "/more/under_the_hood/#hbase", 
            "text": "For HBase based deployment, HADeploy embeds the  jdchtable  tool.    For HBase dataset loading, HADeploy embeds the  hbload  tool.", 
            "title": "HBase"
        }, 
        {
            "location": "/more/under_the_hood/#kafka", 
            "text": "For Kafka based deployment, HADeploy embeds the  jdctopic  tool.", 
            "title": "Kafka"
        }, 
        {
            "location": "/more/execution_order/", 
            "text": "Execution order\n\n\nThe order of action performed on the target host does NOT depend of the order in the description file. Action order is the following:\n\n\n\n\nLinux local groups are created\n\n\nLinux local users are created.\n\n\nLinux nodes folders are created.\n\n\nFiles and trees with nodes as target are deployed\n\n\nApache Ranger policies are applied\n\n\nhdfs_relay host is configured, if needed\n\n\nHDFS folders are created\n\n\nFiles and trees with HDFS as target are deployed\n\n\nElastic indices and templates are created \n\n\nhbase_relay host is configured if needed.\n\n\nHBase namespaces and table are created.\n\n\nHBase datasets are loaded\n\n\nhive_relay host is configured if needed.\n\n\nHive databases and tables are created\n\n\nkafka_relay host is configured, if needed\n\n\nKafka topics are created\n\n\nServices and Supervisor are deployed\n\n\n\n\nRemoval action order is the reverse.\n\n\nInside each action group, order is preserved. For example, with:\n\n\nfolders:\n- { scope: en, path: /etc/broadapp, owner: root, group: root, mode: \n0755\n }\n- { scope: en, path: /opt/broadapp, owner: root, group: root, mode: \n0755\n }\n\n\n\n\n/etc/broadapp\n will be create before \n/opt/broadapp\n.\n\n\nSame for variables. This is a single pass evaluation. So, obviously:\n\n\n  app_version: \n0.1.1\n\n  repository_server: \nmyserver.com\n\n  app_jar_url: \nhttp://${repository_server}/repo/broadapp-${app_version}.jar\n\n\n\n\n\nWill be OK. While:\n\n\n  app_jar_url: \nhttp://${repository_server}/repo/broadapp-${app_version}.jar\n\n  app_version: \n0.1.1\n\n  repository_server: \nmyserver.com\n\n\n\n\n\nWill fail, with a \nvariable undefined\n error.\n\n\nPlugin Priority\n\n\nThe order described above result from an ordering of the plugin execution. \n\n\nTo achieve this, internally, each plugin is granted with a priority for each action. And execution follow ascending order of priority.\n\n\nHere are the values per plugin and action:\n\n\n\n\n\n\n\n\nPlugin\n\n\ngrooming\n\n\ndeploy\n\n\nremove\n\n\nstart\n\n\nstop\n\n\nstatus\n\n\n\n\n\n\n\n\n\n\nheader\n\n\n1100\n\n\n1100\n\n\n1100\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\n\n\nansible_inventories\n\n\n1200\n\n\n1200\n\n\n1200\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\n\n\ninventory\n\n\n1300 (3)\n\n\n1300\n\n\n1300\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\n\n\nansible\n\n\n1400\n\n\n(2)\n\n\n(2)\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\n\n\nusers\n\n\n2000\n\n\n2000\n\n\n7000\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\n\n\nNodes files \n folders\n\n\n3000\n\n\n3000\n\n\n4000\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\n\n\nranger\n\n\n8000 (1)\n\n\n3200\n\n\n3700\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\n\n\nhdfs files \n folders\n\n\n3500 (4)\n\n\n3500\n\n\n3500\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\n\n\nElastic\n\n\n3700\n\n\n3700\n\n\n3200\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\n\n\nhbase\n\n\n4000\n\n\n4000\n\n\n3000\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\n\n\nhive\n\n\n4500\n\n\n4500\n\n\n2500\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\n\n\nkafka\n\n\n5000\n\n\n5000\n\n\n2000\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\n\n\nSystemd\n\n\n2500 (6)\n\n\n6000\n\n\n1800\n\n\n5000\n\n\n5000\n\n\n5000\n\n\n\n\n\n\nSupervisor\n\n\n2510 (6)\n\n\n7000\n\n\n1600\n\n\n6000\n\n\n4000\n\n\n5000\n\n\n\n\n\n\nStorm\n\n\n2520 (6)\n\n\n7100 (5)\n\n\n1500 (7)\n\n\n6500\n\n\n3500\n\n\n5000\n\n\n\n\n\n\nYarn\n\n\n2520 (6)\n\n\n7050 (5)\n\n\n1550 (7)\n\n\n6400\n\n\n3600\n\n\n5000\n\n\n\n\n\n\n\n\nNB: \ngrooming\n is an internal action, performed on \nstep 4 of the run\n\n\nThese priorities value are of interest if you insert some raw ansible playbooks or roles (Using \nansible\n module) and want to control at which steps they will be executed.\n\n\n(1): Ranger grooming must occurs after all ranger aware resources grooming\n\n\n(2): Configured by the user in the deployment file\n\n\n(3): Must be after ansible inventory, for host overriding\n\n\n(4): Must be after files\n\n\n(5): This plugin is involved in 'deploy' action only by files notification\n\n\n(6): Must be before 'files', for notification handling\n\n\n(7): Does not remove anything, but stop all running Storm topologies and/or Yarn services", 
            "title": "Execution order"
        }, 
        {
            "location": "/more/execution_order/#execution-order", 
            "text": "The order of action performed on the target host does NOT depend of the order in the description file. Action order is the following:   Linux local groups are created  Linux local users are created.  Linux nodes folders are created.  Files and trees with nodes as target are deployed  Apache Ranger policies are applied  hdfs_relay host is configured, if needed  HDFS folders are created  Files and trees with HDFS as target are deployed  Elastic indices and templates are created   hbase_relay host is configured if needed.  HBase namespaces and table are created.  HBase datasets are loaded  hive_relay host is configured if needed.  Hive databases and tables are created  kafka_relay host is configured, if needed  Kafka topics are created  Services and Supervisor are deployed   Removal action order is the reverse.  Inside each action group, order is preserved. For example, with:  folders:\n- { scope: en, path: /etc/broadapp, owner: root, group: root, mode:  0755  }\n- { scope: en, path: /opt/broadapp, owner: root, group: root, mode:  0755  }  /etc/broadapp  will be create before  /opt/broadapp .  Same for variables. This is a single pass evaluation. So, obviously:    app_version:  0.1.1 \n  repository_server:  myserver.com \n  app_jar_url:  http://${repository_server}/repo/broadapp-${app_version}.jar   Will be OK. While:    app_jar_url:  http://${repository_server}/repo/broadapp-${app_version}.jar \n  app_version:  0.1.1 \n  repository_server:  myserver.com   Will fail, with a  variable undefined  error.", 
            "title": "Execution order"
        }, 
        {
            "location": "/more/execution_order/#plugin-priority", 
            "text": "The order described above result from an ordering of the plugin execution.   To achieve this, internally, each plugin is granted with a priority for each action. And execution follow ascending order of priority.  Here are the values per plugin and action:     Plugin  grooming  deploy  remove  start  stop  status      header  1100  1100  1100  -  -  -    ansible_inventories  1200  1200  1200  -  -  -    inventory  1300 (3)  1300  1300  -  -  -    ansible  1400  (2)  (2)  -  -  -    users  2000  2000  7000  -  -  -    Nodes files   folders  3000  3000  4000  -  -  -    ranger  8000 (1)  3200  3700  -  -  -    hdfs files   folders  3500 (4)  3500  3500  -  -  -    Elastic  3700  3700  3200  -  -  -    hbase  4000  4000  3000  -  -  -    hive  4500  4500  2500  -  -  -    kafka  5000  5000  2000  -  -  -    Systemd  2500 (6)  6000  1800  5000  5000  5000    Supervisor  2510 (6)  7000  1600  6000  4000  5000    Storm  2520 (6)  7100 (5)  1500 (7)  6500  3500  5000    Yarn  2520 (6)  7050 (5)  1550 (7)  6400  3600  5000     NB:  grooming  is an internal action, performed on  step 4 of the run  These priorities value are of interest if you insert some raw ansible playbooks or roles (Using  ansible  module) and want to control at which steps they will be executed.  (1): Ranger grooming must occurs after all ranger aware resources grooming  (2): Configured by the user in the deployment file  (3): Must be after ansible inventory, for host overriding  (4): Must be after files  (5): This plugin is involved in 'deploy' action only by files notification  (6): Must be before 'files', for notification handling  (7): Does not remove anything, but stop all running Storm topologies and/or Yarn services", 
            "title": "Plugin Priority"
        }, 
        {
            "location": "/more/altering_scope/", 
            "text": "Altering scope\n\n\nSometime, it may be useful to execute only parts of a full deployment. May be you have only a small modification to propagate, \nand we want cut off execution time. Or may be some part are not relevant for your current context.\n\n\nHADeploy provide a mechanism to scope the deployment (or removal) process to only some part, thus achieving partial execution.\n\n\nThis mechanisme is driven by:\n\n\n\n\nThe entry \nincluded_scopes\n in the application definition file, which define the list of 'scopes' as the only ones to execute.\n\n\nThe command line parameter \n--scope\n which allow full overriden of this list on command line.\n\n\nThe entry \nexcluded_scopes\n in the application definition file, which define a list of 'scopes' to exclude from execution.\n\n\nThe command line parameter \n--noScope\n which can provide other scopes to add the the exclusion list.\n\n\n\n\nScopes list\n\n\nThe valid scope values are:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nall\n\n\nAll scopes.\n\n\n\n\n\n\nnone\n\n\nNothing (Empty execution).\n\n\n\n\n\n\nfiles\n\n\nFiles, Folders and Trees on the nodes.\n\n\n\n\n\n\nhdfs\n\n\nFiles, Folders and Tree on HDFS.\n\n\n\n\n\n\nhbase\n\n\nAll HBase releted operations (Including Datasets).\n\n\n\n\n\n\nkafka\n\n\nAll Kafka related operations.\n\n\n\n\n\n\nhive\n\n\nAll Hive related operations.\n\n\n\n\n\n\nansible\n\n\nAll ansible plugin related operation\n\n\n\n\n\n\nranger\n\n\nAll Ranger related operations.\n\n\n\n\n\n\n\n\nPlus all the values provided as 'scope' attribute for the \nfiles\n,\n\nfolders\n and \ntrees\n entries.\n\n\nAs Command line parameters\n\n\nA stated above, the \nincluded_scopes\n list can also be provided on the command line:\n\n\nhadeploy --src app.yml --src infra.yml --scope files --scope kafka,hbase --action DEPLOY\n\n\n\n\nNote there is two methods to provide multiple scopes: Entering \n--scope\nseveral times, or provide a comma separated list of value (But without spaces).\n\n\nAnd to add to the \nexcluded_scopes\n list, there is the \n--noScope\n parameter, with the same logic:\n\n\nhadeploy --src app.yml --src infra.yml --noScope ranger --noScope hive,hbase --action DEPLOY", 
            "title": "Altering scope"
        }, 
        {
            "location": "/more/altering_scope/#altering-scope", 
            "text": "Sometime, it may be useful to execute only parts of a full deployment. May be you have only a small modification to propagate, \nand we want cut off execution time. Or may be some part are not relevant for your current context.  HADeploy provide a mechanism to scope the deployment (or removal) process to only some part, thus achieving partial execution.  This mechanisme is driven by:   The entry  included_scopes  in the application definition file, which define the list of 'scopes' as the only ones to execute.  The command line parameter  --scope  which allow full overriden of this list on command line.  The entry  excluded_scopes  in the application definition file, which define a list of 'scopes' to exclude from execution.  The command line parameter  --noScope  which can provide other scopes to add the the exclusion list.", 
            "title": "Altering scope"
        }, 
        {
            "location": "/more/altering_scope/#scopes-list", 
            "text": "The valid scope values are:     Name  Description      all  All scopes.    none  Nothing (Empty execution).    files  Files, Folders and Trees on the nodes.    hdfs  Files, Folders and Tree on HDFS.    hbase  All HBase releted operations (Including Datasets).    kafka  All Kafka related operations.    hive  All Hive related operations.    ansible  All ansible plugin related operation    ranger  All Ranger related operations.     Plus all the values provided as 'scope' attribute for the  files , folders  and  trees  entries.", 
            "title": "Scopes list"
        }, 
        {
            "location": "/more/altering_scope/#as-command-line-parameters", 
            "text": "A stated above, the  included_scopes  list can also be provided on the command line:  hadeploy --src app.yml --src infra.yml --scope files --scope kafka,hbase --action DEPLOY  Note there is two methods to provide multiple scopes: Entering  --scope several times, or provide a comma separated list of value (But without spaces).  And to add to the  excluded_scopes  list, there is the  --noScope  parameter, with the same logic:  hadeploy --src app.yml --src infra.yml --noScope ranger --noScope hive,hbase --action DEPLOY", 
            "title": "As Command line parameters"
        }, 
        {
            "location": "/more/secured_cluster/", 
            "text": "Secured Hadoop clusters\n\n\nKerberos support\n\n\nAll Kerberos configuration will occur in the relay definition. So, refer to\n\n\n\n\nhdfs_relay\n\n\nhive_relay\n\n\nhbase_relay\n\n\nkafka_relay\n\n\nstorm_relay\n\n\nsource_host_credentials\n\n\n\n\nIn the Reference part for how to configure Kerberos access for all HADeploy operations.\n\n\nRanger tricks\n\n\nSSL Certificate validation\n\n\nIf Ranger admin is configured with SSL, default configuration will require an valid certificate, one recognized be a registered certificate authority. If this is not the case, all Ranger access will throw an error, unless you:\n\n\n\n\nDisable certificate validation: \nvalidate_certs: no\n in \nranger_relay\n\n\n\n\nOr:\n\n\n\n\nProvide a CA_BUNDLE.\n\n\n\n\nIn its simplest case, a CA_BUNDLE can be simply the certificate of the Ranger server, in PEM format.\n\n\nTo grab this certificate, you may use a tiny python program like the following:\n\n\nimport ssl\n\nif __name__ == '__main__':\n    cert = ssl.get_server_certificate((\nranger.mycluster.corp.com\n, 6182), ssl_version=ssl.PROTOCOL_SSLv23)\n    print cert\n    f = open(\ncert.pem\n, \nw\n)\n    f.write(cert)\n    f.close()\n\n\n\n\nResources collision\n\n\nApache Ranger does not allow to have several policies granting access to the same set of resources (Path, table, topics,...).\n\n\nTo work around this limitation, a simple trick it to add an un-existing, fake resource to the resource list on one of the colliding policies.\n\n\nFor example:\n\n\nkafka_ranger_policies:\n- name: \nallToJim\n\n  topics: \n  - \n*\n\n  - \ndummy_topic\n\n  permissions:\n  - users:\n    - jim\n  accesses:\n    - publish\n    - consume\n    - configure\n    - describe\n    - create\n    - delete\n    - kafka_admin", 
            "title": "Secured cluster"
        }, 
        {
            "location": "/more/secured_cluster/#secured-hadoop-clusters", 
            "text": "", 
            "title": "Secured Hadoop clusters"
        }, 
        {
            "location": "/more/secured_cluster/#kerberos-support", 
            "text": "All Kerberos configuration will occur in the relay definition. So, refer to   hdfs_relay  hive_relay  hbase_relay  kafka_relay  storm_relay  source_host_credentials   In the Reference part for how to configure Kerberos access for all HADeploy operations.", 
            "title": "Kerberos support"
        }, 
        {
            "location": "/more/secured_cluster/#ranger-tricks", 
            "text": "", 
            "title": "Ranger tricks"
        }, 
        {
            "location": "/more/secured_cluster/#ssl-certificate-validation", 
            "text": "If Ranger admin is configured with SSL, default configuration will require an valid certificate, one recognized be a registered certificate authority. If this is not the case, all Ranger access will throw an error, unless you:   Disable certificate validation:  validate_certs: no  in  ranger_relay   Or:   Provide a CA_BUNDLE.   In its simplest case, a CA_BUNDLE can be simply the certificate of the Ranger server, in PEM format.  To grab this certificate, you may use a tiny python program like the following:  import ssl\n\nif __name__ == '__main__':\n    cert = ssl.get_server_certificate(( ranger.mycluster.corp.com , 6182), ssl_version=ssl.PROTOCOL_SSLv23)\n    print cert\n    f = open( cert.pem ,  w )\n    f.write(cert)\n    f.close()", 
            "title": "SSL Certificate validation"
        }, 
        {
            "location": "/more/secured_cluster/#resources-collision", 
            "text": "Apache Ranger does not allow to have several policies granting access to the same set of resources (Path, table, topics,...).  To work around this limitation, a simple trick it to add an un-existing, fake resource to the resource list on one of the colliding policies.  For example:  kafka_ranger_policies:\n- name:  allToJim \n  topics: \n  -  * \n  -  dummy_topic \n  permissions:\n  - users:\n    - jim\n  accesses:\n    - publish\n    - consume\n    - configure\n    - describe\n    - create\n    - delete\n    - kafka_admin", 
            "title": "Resources collision"
        }, 
        {
            "location": "/more/yaml_tricks/", 
            "text": "YAML tricks\n\n\nBlock merging\n\n\nTo provide more flexibility about source file organization, HADeploy allow you to split definition in several logical parts, using block merging.\n\n\nFor example, let's say you have the following:\n\n\ngroups:\n- name: grp1\n- name: grp2\n\nusers: \n- login: user1\n  groups: \ngrp1,grp2\n\n- login: user2\n  groups: \ngrp1,grp2\n\n\n\n\n\nThis could also be expressed:\n\n\ngroups:\n- name: grp1\n\ngroups:\n- name: grp2\n\nusers: \n- login: user1\n  groups: \n grp1,grp2\n\n\nusers:\n- login: user2\n  groups: \n grp1,grp2\n\n\n\n\n\nor:\n\n\ngroups:\n- name: grp1\n\nusers: \n- login: user1\n  groups: \n grp1,grp2\n\n\ngroups:\n- name: grp2\n\nusers:\n- login: user2\n  groups: \n grp1,grp2\n\n\n\n\n\nThis can also be split in two files:\n\n\ngroups:\n- name: grp1\n\nusers: \n- login: user1\n  groups: \n grp1,grp2\n\n\n\n\n\ngroups:\n- name: grp2\n\nusers:\n- login: user2\n  groups: \n grp1,grp2\n\n\n\n\n\nNote than file order will be irrelevant, as effective deployment action ordering is NOT driven by the order of occurrence of declaration. See \nExecution order\n\n\nYAML anchors\n\n\nYAML anchors are a powerful YAML feature, which could be of great help in some cases.\nFor example, lets consider the following snippet:\n\n\nfiles:\n- scope: hdfs\n  src: file://myapp1.jar\n  dest_folder: /apps/mayapp\n  owner: myappuser\n  group: myappgrp\n  mode: \n0644\n\n- scope: hdfs\n  src: file://myapp2.jar\n  dest_folder: /apps/mayapp\n  owner: myappuser\n  group: myappgrp\n  mode: \n0644\n\n- scope: hdfs\n  src: file://myapp3.jar\n  dest_folder: /apps/mayapp\n  owner: myappuser\n  group: myappgrp\n  mode: \n0644\n\n\n\n\n\nIt can be written as:\n\n\nvars: \n  permsFiles: \npermsFiles\n    owner: myappuser\n    group: myappgrp\n    mode: \n0644\n\n\nfiles:\n- scope: hdfs\n  src: file://myapp1.jar\n  dest_folder: /apps/mayapp\n  \n: *permsFiles\n- scope: hdfs\n  src: file://myapp2.jar\n  dest_folder: /apps/mayapp\n  \n: *permsFiles\n- scope: hdfs\n  src: file://myapp3.jar\n  dest_folder: /apps/mayapp\n  \n: *permsFiles\n\n\n\n\nOr, using flow style:\n\n\nvars: \n  permsFiles: \npermsFiles\n    owner: myappuser\n    group: myappgrp\n    mode: \n0644\n\n\nfiles:\n- { scope: hdfs, src: \nfile://myapp1.jar\n, dest_folder: /apps/mayapp, \n: *permsFiles }\n- { scope: hdfs, src: \nfile://myapp2.jar\n, dest_folder: /apps/mayapp, \n: *permsFiles }\n- { scope: hdfs, src: \nfile://myapp3.jar\n, dest_folder: /apps/mayapp, \n: *permsFiles }", 
            "title": "YAML tricks"
        }, 
        {
            "location": "/more/yaml_tricks/#yaml-tricks", 
            "text": "", 
            "title": "YAML tricks"
        }, 
        {
            "location": "/more/yaml_tricks/#block-merging", 
            "text": "To provide more flexibility about source file organization, HADeploy allow you to split definition in several logical parts, using block merging.  For example, let's say you have the following:  groups:\n- name: grp1\n- name: grp2\n\nusers: \n- login: user1\n  groups:  grp1,grp2 \n- login: user2\n  groups:  grp1,grp2   This could also be expressed:  groups:\n- name: grp1\n\ngroups:\n- name: grp2\n\nusers: \n- login: user1\n  groups:   grp1,grp2 \n\nusers:\n- login: user2\n  groups:   grp1,grp2   or:  groups:\n- name: grp1\n\nusers: \n- login: user1\n  groups:   grp1,grp2 \n\ngroups:\n- name: grp2\n\nusers:\n- login: user2\n  groups:   grp1,grp2   This can also be split in two files:  groups:\n- name: grp1\n\nusers: \n- login: user1\n  groups:   grp1,grp2   groups:\n- name: grp2\n\nusers:\n- login: user2\n  groups:   grp1,grp2   Note than file order will be irrelevant, as effective deployment action ordering is NOT driven by the order of occurrence of declaration. See  Execution order", 
            "title": "Block merging"
        }, 
        {
            "location": "/more/yaml_tricks/#yaml-anchors", 
            "text": "YAML anchors are a powerful YAML feature, which could be of great help in some cases.\nFor example, lets consider the following snippet:  files:\n- scope: hdfs\n  src: file://myapp1.jar\n  dest_folder: /apps/mayapp\n  owner: myappuser\n  group: myappgrp\n  mode:  0644 \n- scope: hdfs\n  src: file://myapp2.jar\n  dest_folder: /apps/mayapp\n  owner: myappuser\n  group: myappgrp\n  mode:  0644 \n- scope: hdfs\n  src: file://myapp3.jar\n  dest_folder: /apps/mayapp\n  owner: myappuser\n  group: myappgrp\n  mode:  0644   It can be written as:  vars: \n  permsFiles:  permsFiles\n    owner: myappuser\n    group: myappgrp\n    mode:  0644 \n\nfiles:\n- scope: hdfs\n  src: file://myapp1.jar\n  dest_folder: /apps/mayapp\n   : *permsFiles\n- scope: hdfs\n  src: file://myapp2.jar\n  dest_folder: /apps/mayapp\n   : *permsFiles\n- scope: hdfs\n  src: file://myapp3.jar\n  dest_folder: /apps/mayapp\n   : *permsFiles  Or, using flow style:  vars: \n  permsFiles:  permsFiles\n    owner: myappuser\n    group: myappgrp\n    mode:  0644 \n\nfiles:\n- { scope: hdfs, src:  file://myapp1.jar , dest_folder: /apps/mayapp,  : *permsFiles }\n- { scope: hdfs, src:  file://myapp2.jar , dest_folder: /apps/mayapp,  : *permsFiles }\n- { scope: hdfs, src:  file://myapp3.jar , dest_folder: /apps/mayapp,  : *permsFiles }", 
            "title": "YAML anchors"
        }, 
        {
            "location": "/more/release_notes/", 
            "text": "Release notes\n\n\n0.6.0\n\n\n\n\n/var/run/supervisor_xxxx folder was sometime deleted (i.e. after a hard restart). Now automatically recreated\n\n\nYarn service failed when existing job list was empty. Fixed\n\n\nImproving documentation on Yarn services integration with Kerberos.\n\n\nAdded handling of secured access to elasticsearch cluster (Authentication and SSL).\n\n\n\n\nINCOMPATIBILITY\n\n\n\n\nkafka_relay\n has a new mandatory parameter: \nkafka_version\n. \n\n\nDue to change in the Kerberos handling of the Kafka plugin, \nprincipal\n and \nxxx_keytab_path\n parameters has been removed from the \nkafka_relay\n definition.\n\n\n\n\n0.5.7\n\n\n\n\nFixed this [DEPRECATION WARNING]: Using tests as filters is deprecated. Instead of using \nresult|succeeded\n instead use \nresult is succeeded\n\n\n--action dumpvars\n now display the file where the variable is defined.\n\n\n\n\n0.5.6\n\n\n\n\nAdded a \nYarn plugin\n to handle Yarn services (long running jobs) lifecycle.\n\n\nStorm topologies ACTIVE status is now displayed in upper case, for better visibility.\n\n\nHDFS plugin: \nhadoop_conf_dir\n and \nwebhdfs_endpoint\n was not propagated on all HDFS commands. Fixed.\n\n\nUsers plugin: When a user is not managed and an authorized_keys is to be set, HADeploy ensure home folder is accessible by performing an \nsu\n command. \n\n\n\n\n0.5.5\n\n\n\n\nAdded Elasticsearch \nindices\n and \nTemplates\n management.\n\n\nScope for Storm's notification handler was erroneous. Fixed\n\n\nNow allow missing both \nssh_private_key_file\n and \nssh_password\n in host definition. (As already stated in the doc)\n\n\nNotification system generated some errors when scope was limited to \nfiles\n. Fixed by suppressing notification in such case.\n\n\nModification of \ngrooming order\n for \nStorm\n, \nSystemd\n and \nSupervisor\n plugins. Now grooming occurs before \nfiles\n, to handle notifications correctly.\n\n\nansible_inventories\n is now compatible with \nansible_user/host/...\n variables (Formerly \nansible_ssh_user/host/...\n).\n\n\nRefactoring of \nsupervisors\n plugin to have one configuration file per program and group (Instead of a single file for all).\n\n\nAdded a \nscope\n attribute to \nsupervisor_programs\n items. \n\n\nAction \nremove\n on \nStorm plugin\n now kill all running topologies.\n\n\n\n\n0.5.4\n\n\n\n\nAdded an action \ndumpvars\n, to dump all variables.\n\n\nWith Ranger 0.7, there was unjustified 'changed' on policies settings. Fixed\n\n\nAdded a \nStorm plugin\n to handle Storm topologies lifecycle.\n\n\nAdded \nStorm Ranger policies\n management.\n\n\nAction \nstatus\n as been implemented in \nStorm\n, \n\nSystemd\n and \nSupervisor\n plugins.\n\n\n\n\nINCOMPATIBILITY\n\n\n\n\nNotification syntax has been modified for \nfiles\n definition.\n\n\n\n\n0.5.3\n\n\n\n\nIn some cases, using include directive disrupted some relative file relocation. Fixed\n\n\nAn \nalternate variable notation\n (\n...\n) has been introduced to allow non-string variable in flow style notation.\n\n\nConditional deployment\n was implemented in all plugins.\n\n\nA switch \nno_log\n has been added to \nranger_relay\n to ease debugging.\n\n\n\n\n0.5.2\n\n\n\n\nA change in the API of Ansible 2.4 broke \nansible_inventory\n plugin. Fixed.\n\n\n\n\n0.5.1\n\n\n\n\nAdded almost all existing Ansible configuration variables for \nhosts\n and \nhost_override\n in inventory.\n\n\nAdded maven artifact download, using \n'maven_repositories`\n definition.\n\n\nChanged all references to ansible variable \nansible_ssh_user\n to \nansible_user\n (Following Ansible evolution).\n\n\nSome (small) documentation improvements.\n\n\nAdded \npriority\n attribute on \nhost_overrides\n\n\nAdded \nsystemd\n plugin with \nsystemd_units\n service management.\n\n\nAdded \nsupervisor\n plugin with \nsupervisor\n process controler management.\n\n\n\n\n0.5.0\n\n\n\n\nPlugins architecture refactoring\n\n\nNew plugin: \nansible\n, to insert raw Ansible playbook or role in the deployment.\n\n\nscope value are now checked.\n\n\nAdded an \nencrypted_vars\n block to have a more generic encryption capability.\n\n\n\n\nINCOMPATIBILITY\n\n\n\n\nThe \nranger_relay.admin_password\n encrypted with 0.4.0 method must be modified to comply to new, generic syntax.\n\n\n\n\n0.4.1\n\n\n\n\nAdded \nscope limitation\n mechanism on performed operation\n\n\nDefault HDFS relay cache is now /tmp/hadeploy_{{ansible_ssh_user}}/files\n\n\nAdded \nremote_tmp = /tmp/.ansible-${USER}/tmp\n in generated ansible.cfg\n\n\n\n\n0.4.0\n\n\n\n\nSome quotes was missing in plugin yaml files. This generated errors when a HADeploy variable was substituted with a string begining with {{some_ansible_variable}}. Fixed\n\n\nhdfs_relay.cache_folder\n default value is modified, to be in the user home folder.\n\n\nhbase_relay.tools_folder\n default value is modified to be in \n/tmp/hadeploy_{{ansible_ssh_user}}\n\n\nhive_relay.tools_folder\n default value is modified to be in \n/tmp/hadeploy_{{ansible_ssh_user}}\n\n\nkafka_relay.tools_folder\n default value is modified to be in \n/tmp/hadeploy_{{ansible_ssh_user}}\n\n\nLocal file definition now works with ~/xxx\n\n\nhdfs_relay.user\n default value is now \nhdfs\n only if \nssh_user\nis root. Otherwise, is \nssh_user\n\n\nAdded global \nexit_on_fail\n flag.\n\n\nAdded some retry on user/groups creation/removal\n\n\nAdded \nbecome_user/become_method\n on \nhbase_relay\n\n\nAdded \nbecome_user/become_method\n on \nkafka_relay\n\n\nAdded \nhbase_relay.local_keytab_path\n\n\nAdded \nkafka_relay.local_keytab_path\n\n\nAdded \nhdfs_relay.local_keytab_path\n\n\nAdded \nsource_host_credential.local_keytab_path\n\n\nAdded \ngroups\n in \nhost_groups\n\n\nPrevious version was unable to fetch an existing Ansible inventory when it contains some encrypted file(s). \nThe Description of Ansible inventory has been modified (now \nansible_inventories\n) to include a password file or user password request.\n\n\nAdded a method to encrypt password in \nranger_relay.admin_password\n.   \n\n\n\n\nDEPRECATION\n\n\nansible_inventory_files\n has been marked deprecated. Replaced by \nansible_inventories\n.\n\n\nINCOMPATIBILITY\n\n\nThere is some incompatible change with previous version. You may need to modify your source files:\n\n\n\n\nhive_relay.user\n is renamed \nhive_relay.become_user\n.\n\n\nhbase_relay.keytab_path\nis renamed \nhbase_relay.relay_keytab_path\n\n\nkafka_relay.keytab_path\nis renamed \nkafka_relay.relay_keytab_path\n\n\nhdfs_relay.keytab_path\nis renamed \nhdfs_relay.relay_keytab_path\n\n\nsource_host_credential.keytab_path\nis renamed \nsource_host_credential.relay_keytab_path\n\n\nFor \nfiles\n and \ntrees\n, \nnode\n:///\n is replaced by \nnode://\nnode\n/...\n notation", 
            "title": "Release notes"
        }, 
        {
            "location": "/more/release_notes/#release-notes", 
            "text": "", 
            "title": "Release notes"
        }, 
        {
            "location": "/more/release_notes/#060", 
            "text": "/var/run/supervisor_xxxx folder was sometime deleted (i.e. after a hard restart). Now automatically recreated  Yarn service failed when existing job list was empty. Fixed  Improving documentation on Yarn services integration with Kerberos.  Added handling of secured access to elasticsearch cluster (Authentication and SSL).", 
            "title": "0.6.0"
        }, 
        {
            "location": "/more/release_notes/#incompatibility", 
            "text": "kafka_relay  has a new mandatory parameter:  kafka_version .   Due to change in the Kerberos handling of the Kafka plugin,  principal  and  xxx_keytab_path  parameters has been removed from the  kafka_relay  definition.", 
            "title": "INCOMPATIBILITY"
        }, 
        {
            "location": "/more/release_notes/#057", 
            "text": "Fixed this [DEPRECATION WARNING]: Using tests as filters is deprecated. Instead of using  result|succeeded  instead use  result is succeeded  --action dumpvars  now display the file where the variable is defined.", 
            "title": "0.5.7"
        }, 
        {
            "location": "/more/release_notes/#056", 
            "text": "Added a  Yarn plugin  to handle Yarn services (long running jobs) lifecycle.  Storm topologies ACTIVE status is now displayed in upper case, for better visibility.  HDFS plugin:  hadoop_conf_dir  and  webhdfs_endpoint  was not propagated on all HDFS commands. Fixed.  Users plugin: When a user is not managed and an authorized_keys is to be set, HADeploy ensure home folder is accessible by performing an  su  command.", 
            "title": "0.5.6"
        }, 
        {
            "location": "/more/release_notes/#055", 
            "text": "Added Elasticsearch  indices  and  Templates  management.  Scope for Storm's notification handler was erroneous. Fixed  Now allow missing both  ssh_private_key_file  and  ssh_password  in host definition. (As already stated in the doc)  Notification system generated some errors when scope was limited to  files . Fixed by suppressing notification in such case.  Modification of  grooming order  for  Storm ,  Systemd  and  Supervisor  plugins. Now grooming occurs before  files , to handle notifications correctly.  ansible_inventories  is now compatible with  ansible_user/host/...  variables (Formerly  ansible_ssh_user/host/... ).  Refactoring of  supervisors  plugin to have one configuration file per program and group (Instead of a single file for all).  Added a  scope  attribute to  supervisor_programs  items.   Action  remove  on  Storm plugin  now kill all running topologies.", 
            "title": "0.5.5"
        }, 
        {
            "location": "/more/release_notes/#054", 
            "text": "Added an action  dumpvars , to dump all variables.  With Ranger 0.7, there was unjustified 'changed' on policies settings. Fixed  Added a  Storm plugin  to handle Storm topologies lifecycle.  Added  Storm Ranger policies  management.  Action  status  as been implemented in  Storm ,  Systemd  and  Supervisor  plugins.", 
            "title": "0.5.4"
        }, 
        {
            "location": "/more/release_notes/#incompatibility_1", 
            "text": "Notification syntax has been modified for  files  definition.", 
            "title": "INCOMPATIBILITY"
        }, 
        {
            "location": "/more/release_notes/#053", 
            "text": "In some cases, using include directive disrupted some relative file relocation. Fixed  An  alternate variable notation  ( ... ) has been introduced to allow non-string variable in flow style notation.  Conditional deployment  was implemented in all plugins.  A switch  no_log  has been added to  ranger_relay  to ease debugging.", 
            "title": "0.5.3"
        }, 
        {
            "location": "/more/release_notes/#052", 
            "text": "A change in the API of Ansible 2.4 broke  ansible_inventory  plugin. Fixed.", 
            "title": "0.5.2"
        }, 
        {
            "location": "/more/release_notes/#051", 
            "text": "Added almost all existing Ansible configuration variables for  hosts  and  host_override  in inventory.  Added maven artifact download, using  'maven_repositories`  definition.  Changed all references to ansible variable  ansible_ssh_user  to  ansible_user  (Following Ansible evolution).  Some (small) documentation improvements.  Added  priority  attribute on  host_overrides  Added  systemd  plugin with  systemd_units  service management.  Added  supervisor  plugin with  supervisor  process controler management.", 
            "title": "0.5.1"
        }, 
        {
            "location": "/more/release_notes/#050", 
            "text": "Plugins architecture refactoring  New plugin:  ansible , to insert raw Ansible playbook or role in the deployment.  scope value are now checked.  Added an  encrypted_vars  block to have a more generic encryption capability.", 
            "title": "0.5.0"
        }, 
        {
            "location": "/more/release_notes/#incompatibility_2", 
            "text": "The  ranger_relay.admin_password  encrypted with 0.4.0 method must be modified to comply to new, generic syntax.", 
            "title": "INCOMPATIBILITY"
        }, 
        {
            "location": "/more/release_notes/#041", 
            "text": "Added  scope limitation  mechanism on performed operation  Default HDFS relay cache is now /tmp/hadeploy_{{ansible_ssh_user}}/files  Added  remote_tmp = /tmp/.ansible-${USER}/tmp  in generated ansible.cfg", 
            "title": "0.4.1"
        }, 
        {
            "location": "/more/release_notes/#040", 
            "text": "Some quotes was missing in plugin yaml files. This generated errors when a HADeploy variable was substituted with a string begining with {{some_ansible_variable}}. Fixed  hdfs_relay.cache_folder  default value is modified, to be in the user home folder.  hbase_relay.tools_folder  default value is modified to be in  /tmp/hadeploy_{{ansible_ssh_user}}  hive_relay.tools_folder  default value is modified to be in  /tmp/hadeploy_{{ansible_ssh_user}}  kafka_relay.tools_folder  default value is modified to be in  /tmp/hadeploy_{{ansible_ssh_user}}  Local file definition now works with ~/xxx  hdfs_relay.user  default value is now  hdfs  only if  ssh_user is root. Otherwise, is  ssh_user  Added global  exit_on_fail  flag.  Added some retry on user/groups creation/removal  Added  become_user/become_method  on  hbase_relay  Added  become_user/become_method  on  kafka_relay  Added  hbase_relay.local_keytab_path  Added  kafka_relay.local_keytab_path  Added  hdfs_relay.local_keytab_path  Added  source_host_credential.local_keytab_path  Added  groups  in  host_groups  Previous version was unable to fetch an existing Ansible inventory when it contains some encrypted file(s). \nThe Description of Ansible inventory has been modified (now  ansible_inventories ) to include a password file or user password request.  Added a method to encrypt password in  ranger_relay.admin_password .", 
            "title": "0.4.0"
        }, 
        {
            "location": "/more/release_notes/#deprecation", 
            "text": "ansible_inventory_files  has been marked deprecated. Replaced by  ansible_inventories .", 
            "title": "DEPRECATION"
        }, 
        {
            "location": "/more/release_notes/#incompatibility_3", 
            "text": "There is some incompatible change with previous version. You may need to modify your source files:   hive_relay.user  is renamed  hive_relay.become_user .  hbase_relay.keytab_path is renamed  hbase_relay.relay_keytab_path  kafka_relay.keytab_path is renamed  kafka_relay.relay_keytab_path  hdfs_relay.keytab_path is renamed  hdfs_relay.relay_keytab_path  source_host_credential.keytab_path is renamed  source_host_credential.relay_keytab_path  For  files  and  trees ,  node :///  is replaced by  node:// node /...  notation", 
            "title": "INCOMPATIBILITY"
        }
    ]
}